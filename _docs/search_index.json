[["index.html", "WorldPop Population Modelling Training Manual, Vol. I Welcome Preface Book Contents", " WorldPop Population Modelling Training Manual, Vol. I WorldPop, University of Southampton 2025-07-10 Welcome Welcome to WorldPop Population Modelling Training Manual, vol I. This is an open book designed to provide in-depth guide and understanding to researchers and users of WorldPop small area population datasets on how the underlying statistical methods are developed and implemented across various contexts. It complements the WorldPop Book of Methods by going a step further to demonstrate the step-by-step processes of ‘how-to-do’ rather than ‘how-it-is-done’ by providing relevant background knowledge and practical examples. The manual which was developed by the WorldPop Research Group within the School of Geography and Environmental Science at the University of Southampton, is made up of 11 modules tailored to meet the needs of diverse audience. Although mainly based on R statistical programming language, the manual is designed to accommodate users at various levels of expertise and backgrounds by including very basics of R and statistical methods, in addition to more complex methodologies within the later modules. The manual is licensed under a Creative Commons Attribution-NoDerivatives 4.0 International (CC BY-ND 4.0) License, and the lead developer and compiler was Layna Dennett, with the support of Somnath Chaudhuri, under the coordination and supervision of Chris Nnanatu, while oversight was provided by Andrew Tatem. To cite this manual, please use: WorldPop. 2025. WorldPop Population Modelling Training Manual, vol I. WorldPop, University of Southampton. 10 July 2025 (https://wpgp.github.io/xxxxx/)[CN1] Preface Welcome to the “Population Modelling Training Manual,” produced by the WorldPop Research Group at the University of Southampton. This manual is designed to serve as a comprehensive resource for individuals interested in learning how WorldPop produces modelled small area population estimates, offering a structured approach to both fundamental and advanced techniques. As global population dynamics become increasingly complex, the need for robust modelling methodologies is paramount. This training manual addresses this need by providing a step-by-step guide which covers introduction to the R programming language, alongside essential statistical methods. Whether you are a beginner or an experienced researcher, you will find valuable insights and practical guidance tailored to your skill level. We explore both bottom-up and top-down population modelling techniques, ensuring a holistic understanding of how these methodologies can be applied to real-world scenarios using real data examples, wherever appropriate. Additionally, we delve into relevant geospatial modelling methods, recognizing the importance of spatial data in population studies. Each section is designed to build your competency progressively, beginning with foundational concepts and advancing to more sophisticated techniques. Case studies and practical exercises are included to facilitate hands-on learning and application of the methods discussed. We encourage readers to actively engage with the material, collaborate with peers, and apply the knowledge gained to address pressing population issues. Our goal is to empower you with the tools necessary to contribute to the field of small area population estimation and support informed decision-making. We extend our gratitude to the contributors and collaborators within the WorldPop Research Group, whose expertise and dedication have greatly enriched this manual, especially Layna Dennett who was the lead developer and compiler, Somnath Chaudhuri who provided all essential technical supports, Chris Nnanatu who initiated the idea and provided coordination and supervision throughout, and many others whose valuable supports and feedback are highly acknowledged. Your commitment to advancing population research is invaluable. Thank you for joining us on this journey into the world of population modelling. We hope this manual serves as a valuable asset in your academic and professional endeavours. Prof. Andrew Tatem Director WorldPop University of Southampton Acknowledgements We express our profound gratitude to key individuals and organisations who played various roles in making this population modelling training manual a reality. We appreciate Layna Dennett for doing a fantastic job leading the development and compilation of the training manual, and we are very grateful to Somnath Chaudhuri for all the essential technical supports he provided throughout the development and compilation of the training manual. We thank Chris Nnanatu for initiating the project and providing coordination and supervision throughout. Additionally, we thank Ortis Yankey and Assane Gadiaga for their various contributions, and we are grateful to Heather Chamberlain and Sarchil Qadar for providing some images used in the manual. The manual benefited immensely from reviews and valuable feedback from Gianluca Boo, Amy Bonnie, Iyanuloluwa Olowe, and Andrew Tatem, and we appreciate them for this. Finally, this manual would not have been a reality without the demographic datasets provided by Cameroon National Institute of Statistics (CNIS), and we thank Mr Anaclet Desire Dzossa specially for his immense support. And we wish to acknowledge the continued generous financial supports of our key funding partners: the Bill &amp; Melinda Gates Foundation, the United Nations Population Fund (UNFPA), Geo-Referenced Infrastructure and Demographic Data for Development (GRID3), the Foreign, Commonwealth &amp; Development Office (FCDO), and the World Bank, as well as other United Nations agencies. Book Contents Module 1: Basics of R Programming Ideal for individuals who have little to no experience with R, or who would like a reminder. It contains the basics on getting started with R, beginning with how to install R on your computer and understanding the key aspects of the software. Lastly, Module 1 gives an introduction to using R, including information on the basic operations, functions, and data structures. Module 2: Working with Data Frames Focuses on working with data frames, including how to install data into the R environment, basic data wrangling methods and data visualisation. Module 3: Working with Spatial Data in R Introduction spatial data, with an overview of the different types at the start and how to work with them. This module also includes some basic GIS/geoprocessing methods in R, and expands on the data visualisation methods from Module 2, accounting for the spatial aspect to the data. Module 4: Introduction to Statistical Modelling with Implementation in R Introduction statistical modelling and the implementation of the different methods into R. Additionally, it covers different methods of making predictions from the models, model selection, and cross-validation. Module 5: Probability Theory and Applications Introduction to probability theory, focusing on the methods and applications rather than the code. This module covers the foundations of both theoretical and experimental probability, the axioms of probability, and more detailed explanations of joint and conditional probability, with simple, easy-to-follow examples throughout. Module 6: Bayesian Statistical Inference Covers the basics to more in-depth Bayesian statistical inference, including Bayes’ rule (also called Bayes’ theorem), likelihoods, the prior distribution, and the posterior distribution. Module 7: Introduction to Small Area Population Estimation and Modelling (SAPEM) Focuses on small area population estimation, with information on both direct estimation and indirect estimation, where the latter includes both top-down and bottom-up methodology, with the relevant code given in the Book of Methods. Module 8: Bayesian Hierarchical Population Modelling in R Expands on the Bayesian hierarchical population modelling covered in earlier modules, focusing instead on the methods with spatial data. It introduces the different key data sources, methods for covariate extraction, exploratory analysis, and model setup. The remainder of the module briefly covers the STAN (MCMC) and goes into detail for the INLA-SPDE approach to Bayesian hierarchical modelling. Module 9: Model Fit Checks and Cross-Validation Expands on methods discussed in earlier modules, detailing on model assumption checking methods, and different techniques for model selection. Additionally, this module covers cross-validation methods in more detail than in Module 4. Module 10: Population Prediction and Uncertainty Quantification Looks at population prediction methods and the associated uncertainty quantification, starting with covariate stacking. It also covers posterior distribution simulation, aggregation to area units of interest, and grid cell/level prediction. Module 11: Age-Sex Disaggregation The final module and covers disaggregation of population totals by age-sex proportions for instances where there is age-sex data available as well as instances where there is not available. "],["basics-of-r-programming.html", "1 Basics of R Programming 1.1 Overview of R 1.2 Data types and key components 1.3 Key functions in R 1.4 Data structures 1.5 End of module exercises 1.6 Useful resources", " 1 Basics of R Programming This module is ideal for individuals who have little to no experience with R, and those who would like a reminder. It contains the basics for getting started with R, beginning with how to install R on your computer, and understanding the key aspects of the software. Lastly, Module 1 gives an introduction to using R, including information on the basic operations, functions, and data structures. 1.1 Overview of R R is a free, open source statistical programming language and environment used for statistical computing and has a wide range of graphical facilities for data analysis. Its uses also include (but not limited to) the following: Data manipulation - e.g. extract, clean, store, process, … Calculations (simple and complex) - e.g. addition, subtraction, … Data visualisation - e.g. charts, line graphs, maps, exploratory analysis, … Statistical and data analysis - e.g. regression, t-tests, correlation, … Machine and deep learning - e.g. modelling, cross-validation, descriptive statistics, … Reporting and creating documents (using Markdown and LaTeX) - e.g. reports, presentations, reproducible research, … Mapping (Geographic Information System, GIS) - e.g. interactive visualisation, population size estimation,… Getting started with R Downloading R and RStudio R can be downloaded on Windows, Mac and Linux from R-Project.org. Select the link to download for the correct operating system for your computer. Figure 1.1: R download webpage For Windows, click “install R for the first time”. Figure 1.2: R download webpage: Windows For macOS, click the correct download package for your Mac’s software version. Figure 1.3: R download webpage: macOS R does not have a particularly user-friendly graphical user interface (GUI). RStudio is a capable and free GUI add-on for R with improved functionality and usability. It can be downloaded from Postit.co. The webpage for downloading RStudio detects whether the operating system is Windows or macOS (or Linux), so the correct operating system does not need to be selected. For Windows users, the webpage will look like the image below. Figure 1.4: RStudio download webpage: Windows For macOS users, the webpage will look like the image below. Figure 1.5: RStudio download webpage: macOS Once “Download RStudio desktop” has been selected, the preferred version of RStudio can be selected, e.g. 2023.03.1+446, and the installation process can be followed once the download is complete. To check both R and RStudio are installed on the computer, search for “R” and “RStudio” in the search bar of the computer. Once both are installed, the process is complete and they can be used. Note: RStudio is just one example of R GUI, others are available such as Positron. Features of the R GUI environment When opening R, the user interface should have a window open for the console, where calculations can be made and results will be printed. It should look something like the image below. Figure 1.6: R GUI upon opening Additional windows can be opened, such as windows for scripts to code in and windows to display plots, seen in the image below. Figure 1.7: Annotated R GUI Features of the RStudio environment The user interface for RStudio is more efficient, with designated panes for displaying different information all at once in a much more convenient way. Figure 1.8: Annotated RStudio GUI There are four panes as seen in the figure below: Script - this pane is only present once a script has been created Console - where numerical output will be returned Viewer - computer files, plots, packages, help files, reports and presentations can be viewed here Environment - any objects that have been created are present in the environment and can be cleared by clicking on the ‘broom’ icon toward the centre of the toolbar See the image below for assistance in navigating the key aspects of the RStudio GUI. Figure 1.9: Annotated RStudio GUI Creating, opening and saving R scripts Whilst commands can be typed directly into the console, it is difficult to keep record of what has been run to ensure the code is reproducible and inconvenient when typographical mistakes are made. Utilising a script solves these issues, with the ability to make changes to and save the code written. In RStudio, to open a new script, use the main toolbar and follow File &gt; New File &gt; R Script and pane 1 will appear. If a script is already present, it will open the new script in a new tab in the pane. Figure 1.10: Annotated RStudio GUI: New script To run the code from a script, highlight the desired line(s) to run and click Run on the script pane at the top right. Alternatively, keyboard short cuts can be used to run the code, press Control + Enter on Windows or Command + Enter on Mac. If code is run without highlighting any lines, only the current line (where the cursor is) will be run. To open an existing R script, use the main toolbar and follow File &gt; Open File then navigate to the desired R script in the files pop-up window. Figure 1.11: Annotated RStudio GUI: Open script To save an R script, use the main toolbar and follow File &gt; Save As and navigate to the desired location in the pop-up file directory window and name the script. To save updated versions of the script, use the main toolbar and follow File &gt; Save Figure 1.12: Annotated RStudio GUI: Save script 1.1.1 Getting and setting the working directory The working directory is the folder on the computer where RStudio will access data and files from and/or save data to without any extra commands or steps. To avoid data and files being saved in an unknown location as well as to allow RStudio to access the data, it is important to know the working directory of your R session. To check the location of the working directory, use the function getwd(). In the console, the working directory file path name will be printed. To set the working directory, you use the function setwd(“location of your data folder”) and include the file path name in the parentheses. Alternatively, the main toolbar menus can be used to set the working directory following the steps below, also demonstrated in the figure below: Click on “Session” Navigate to “Set Working Directory” Navigate to “Choose Working Directory” Select in your computer where you would like your R session files saved Figure 1.13: Annotated RStudio GUI: Set working directory 1.1.2 Creating, opening and saving R projects When working with files in R, it is imperative that the working directory is set at the start of each script in order for R to be able to access the files. However, when working on different computers or working collaboratively with others, the file path will change leading to reproducibility issues. An R Project is essentially a reproducible working directory, preventing the need to use the function setwd() in each script through having all of your work in a self-contained folder with a designated .Rproj file which can be opened and run seamlessly by anybody who has access. To create an R project, use the main toolbar and follow the path File &gt; New Project… &gt; New Directory &gt; New Project, give your directory a name and then click on Create Project. Figure 1.14: Annotated RStudio GUI: Create project Figure 1.15: Annotated RStudio GUI: Create project To open an existing R project, use the main toolbar and follow the path File &gt; Open Project… &gt; and select the desired project from the pop-up file directory window. Figure 1.16: Annotated RStudio GUI: Open project To save an R project, the same method is used as for R scripts, using the main toolbar and following the path File &gt; Save As and navigate to the desired location in the pop-up file directory window and name the project To save updated versions of the project, use the main toolbar and follow File &gt; Save. 1.1.3 Getting help in R There are inbuilt facilities which provide information to help about any specific command or package in R. The first method to get help in R is to run lines of code. For example, to get information about ‘mean’, run any of the following help(mean): displays help in pane 3 ?mean: displays help in pane 3 help.start(mean): displays help as html ??mean: displays help as html Alternatively, use the toolbar of pane 3, navigate to the Help tab and use the search bar to look for the command. Figure 1.17: Annotated RStudio GUI: Help files The help files for R functions and commands contain information on the following sections: Description: A brief description on what the function(s) does and its usage Usage: How to use the function(s) with what arguments need to be supplied and any default values of arguments Arguments: Explanation on what the arguments are Details: More in-depth information on the background of the function(s) Value: Values that the function(s) returns Source: What the function(s) is based on References: Bibliography used for creating function(s) See Also: Links to similar functions that may be of use Examples: Examples on how to use the function(s) If the information provided in the help files is insufficient, Stack Overflow is a great website where individuals can ask and answer questions. There are over 300,000 questions already asked on there so it is likely that the answer to your question has already been asked. For some aspects of R, there are ‘Cheat Sheets’ available, posters which provide the essential, key information on the topic. To access this help, use the main toolbar and follow Help &gt; Cheat Sheets &gt; and select the file which matches your needs best. If the cheat sheet you require is not provided in the list, more options can be found by selecting Browse Cheat Sheets…. Figure 1.18: Annotated RStudio GUI: Help files 1.1.4 R coding best practices 1.1.4.1 Commenting Including comments describing the code is good practice as it ensures that the code can be understood by both yourself in the future and others that the code is shared with. Comments can be added into the code by using the hash key, #, and stops the line being commented from running with the rest of the code. #adding a comment with a hash tag ensures that yourself and others can understand #the code quickly if returned to after some time. X &lt;- c(2, 3, 5, 7, 11, 13, 17, 19, 23, 29) #X contains prime numbers in order Additionally, the hash can be used to create sections within scripts. To create a section, use at least 3 in a row without spaces in between (###). This is good practice for keeping your scripts organised. 1.1.4.2 Naming conventions It is not only important to ensure anything in your code that is assigned a name is clear and identifiable, but also that the name chosen does not conflict with something that already has assignment by R (for example, c is already assigned to a function in R, used for combining values into vectors or lists). Some names are reserved by R, such as the names of existing functions. These names should not be used as your code will overwrite the existing code and the functions/values will no longer work. To ensure that you are not assigning names that are not already taken, you can run the desired name and if it returns any code that you did not write, then R has already reserved that name. However, if it returns an error message, then the name is available for you to use. #check if the name &quot;abs&quot; is available abs #&quot;abs&quot; is not available as it returns code for an existing function ## function (x) .Primitive(&quot;abs&quot;) #check if &quot;absolute&quot; is available instead absolute #returns error message, &quot;absolute&quot; is free to use ## Error: object &#39;absolute&#39; not found #&quot;c&quot; is also not available c ## function (...) .Primitive(&quot;c&quot;) #&quot;pi&quot; is also not available pi ## [1] 3.141593 1.1.4.3 Spacing R ignores spaces provided that they are not in the middle of a command name or operator. As a result, R treats the following as identical lines of code. x&lt;-1+2+3 x &lt;- 1+2+3 x &lt;- 1 + 2 + 3 x &lt;- 1 + 2 + 3 However, if you include the space in the middle of the operator, R will not return the result, but FALSE instead. x &lt; - 1 + 2 + 3 ## [1] FALSE R also ignores indentations at the start of a line, so spaces at the start of a line of code will appear the same as if there are no spaces. This is different to Python. 1.1.4.4 Punctuation If you are putting different R commands on different lines, then there is no need for punctuation. However, if you are putting multiple R commands on the same line, then each command needs to be separated by a semicolon. #separate each R command with a semicolon 2*3; 4+5; 1-6 ## [1] 6 ## [1] 9 ## [1] -5 1.2 Data types and key components There are six common data types: Boolean: logical statements e.g. TRUE, FALSE, T, F Character: letters or non-numbers e.g. “alphabets”, “names”, … Factor: non-numbers (characters) with levels, both ordered and unordered, with a predefined, finite number of values e.g. marital status field only allowing the options of: “single”, “separated”, “married”, “widowed/widower” or “divorced” Integer: whole numbers e.g. 1L, 2L, 100L, … Numeric: 1 and any number with decimals e.g. 1, 1.5, 8.9, … Complex: complex numbers with real and imaginary parts e.g. 3+4i, 1-I, … To check the data type in R, use the class() function. There are three key components in R: Objects: used to store values Functions: allow for manipulations on objects to be performed Operators: used to create interactions between objects or between an object and a function 1.2.1 Basic operations in R 1.2.1.1 Arithmetic operators Operator Description + addition - subtraction * multiplication / division ^ or ** exponentiation %% modulus of a number %/% integer division %*% matrix multiplication 1.2.1.2 Logical operators Operator Description &lt; less than &lt;= less than or equal to &gt; greater than &gt;= greater than or equal to == exactly equal to != not equal to !x not x &amp; and | or isTRUE(x) test if X is TRUE 1.2.1.3 Other operators Operator Description # commenting (do not run line) $ extract value &lt;- assign value to object = equals or assign value to object ~ assign variable to another 1.3 Key functions in R Functions are always indicated with a name followed by (), and anything inside the parentheses is an argument, or option that is passed. Some arguments have a default value, meaning that if no value for that argument is given in the code, the default value will be used. The following table contains some of the more commonly used functions that are available in base R, with a description of what they are used for. Examples on how to use many of these functions are seen later in the module. Function Description log() logarithm exp() exponential abs() absolute value seq() sequence of values sqrt() square root sd() standard deviation var() variance median() median mean() mean quantile() quantiles sum() add all elements together diff() difference min() minimum value/ element max() maximum value/ element range() gives smallest &amp; largest numbers table() tabulation length() length of the vector summary() summarise the entire vector c() combine values into list or vector cbind() combine vectors, matrices or data frame by column rbind() combine vectors, matrices or data frame by row which() gives the TRUE indices of a logical object round() rounds the input to the specified accuracy paste() link vectors after converting to characters paste0() pastes everything as given cat() converts arguments to character strings print() outputs the object(s) given Exercise: Which of the following will give a different output from the other 3? Hint: Look at the help file for the log() function. log(x=6, base=4) log(4, 6) log(base=4, x=6) log(6, 4) 1.4 Data structures There are five data structures: Vector: one dimensional where all elements must be of the same type, either numeric, character or factor Matrix: two dimensional where data elements of the same data type are stored in rows and columns Array: a data structure that contains multiple matrices where the elements are arranged sequentially Data frame: two dimensional and can contain vectors of different data structures and can combine different variables List: a flexible data structure that can contain all other data structures 1.4.1 Vectors A vector can be a sequence of numbers, characters, logical, complex or a combination, and can be created in numerous ways. One way of creating a vector is to use the function c() which combines the values input as arguments into a vector, providing that each value is separated with a comma. #vector which contains the numbers 1, 2, 5, 3 and 7 in that order c(1,2,5,3,7) ## [1] 1 2 5 3 7 If creating a vector that is a string of characters, it is imperative that each character is separated is enclosed by either ' or \". Regions &lt;- c(&quot;Northern&quot;, &quot;Northeastern&quot;, &quot;Western&quot;, &quot;Central&quot;, &quot;Eastern&quot;, &quot;Southern&quot;) Regions ## [1] &quot;Northern&quot; &quot;Northeastern&quot; &quot;Western&quot; &quot;Central&quot; &quot;Eastern&quot; &quot;Southern&quot; A vector can be created to contain a combination of both numbers as strings. mixed_vec &lt;- c(4, &quot;oranges&quot;, 5, &quot;apples&quot;, 12, &quot;carrots&quot;) mixed_vec ## [1] &quot;4&quot; &quot;oranges&quot; &quot;5&quot; &quot;apples&quot; &quot;12&quot; &quot;carrots&quot; Factors are related to vectors, and in R, factors are stored as integer vectors, where there are a finite number of categories and each level (or label) is a character. These can be created with the factor() function, inputting the vector you wish to convert to a factor as the argument. #create factor for marital status marital_status &lt;- factor(c(&quot;single&quot;, &quot;separated&quot;, &quot;married&quot;, &quot;widowed/widower&quot;, &quot;divorced&quot;, &quot;married&quot;, &quot;widowed/widower&quot;, &quot;single&quot;, &quot;single&quot;, &quot;married&quot;)) #transform Regions to factor Regions_fac &lt;- factor(Regions) To access the levels of the factor, you can either just print the variable itself or use the levels() function, inputting the factor variable as the argument. #marital status levels marital_status ## [1] single separated married widowed/widower divorced married ## [7] widowed/widower single single married ## Levels: divorced married separated single widowed/widower #Regions levels levels(Regions_fac) ## [1] &quot;Central&quot; &quot;Eastern&quot; &quot;Northeastern&quot; &quot;Northern&quot; &quot;Southern&quot; &quot;Western&quot; The structure of the factor can be accessed with the str() function, where the output shows the different levels and indicates which level each of the elements of the variable are. #structure of marital status str(marital_status) ## Factor w/ 5 levels &quot;divorced&quot;,&quot;married&quot;,..: 4 3 2 5 1 2 5 4 4 2 Note: the data.frame() function explored later in this module automatically converts character vectors into factors if the argument stringsAsFactors=FALSE is not passed. Another way to create a vector is to use a colon : which generates a regular sequence. a:b creates a sequence of numbers from a to b in increments/steps of either +1 or -1. #vector of a sequence from 1 to 10, going up in increments of 1 1:10 ## [1] 1 2 3 4 5 6 7 8 9 10 #vector of a sequence from 20 to 10, going down in increments of 1 20:10 ## [1] 20 19 18 17 16 15 14 13 12 11 10 #&#39;a&#39; and &#39;b&#39; do not need to be integers: #vector of a sequence from 5.5 to 15.5, going up in increments of 1 5.5:15.5 ## [1] 5.5 6.5 7.5 8.5 9.5 10.5 11.5 12.5 13.5 14.5 15.5 Also to create a vector, the function seq() in R can be used, with the following arguments: from: the starting value (default=1) to: the end value (default=1) by: a numeric value indicating the size of each increment/step (default= (to-from)/(length.out-1)) length.out: a non-negative numeric value indicating the desired length of the sequence #vector of a sequence from 1 to 50, going up in increments of 2 seq(from = 1, to = 51, by = 2) ## [1] 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 #vector of a sequence from 1 to 50, with the sequence of length 20 seq(from = 1, to = 50, length.out = 20) ## [1] 1.000000 3.578947 6.157895 8.736842 11.315789 13.894737 16.473684 19.052632 21.631579 24.210526 ## [11] 26.789474 29.368421 31.947368 34.526316 37.105263 39.684211 42.263158 44.842105 47.421053 50.000000 The function rep() which replicates the value(s) input can also be used to create a vector in R, using the following arguments: x: a number, vector or a factor which you want to replicate times: a non-negative integer or integer-valued vector giving the number of times to repeat x each: a non-negative integer giving the number of times to repeat each element in x in order/consecutively length.out: a non-negative integer to give the value of the desired length of the vector #vector of 1s repeated 10 times rep(x = 1, times = 10) ## [1] 1 1 1 1 1 1 1 1 1 1 #vector of a sequence from 1 to 10, going up in increments of 1, repeated twice rep(x = 1:10, times = 2) ## [1] 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 #vector of a sequence from 1 to 10, going up in increments of 1, where each #element is repeated twice consecutively rep(x = 1:10, each = 2) ## [1] 1 1 2 2 3 3 4 4 5 5 6 6 7 7 8 8 9 9 10 10 #vector of a sequence from 1 to 10, going up in increments of 1, repeated until #the length of the vector is 15 rep(x = 1:10, length.out = 15) ## [1] 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 Exercise: Create a vector that goes from 5 to 25 by increments of 1. A combination of the different vector construction methods can be used at once, for example, the combine function c() can be used within the other functions and can also be used to combine existing vectors. vec1 &lt;- seq(from = 10, to = 100, by = 10) vec2 &lt;- rep(x = 15:10, each = 2) #combine vectors vec1 and vec2 vec3 &lt;- c(vec1, vec2) vec3 ## [1] 10 20 30 40 50 60 70 80 90 100 15 15 14 14 13 13 12 12 11 11 10 10 Exercise: Create another vector that goes from 5 to 25 by increments of 1, but using a different method. Call this vector V1 Elements from vectors can be indexed (selected) using square brackets []. #extract the first (1st) element from vec1 vec1[1] ## [1] 10 #extract the first (1st), fourth (4th) and sixth (6th) element from vec2 vec2[c(1, 4, 6)] ## [1] 15 14 13 #show all elements of vec2 except the 4th and 5th elements vec2[-c(4, 5)] ## [1] 15 15 14 13 12 12 11 11 10 10 #element-wise calculations: multiply the third (3rd) element from vec3 by the #fourth (4th) element of vec2 vec3[3]*vec2[4] ## [1] 420 Exercise: Extract the 5th element of V1. Through indexing elements from vectors, elements within vectors can be updated. mixed_vec ## [1] &quot;4&quot; &quot;oranges&quot; &quot;5&quot; &quot;apples&quot; &quot;12&quot; &quot;carrots&quot; #update the first (1st) element of mixed_vec from 4 to 6 mixed_vec[1] &lt;- 6 #update the second (2nd) and fourth (4th) elements of mixed_vec from oranges #and apples to lemons and bananas mixed_vec[c(2, 4)] &lt;- c(&quot;lemons&quot;, &quot;bananas&quot;) mixed_vec ## [1] &quot;6&quot; &quot;lemons&quot; &quot;5&quot; &quot;bananas&quot; &quot;12&quot; &quot;carrots&quot; Exercise: Remove the 7th element from the vector V1 and call this new vector V2. Vectors can be summarised, inputting the vector into functions different functions. #find the length of vec1 length(vec1) ## [1] 10 #sum the elements of vec2 sum(vec2) ## [1] 150 #find the mean of the elements in vec3 mean(vec3) ## [1] 31.81818 #create a vector vec4 &lt;- c(12, 3, -1, 239, 84 ) #find the mean of the elements in vec4 mean(vec4) ## [1] 67.4 #round the mean of vec4 to the nearest whole number round(x = mean(vec4), digits = 0) ## [1] 67 Logical operators can be used with vectors to return TRUE or FALSE for each element depending on the conditions given. #return TRUE for elements which are both greater than 10 and less than 100, and #FALSE if both conditions are not met vec4 &gt; 10 &amp; vec4 &lt; 100 ## [1] TRUE FALSE FALSE FALSE TRUE Indexing used with logical vectors returns the elements in the vector which meet the conditions given. #both conditions must be true, greater than 10 AND less than 100 vec4[vec4 &gt; 10 &amp; vec4 &lt; 100] ## [1] 12 84 #either condition can be true, greater than 100 OR less than 0 vec4[vec4 &gt; 100 | vec4 &lt; 0] ## [1] -1 239 The function which() can be used in conjunction with logical operators to return the index/indices of the element(s) which meet the conditions given, rather than the logical statements or values. #gives the indices of the elements in vec4 which are greater than 10 which(vec4 &gt; 10) ## [1] 1 4 5 Extensions of the which() function are which.min() and which.max() which respectively give the indices of the elements with the minimum value and the maximum value. #gives index of the element in vec4 which has the smallest value which.min(vec4) ## [1] 3 #gives index of the element in vec4 which has the largest value which.max(vec4) ## [1] 4 Exercise: How many elements in your vector, V1, are greater than 11? 1.4.2 Matrices Used for data storage and often faster to work with than a data frame. Create a matrix using the function matrix() in R, with the following arguments: data: the data that you want to include in the matrix nrow: the number of rows you want the matrix to have ncol: the number of columns you want the matrix to have byrow: logical, if FALSE (the default), the matrix fills by columns, if TRUE, the matrix fills by row dimnames: (optional, NULL by default) input a list of length 2, containing the row and column names respectively #example of creating a 4x4 matrix: mat &lt;- matrix(data = seq(1, 16), nrow = 4, ncol = 4, byrow = TRUE) mat ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 5 6 7 8 ## [3,] 9 10 11 12 ## [4,] 13 14 15 16 #combine two matrices by column: matrix1 &lt;- matrix(data = 1, nrow = 2, ncol = 2) matrix2 &lt;- matrix(data = 2, nrow = 2, ncol = 2) #returns a 2x4 matrix cbind(matrix1, matrix2) ## [,1] [,2] [,3] [,4] ## [1,] 1 1 2 2 ## [2,] 1 1 2 2 Matrices can also be created through combining multiple vectors. #create two vectors of the same length vec_mat_a &lt;- 1:10 vec_mat_b &lt;- 11:20 #use rbind to create a 2x10 matrix vec_mat1 &lt;- rbind(vec_mat_a, vec_mat_b) vec_mat1 ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## vec_mat_a 1 2 3 4 5 6 7 8 9 10 ## vec_mat_b 11 12 13 14 15 16 17 18 19 20 #use cbind to create a 10x2 matrix vec_mat2 &lt;- cbind(vec_mat_a, vec_mat_b) vec_mat2 ## vec_mat_a vec_mat_b ## [1,] 1 11 ## [2,] 2 12 ## [3,] 3 13 ## [4,] 4 14 ## [5,] 5 15 ## [6,] 6 16 ## [7,] 7 17 ## [8,] 8 18 ## [9,] 9 19 ## [10,] 10 20 Exercise: Create a matrix that contains the sequence of numbers from 1 to 16, going up in increments of 1. Let the matrix have 4 rows and have the matrix elements fill by row. Call this matrix M1. To check the dimensions of a matrix, you can use the function dim(). dim(vec_mat1) ## [1] 2 10 dim(vec_mat2) ## [1] 10 2 Using square brackets [,] you can take subsets of matrices, the first element in the square bracket corresponds to the row indexing and the second element corresponds to the column indexing, [row, column]. # a vector of all the first row elements of the matrix mat[1,] ## [1] 1 2 3 4 # a vector of all the first column elements of the matrix mat[,1] ## [1] 1 5 9 13 You can subset multiple rows and multiple columns by indexing using vectors. #a matrix containing the first and second rows of the matrix mat[c(1,2),] ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 5 6 7 8 #a matrix containing the third and fourth columns of the matrix mat[,c(3,4)] ## [,1] [,2] ## [1,] 3 4 ## [2,] 7 8 ## [3,] 11 12 ## [4,] 15 16 Both rows and columns can be indexed for selecting either a single element from a matrix or indexing a selection of rows and columns only. #a single element from the 4th row and the 4th column of the matrix mat[4,4] ## [1] 16 #a matrix containing elements present in the first and second rows AND the #third and fourth columns mat[c(1,2), c(3,4)] ## [,1] [,2] ## [1,] 3 4 ## [2,] 7 8 Exercise: Extract the 3rd and 4th rows and the 1st and 2nd columns of your matrix, M1. Call this matrix M2. As with vectors, through indexing, elements of the matrix can be updated. citrus_mat &lt;- matrix(data = c(&quot;oranges&quot;, &quot;lemons&quot;, &quot;limes&quot;, &quot;apples&quot;), nrow = 2, ncol = 2, byrow = TRUE) citrus_mat ## [,1] [,2] ## [1,] &quot;oranges&quot; &quot;lemons&quot; ## [2,] &quot;limes&quot; &quot;apples&quot; #update the element in the second (2nd) row and the second (2nd) column to also #be a citrus fruit citrus_mat[2,2] &lt;- &quot;pomelo&quot; citrus_mat ## [,1] [,2] ## [1,] &quot;oranges&quot; &quot;lemons&quot; ## [2,] &quot;limes&quot; &quot;pomelo&quot; The functions rowSums() and colSums() are used to find the row sums and column sums respectively, returning a vector of the values. #returns a vector of the row sums rowSums(mat) ## [1] 10 26 42 58 #returns a vector of the column sums colSums(mat) ## [1] 28 32 36 40 The apply() function can be used to obtain row or column summaries of a data matrix, with the following arguments: X: the matrix you wish to summarise MARGIN: a vector that indicates which subscripts to apply the function to. For example, 1 indicates all rows, 2 indicates all columns and c(1,2) indicates rows and columns FUN: the function that is to be applied to the data simplify: logical, if TRUE (the default), results will be simplified if possible #calculates the column sums of the matrix apply(mat, 2, sum) ## [1] 28 32 36 40 #calculates the row means of the matrix apply(mat, 1, mean) ## [1] 2.5 6.5 10.5 14.5 Exercise: Find the row sums of the matrix M2. To multiply two matrices together, the operator %*% can be used, providing that the two matrices conform (the number of columns of one matrix must be equal to the number of rows on the other one). #multiply the two 2x2 matrices together matrix1%*%matrix2 ## [,1] [,2] ## [1,] 4 4 ## [2,] 4 4 1.4.3 Arrays An array contains one or more matrices. Create an array using the function array() in R, with the following arguments: data: a vector containing data to fill the array dim: an integer vector of length one or more giving the dimensions of the array dimnames: a list with one component for each dimension of the array #an array of 3, 3x3 matrices array1 &lt;- array(c(round(seq(1,36, length=9)), round(seq(10,30, length=9)), round(seq(40,200, length=9))), dim=c(nrow=3,ncol=3, 3)) Arrays can be subset in the same way as matrices, using square brackets, except with an extra comma to indicate which of the matrices in the array is to be indexed. #subsets the first matrix in the array array1[,,1] ## [,1] [,2] [,3] ## [1,] 1 14 27 ## [2,] 5 18 32 ## [3,] 10 23 36 1.4.4 Data frames A data frame is a two-dimensional, tabular data type which can store multiple data types in R. It is also the most widely used data format as it can combine different types of variables. You can create a data frame by using the function data.frame(), combining collections of variables. For an example, a data frame can be constructed by creating some dummy/fake data: ##--Create some dummy or fake data #paste0 function pastes everything together, returns region1, ..., region6 regions = paste0(&quot;region&quot;, 1:6) tot_pop &lt;- c(1000000, 920000, 2050000, 3100000, 1535000, 743000) tot_hh &lt;- c(200505, 124000, 882012,1051200, 452000, 79000) male_prop &lt;- c(0.55, 0.49, 0.45, 0.58, 0.56, 0.55) male_pop &lt;- tot_pop*male_prop female_pop &lt;- tot_pop - male_pop and then using the function data.frame(), using the variables created above as the arguments: data_frame1 &lt;- data.frame(Region = regions,Tot_pop = tot_pop,Tot_hh = tot_hh, Male_pop = male_pop, Female_pop = female_pop) data_frame1 ## Region Tot_pop Tot_hh Male_pop Female_pop ## 1 region1 1000000 200505 550000 450000 ## 2 region2 920000 124000 450800 469200 ## 3 region3 2050000 882012 922500 1127500 ## 4 region4 3100000 1051200 1798000 1302000 ## 5 region5 1535000 452000 859600 675400 ## 6 region6 743000 79000 408650 334350 1.4.5 List A list is a vector where each element itself is an object. Informally, it can be described as a ‘bag’ that contains data of different forms, types and dimensions, including other lists. Create a list using the function list() in R, for example: #create a list using the variables, matrices and arrays from above list1 &lt;- list(Admin = Regions, matrix = mat, array = array1) list1 ## $Admin ## [1] &quot;Northern&quot; &quot;Northeastern&quot; &quot;Western&quot; &quot;Central&quot; &quot;Eastern&quot; &quot;Southern&quot; ## ## $matrix ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 5 6 7 8 ## [3,] 9 10 11 12 ## [4,] 13 14 15 16 ## ## $array ## , , 1 ## ## [,1] [,2] [,3] ## [1,] 1 14 27 ## [2,] 5 18 32 ## [3,] 10 23 36 ## ## , , 2 ## ## [,1] [,2] [,3] ## [1,] 10 18 25 ## [2,] 12 20 28 ## [3,] 15 22 30 ## ## , , 3 ## ## [,1] [,2] [,3] ## [1,] 40 100 160 ## [2,] 60 120 180 ## [3,] 80 140 200 Data can be retrieved from a list using the dollar sign $, and subset the retrieved data in the same way as without lists. list1$Admin ## [1] &quot;Northern&quot; &quot;Northeastern&quot; &quot;Western&quot; &quot;Central&quot; &quot;Eastern&quot; &quot;Southern&quot; list1$array1[,,2] ## NULL Logical operators can be used to subset data within a list. #returns TRUE for element(s) which are greater than 20 and FALSE otherwise list1$vector&gt;20 ## logical(0) #returns TRUE for element(s) which are exactly equal to 10 and FALSE otherwise list1$matrix==10 ## [,1] [,2] [,3] [,4] ## [1,] FALSE FALSE FALSE FALSE ## [2,] FALSE FALSE FALSE FALSE ## [3,] FALSE TRUE FALSE FALSE ## [4,] FALSE FALSE FALSE FALSE Using the dollar sign, $, new variables can be added to a list. list1$vector &lt;- vec3 list1 ## $Admin ## [1] &quot;Northern&quot; &quot;Northeastern&quot; &quot;Western&quot; &quot;Central&quot; &quot;Eastern&quot; &quot;Southern&quot; ## ## $matrix ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 5 6 7 8 ## [3,] 9 10 11 12 ## [4,] 13 14 15 16 ## ## $array ## , , 1 ## ## [,1] [,2] [,3] ## [1,] 1 14 27 ## [2,] 5 18 32 ## [3,] 10 23 36 ## ## , , 2 ## ## [,1] [,2] [,3] ## [1,] 10 18 25 ## [2,] 12 20 28 ## [3,] 15 22 30 ## ## , , 3 ## ## [,1] [,2] [,3] ## [1,] 40 100 160 ## [2,] 60 120 180 ## [3,] 80 140 200 ## ## ## $vector ## [1] 10 20 30 40 50 60 70 80 90 100 15 15 14 14 13 13 12 12 11 11 10 10 Lists can be also be created to demonstrate different levels, for example, age ranges. Age &lt;- list(Infant = &quot;0-59 months&quot;, Child = &quot;5 - 12 years&quot;, Teen = &quot;13 - 17 years&quot;, Adult = &quot;18+&quot;) Age ## $Infant ## [1] &quot;0-59 months&quot; ## ## $Child ## [1] &quot;5 - 12 years&quot; ## ## $Teen ## [1] &quot;13 - 17 years&quot; ## ## $Adult ## [1] &quot;18+&quot; Lists can be ‘flattened’ using the function unlist(), which simplifies the list into a vector which contains the same elements. Age_unlist &lt;- unlist(Age) Age_unlist ## Infant Child Teen Adult ## &quot;0-59 months&quot; &quot;5 - 12 years&quot; &quot;13 - 17 years&quot; &quot;18+&quot; 1.5 End of module exercises 1. Use R to find the value of the square root of 25. (\\(\\sqrt(25)\\)) Hint: Look at the help file for the function sqrt(). 2. Use R to find the value of the exponential of \\(6\\times 14 -3\\). (\\(\\exp(6\\times 14-3)\\)) 3. Use R to find the value of the absolute value of \\(7\\times 3 - 4\\times 9 - 30\\). (\\(|7\\times 3 - 4\\times 9 - 30|\\)) 4. Use R to find the value of \\(\\frac{73-42}{3}+2\\times\\left(\\frac{36}{4}-17\\right)\\). Give your answer to 2 decimal places. 5. How many elements does the following vector have? seq(from = 0, to = 100, by = 3.14) 6. Look at the help file for the function rnorm() and use this function to generate 100 random numbers with mean 10 and standard deviation 5. 1.6 Useful resources Stack Overflow The Comprehensive R Archive Network (CRAN) R-bloggers Datacamp W3Schools "],["working-with-data-frames.html", "2 Working with Data Frames 2.1 Importing and exporting data in and from R 2.2 Basic data wrangling methods for data preparation and handling in R 2.3 Data visualisation 2.4 End of module exercises 2.5 Useful resources", " 2 Working with Data Frames This module focuses on working with data frames, covering the installation and loading of data into the R environment, and basic data wrangling methods. Also included in Module 2 is a thorough introduction into data visualisation methods, key foundational knowledge that will be utilised in future modules. 2.1 Importing and exporting data in and from R 2.1.1 Installing and loading packages R uses packages (libraries) to store different functions, and has thousands (16,000) of these packages. The “official” packages for R can be found on CRAN, but there are many other useful packages available on GitHub. Packages are a collection of R functions, code, and occasionally data that enables efficient work in R without the need to write complicated code. Examples of common R packages include: tidyverse: a collection of R packages designed for data manipulation and visualisation, including the following packages ggplot2: designed for elegant data visualisation and graphics dplyr: designed for data manipulation and cleaning tidyr: designed for reshaping and cleaning data readr: designed for reading and writing delimited files purrr: designed for functional programming in R tibble: designed to be a modern reimplementation of data frames stringr: designed for working with strings lubridate: designed for working with dates and times forcats: designed for working with categorical variables sf: designed for working with spatial vector data rstan or INLA: designed for use with Bayesian statistics To utilise the packages in R, it is required to install and load the packages into the session. Some packages are already installed and loaded as part of the base R package, however, for those which are not the functions install.packages() can be used to install packages, ensuring that the name of any packages included are enclosed by either ' or \" and library() can be used to load the package from the library. #install ggplot2 package install.packages(&quot;ggplot2&quot;) #load the ggplot2 package library(ggplot2) Alternatively, available packages can be accessed from the “Packages” window in Pane 3 (bottom-right pane) of the RStudio GUI. In this window, you can either search for the package directly, or scroll through the list and check the tick box of the desired package to load it into R. 2.1.2 Reading data into the R environment Different data formats require different functions in R for importation into R, with some formats requiring additional packages to allow installation. From base R, the function read.table() reads the chosen .txt file included as an argument. However, this function does not ‘read’ the file into the R environment, only prints the file in the console. To import the file into the R environment, you must assign the results to a variable. The other key functions for importing data into R are given in the table below with example code on how to use the functions provided also. Data type Function Package required .csv read.csv() none (in base) .txt, text files read.table() none (in base) .xslx, excel workbook read.excel() readxl .dta, STATA files read.dta() foreign .sav, SPSS files read.spss() foreign #import a text or .txt file text_data&lt;-read.table(file = paste0(data_path,&quot;sample_data.txt&quot;)) #import a .csv file csv_data1 &lt;-read.table(file = paste0(data_path,&quot;sample_data.csv&quot;), sep = &quot;,&quot;, header = TRUE) csv_data2 &lt;-read.csv(file = paste0(data_path,&quot;sample_data.csv&quot;)) #import an Excel Workbook file install.packages(&quot;readxl&quot;) library(readxl) excel_data&lt;-read_excel(file = paste0(data_path,&quot;sample_data.xlsx&quot;)) #import a STATA file install.packages(&quot;foreign&quot;) library(foreign) stata_data&lt;-read.dta(file = paste0(data_path,&quot;sample_data.dta&quot;)) Alternatively to pasting the data path, the function file.choose() can be used, which opens a window on the computer for you to select the file you wish to import. #import a text or .txt file text_data2&lt;-read.table(file = file.choose()) Another option for loading data in R is to use the menus in the main toolbar following File &gt; Import Dataset &gt; From [data format of choice] &gt; [browse for file on the pop-up window] or in the Environment pane following Import Dataset &gt; From [data format of choice] &gt; [browse for file on the pop-up window]. Both menus are shown in Figures 1 and 2 respectively. This practice is not encouraged however, given that it is not hard-coded in the script, it is better practice to use the functions above in the script to import your chosen dataset. Figure 2.1: Import table via Toolbar Figure 2.2: Import data via Environment Exercise: What are the defaults for the header argument in the functions read.table() and read.csv()? header = T in read.table() and header = T in read.csv() header = T in read.table() and header = F in read.csv() header = F in read.table() and header = T in read.csv() header = F in read.table() and header = F in read.csv() 2.1.3 Creating work paths Setting the working directory to a single folder means that R can only access files within that folder and cannot open any folders within a folder. If you have several folders within a folder that need accessing, or need your input and output files and immediate and final results stored in different folders. In these instances, rather than setting a working directory it can be better to store the path to folder as a string, setting a ‘parent directory’. Then use the function read.table() to import the data, which can read both standard .txt and .csv data setting a work path allows for you to set a ‘parent directory’ which solves this problem. This can be created through setting a path to the parent directory. #parent directory path &lt;- &quot;C:Users/YOUR_NAME&quot; #save in a folder called &quot;training&quot; out_path &lt;- paste0(path, &quot;/training&quot;) if(!file.exists(out_path)) { dir.create(file.path(out_path)) } You can also create a work path using the current working directory as the parent directory. #working directory as parent directory out_path &lt;- paste0(getwd(), &quot;/training&quot;) if(!file.exists(out_path)) { dir.create(file.path(out_path)) } You can set your parent directory to be your working directory as follows. #set parent directory to working directory if(file.exists(out_path)){ setwd(file.path(out_path)) }else{ dir.create(file.path(out_path)) setwd(file.path(out_path)) } 2.1.4 Exploring data attributes Function Description class() class of the variable colnames() gives the column names describe() summary of data - requires psych package dim() gives dimensions of the data factor() change to a factor glimpse() view data structure head() gives first six (6) rows of data head(mydata, n) gives first n (6) rows of data called mydata headtail() first and last rows of data - requires psych package length() length of a vector ls() lists all items available in the R environment names() gives column or variable names ncol() number of columns nrow() number of rows row() gives the row names rm() removes selected items from the R environment str() gives full structure of data summary() summary of a vector table() frequency table of a vector tail() gives last six (6) rows of data unique() unique values of a vector View() view data #&#39;mtcars&#39; is a dataset available in the &#39;datasets&#39; package with data on #11 different aspects of auto mobiles for 32 auto mobiles from the 1974 Motor #Trend US magazine library(datasets) #information on the dataset in the &#39;Help&#39; pane ?mtcars #load data and assign to &#39;cars_data&#39; cars_data &lt;- mtcars #dimension of data dim(cars_data) ## [1] 32 11 #variable names of data names(cars_data) ## [1] &quot;mpg&quot; &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; &quot;drat&quot; &quot;wt&quot; &quot;qsec&quot; &quot;vs&quot; &quot;am&quot; &quot;gear&quot; &quot;carb&quot; #first 6 rows of data head(cars_data) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 #last 6 rows of data tail(cars_data) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.7 0 1 5 2 ## Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.9 1 1 5 2 ## Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.5 0 1 5 4 ## Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.5 0 1 5 6 ## Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.6 0 1 5 8 ## Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.6 1 1 4 2 #install psych package install.packages(&quot;psych&quot;) #load psych package library(psych) #psych package provides more flexible alternatives to functions in base R describe(cars_data) ## vars n mean sd median trimmed mad min max range skew kurtosis se ## mpg 1 32 20.09 6.03 19.20 19.70 5.41 10.40 33.90 23.50 0.61 -0.37 1.07 ## cyl 2 32 6.19 1.79 6.00 6.23 2.97 4.00 8.00 4.00 -0.17 -1.76 0.32 ## disp 3 32 230.72 123.94 196.30 222.52 140.48 71.10 472.00 400.90 0.38 -1.21 21.91 ## hp 4 32 146.69 68.56 123.00 141.19 77.10 52.00 335.00 283.00 0.73 -0.14 12.12 ## drat 5 32 3.60 0.53 3.70 3.58 0.70 2.76 4.93 2.17 0.27 -0.71 0.09 ## wt 6 32 3.22 0.98 3.33 3.15 0.77 1.51 5.42 3.91 0.42 -0.02 0.17 ## qsec 7 32 17.85 1.79 17.71 17.83 1.42 14.50 22.90 8.40 0.37 0.34 0.32 ## vs 8 32 0.44 0.50 0.00 0.42 0.00 0.00 1.00 1.00 0.24 -2.00 0.09 ## am 9 32 0.41 0.50 0.00 0.38 0.00 0.00 1.00 1.00 0.36 -1.92 0.09 ## gear 10 32 3.69 0.74 4.00 3.62 1.48 3.00 5.00 2.00 0.53 -1.07 0.13 ## carb 11 32 2.81 1.62 2.00 2.65 1.48 1.00 8.00 7.00 1.05 1.26 0.29 2.1.5 Viewing data Data imported into R can be viewed in the Environment pane, with the column/variable names given, the data type of each variable and the first 10 observations (provided the observations do not go ‘off’ the display due to length) for each variable given as a summary of the data structure. If the summary is not given, ensure that the display of the Environment is set to ‘list’ and click the arrow to the left of the dataset name. Figure 2.3: View data in Environment Alternatively, the function View() can be used to display the full data in a new tab in the first (1st) pane, adjacent to the script. Figure 2.4: View data in new tab 2.1.6 Extracting data As with the different data structures in R such as vectors and data frames, data can be extracted from a dataset using square brackets, [], known as sub-setting or slicing. Additionally, the dollar sign, $ can be used to extract variables/access columns from a dataset. #subset the data to only show the rows which have mpg greater than 25 cars_data[cars_data$mpg&gt;25,] ## mpg cyl disp hp drat wt qsec vs am gear carb ## Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 ## Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 ## Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 ## Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 ## Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 ## Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 Data can also be sliced by a given index using a colon, :, to represent a given range. #return the fifth (5th) to the tenth (10th) rows of the data cars_data[5:10, ] ## mpg cyl disp hp drat wt qsec vs am gear carb ## Hornet Sportabout 18.7 8 360.0 175 3.15 3.44 17.02 0 0 3 2 ## Valiant 18.1 6 225.0 105 2.76 3.46 20.22 1 0 3 1 ## Duster 360 14.3 8 360.0 245 3.21 3.57 15.84 0 0 3 4 ## Merc 240D 24.4 4 146.7 62 3.69 3.19 20.00 1 0 4 2 ## Merc 230 22.8 4 140.8 95 3.92 3.15 22.90 1 0 4 2 ## Merc 280 19.2 6 167.6 123 3.92 3.44 18.30 1 0 4 4 Through extracting variables from the datasets, the summary statistic functions for vectors can be used. #quantiles for the hp variable quantile(cars_data$hp) ## 0% 25% 50% 75% 100% ## 52.0 96.5 123.0 180.0 335.0 #median value of cyl median(cars_data$cyl) ## [1] 6 The dollar sign doesn’t just extract variables from the data, but can be used to create new variables as with data frames. #create a variable for miles per litre by multiplying mpg by 0.22 (1 litre is #approximately 0.22 gallons) cars_data$mpl &lt;- cars_data$mpg*0.22 2.1.7 Attaching data When working with variables from one dataset in R, it can become tedious to always require the dollar sign, $, to access the variables. The function attach() in R stops the need for this through essentially copying each of the variables from the dataset into the Environment to be used freely, without needing to quote the dataset each time. #cannot compute the mean of mpg from cars_data without extracting it first mean(mpg)#returns error ## Error: object &#39;mpg&#39; not found mean(cars_data$mpg)#works ## [1] 20.09062 #attach the dataset to the Environment attach(cars_data) mean(mpg) #now works ## [1] 20.09062 If you want to detach the data for any reason, for example if you want to attach another dataset, you can use the detach() function in R. #detach data detach(cars_data) mean(mpg) #now does not work again ## Error: object &#39;mpg&#39; not found 2.1.8 Writing data to external repositories Different data formats require different functions in R for exporting from R, with some formats requiring additional packages to allow for exportation. Since the data visible in the Environment (pane 4) is stored in the RAM, and not in the physical memory, it is important that any data you wish to save is written to an external repository, such as writing the data to a .csv file, otherwise it is erased when the R session ends. The key functions used to write the data as the different (common) data types are given in the table below with examples in the code that follows. Data type Function Package required .csv write.csv() none (in base) .txt, text files write.table() none (in base) .xslx, excel workbook write.xlsx() writexl .dta, STATA files write.dta() foreign #export a .csv file write.csv(sample_data, &quot;exported_data.csv&quot;) #export a text or .txt file write.table(sample_data, &quot;exported_data.txt&quot;) #export an Excel file install.packages(&quot;writexl&quot;) library(writexl) write_xlsx(sample_data, &quot;exported_data.xlsx&quot;) #export a STATA file install.packages(&quot;foreign&quot;) library(foreign) write.dta(sample_data, &quot;exported_data.dta&quot;) Note: The entire R Environment can be saved using the save.image() function or by clicking on the “save icon” (floppy disk icon) on the toolbar of the Environment window (pane 4) and selecting where you wish to save the .Rdata in the pop-up window. 2.2 Basic data wrangling methods for data preparation and handling in R 2.2.1 Dealing with NA (‘Not Available’) values NA is typically used to represent for missing (‘Not Available’) values in R, and is an important concept, particularly as R replaces missing values with NA (and replaces impossible values, such as when dividing by zero, with NaN). #import population data Pop_data &lt;- read.csv(file = paste0(data_path, &quot;sim_population.csv&quot;), header = TRUE) #variable for gdp contains missing values, first 50 values Pop_data$gdp[1:50] ## [1] 8 6 2 2 10 2 10 7 9 9 3 9 9 8 3 12 NA 2 6 3 NA 4 NA 8 8 1 9 14 6 15 7 10 2 8 ## [35] 2 14 5 6 7 12 12 1 3 4 15 4 3 13 7 8 #trying to sum the elements of the variable does not work as R cannot deal with #the missing value sum(Pop_data$gdp) ## [1] NA If there is missing data in the variable/vector that you are trying to work with, the function na.rm = TRUE can be applied, which removes the missing data from the variable/vector you are working with. #remove NA values for sum sum(Pop_data$gdp, na.rm = TRUE) ## [1] 10831 To identify which elements are missing from an R object, the function is.na() can be used, where the argument is chosen to be the object of interest. If you would like to know how many missing values there are, you can use the sum() function in addition to the is.na() function as shown below. #identify the missing values in the gdp variable (first 50 values) is.na(Pop_data$gdp[1:50]) ## [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [18] FALSE FALSE FALSE TRUE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [35] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #how many missing values in first 50 values? sum(is.na(Pop_data$gdp[1:50])) ## [1] 3 Alternatively, if you would like to know which of the values are complete (not missing), the function complete.cases() can be used in the same way, but returns a logical vector indicating which of the elements which are complete. #identify the missing values in the gdp variable (first 50 values) complete.cases(Pop_data$gdp[1:50]) ## [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE ## [18] TRUE TRUE TRUE FALSE TRUE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [35] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE #how many missing values in first 50 values? sum(complete.cases(Pop_data$gdp[1:50])) ## [1] 47 Another useful function is the na.omit() function, which instead of returning a logical vector identifying which values are complete or missing, it returns the object with the missing values removed. #remove the missing values gdp_na_omit &lt;- na.omit(Pop_data$gdp[1:50]) gdp_na_omit ## [1] 8 6 2 2 10 2 10 7 9 9 3 9 9 8 3 12 2 6 3 4 8 8 1 9 14 6 15 7 10 2 8 2 14 5 ## [35] 6 7 12 12 1 3 4 15 4 3 13 7 8 ## attr(,&quot;na.action&quot;) ## [1] 17 21 23 ## attr(,&quot;class&quot;) ## [1] &quot;omit&quot; To quickly identify if there are missing values in your data frame/object, the na.fail() function can be used. With this function, if the object included as the argument contains any missing values, it will signal an error, however, if there are no missing values, the object is returned. #check for missing values #na.fail(Pop_data$gdp[1:50]) #returns an error na.fail(gdp_na_omit) #returns an error ## [1] 8 6 2 2 10 2 10 7 9 9 3 9 9 8 3 12 2 6 3 4 8 8 1 9 14 6 15 7 10 2 8 2 14 5 ## [35] 6 7 12 12 1 3 4 15 4 3 13 7 8 ## attr(,&quot;na.action&quot;) ## [1] 17 21 23 ## attr(,&quot;class&quot;) ## [1] &quot;omit&quot; 2.2.2 Key data wrangling methods Some key data manipulation can be done using the following functions in base, stats, dplyr and tidyr packages. #install dplyr and tidyr packages install.packages(&quot;dplyr&quot;) install.packages(&quot;tidyr&quot;) #load the dplyr and tidyr packages library(dplyr) library(tidyr) Function Description Package aggregate() obtain summary statistics by group stats arrange() duplicate existing variable in the data with new name dplyr drop_na() remove all columns with NA tidyr filter() Select rows based on certain conditions dplyr gather() convert to long format tidyr group_by() group the rows of a data frame by one or more variables dplyr merge() join two datasets base mutate() add new columns to a data frame based on existing columns dplyr select() choose specific columns to work with dplyr spread() convert to wide format tidyr subset() subset data by given specifications base summarise()/ summarize() compute summary statistics for each group created by group_by() dplyr rename() change the names of columns in a data frame dplyr The pipe operator, %&gt;%, from the magritr package allows for a sequence of operations to be carried out simultaneously. R offers a built in native pipe, |&gt;, which works in a similar way and can be faster for longer computations. The main difference between the functions is that %&gt;% does not require parentheses (brackets) when calling functions, whereas for |&gt;, the parentheses are always necessary. The with() function in base R performs similar actions. #rename dataset and remove NA values Pop_data_pipe &lt;- Pop_data %&gt;% drop_na() head(Pop_data_pipe) ## ID pop precipitation_mean temperature_mean elevation slope region buildings gdp ## 1 1 164 1581.950 24.00823 650.27273 0.9090909 1 250 8 ## 2 2 192 1628.924 25.07551 364.75000 1.5000000 5 374 6 ## 3 3 203 1072.788 25.11997 107.63636 3.5454545 2 418 2 ## 4 4 206 1219.166 25.29793 340.75000 1.5000000 2 327 2 ## 5 5 226 1117.308 25.17577 90.68421 6.5263158 2 443 10 ## 6 6 229 1401.864 25.61566 305.71429 7.2857143 1 503 2 Subset rows based on certain conditions. #subset observations with a population of more than 10 million people Pop_data %&gt;% filter(pop&gt;=2000) ## ID pop precipitation_mean temperature_mean elevation slope region buildings gdp ## 1 914 2008 1432.101 25.85989 300.5000 1.3750000 1 3061 NA ## 2 915 2047 1469.931 25.95289 292.1111 0.4444444 1 3897 73 ## 3 916 2174 1437.897 25.88929 307.3333 1.1111111 1 4440 48 ## 4 917 2233 1425.850 25.84749 317.0000 2.0000000 1 4766 22 ## 5 918 2329 1420.629 25.82820 308.8750 2.3750000 1 5294 NA ## 6 919 2369 1425.794 25.84525 308.5833 1.3333333 1 4923 NA Select certain variables. #select the variables state and total head(Pop_data %&gt;% select(region, pop)) ## region pop ## 1 1 164 ## 2 5 192 ## 3 2 203 ## 4 2 206 ## 5 2 226 ## 6 1 229 Group observations by variable and summarise by property. #group the observations by region and summarise their population per region Pop_data%&gt;% group_by(region) %&gt;% summarise(regional_population = sum(pop)) ## # A tibble: 5 × 2 ## region regional_population ## &lt;int&gt; &lt;int&gt; ## 1 1 270962 ## 2 2 113832 ## 3 3 90361 ## 4 4 45677 ## 5 5 34583 Adding another variable. #add another variable called density = total/buildings Pop_data &lt;- Pop_data %&gt;% mutate(density = pop/buildings) Renaming variables. #rename density to population_density Pop_data&lt;- Pop_data %&gt;% rename(Pop_density = density) Sort data into ascending or descending order. #sort data by ascending order of ID Pop_data_arranged_asc &lt;- arrange(Pop_data, ID) #sort data by descending order of ID Pop_data_arranged_desc &lt;- arrange(Pop_data, desc(ID)) 2.3 Data visualisation Data visualisation is important as it forms part of the exploratory data analyses, enabling a quick and easy understanding of the dataset. It also enables for patterns to be detected and can be a useful tool for communicating results to non-experts. Data visualisation is also crucial to data exploration. For using base R data visualisation functions, with functions such as plot(), boxplot(), and hist(), the following arguments are useful for customisation of the plots, often making the plots easier to understand. Argument Description main an overall title xlab a title for the \\(x\\)-axis ylab a title for the \\(y\\)-axis sub a sub-title type determines the type of plot e.g. “p” for points, “l” for lines pch determines type of symbol to be used for points col colour of plot lty determines the type of line e.g. “1” for solid, “2” for dashed lwd determines width of the line Further arguments for graphical parameters can be found by going to the help file for par (?par). The ggplot2 and ggpubr packages can be used instead of base R to create more elegant plots. #install ggplot2 and ggpubr packages install.packages(&quot;ggplot2&quot;) install.packages(&quot;ggpubr&quot;) #load the ggplot2 and ggpubr packages library(ggplot2) library(ggpubr) 2.3.1 Bar plots A bar plot can be useful for discrete data. #create a vector which has the values of total population by region regional_population &lt;- c() for(r in 1:5){ regional_population[r] &lt;- sum(Pop_data$pop[Pop_data$region == r]) } #plot the bar plot using base R barplot(regional_population, ylab = &quot;Population Total&quot;, xlab = &quot;Region&quot;, col = &quot;#004C92&quot;, main = &quot;Bar Plot of Population by Region&quot;, names.arg = c(1:5)) mtext(side = 3, &quot;Using base R&quot;) Using the function ggplot() with the argument geom_bar() creates a bar plot. The following example is the same plot as above, but with the depicted information easier to understand. #bar plot of population by region barplot1 &lt;- ggplot(Pop_data, aes(x = region, y = pop, fill = pop)) + geom_bar(stat = &quot;identity&quot;) + scale_fill_gradient(low = &quot;yellow&quot;, high = &quot;red&quot;) + labs(x = &quot;States&quot;, y = &quot;Population Total&quot;, fill = &quot;Total&quot;, subtitle = &quot;Using ggplot2&quot;) + ggtitle(&quot;Bar Plot of Population by Region&quot;) #use the ggpar function to customise plot ggpar(barplot1, xlab = &quot;Region&quot;, ylab = &quot;Population Total&quot;, legend = &quot;right&quot;, legend.title = &quot;Total&quot;, font.x = c(16), font.y = c(16), subtitle = &quot;Using ggplot2 and ggpubr&quot;) #import the age dataset Age_data &lt;- read.csv(file = paste0(data_path, &quot;Age_data.csv&quot;), header = TRUE) #convert the age groups variable into a factor Age_data$age_groups &lt;- factor(Age_data$age_groups, levels = c(&quot;0-4&quot;, &quot;5 - 12&quot;, &quot;13 - 18&quot;, &quot;19 - 24&quot;, &quot;25 - 34&quot;, &quot;35 - 44&quot;, &quot;45 - 54&quot;, &quot;55 - 64&quot;, &quot;65 - 74&quot;, &quot;75 - 84&quot;, &quot;85 - 94&quot;, &quot;95+&quot;)) #convert data into long format age_long &lt;- gather(Age_data, Sex, Pop,females : males) age_long$ID &lt;- paste0(&quot;ID&quot;, 1:nrow(age_long)) A type of bar plots are descending and ascending bar plots, displaying the data in order of descending or ascending order of the \\(y\\) variable rather than the given order of the \\(x\\) variable. #making descending and ascending bar plots #descending order barplot_descending &lt;- ggbarplot(age_long, x = &quot;ID&quot;, y = &quot;Pop&quot;, fill = &quot;Sex&quot;, color = &quot;white&quot;, palette = &quot;jco&quot;, sort.val = &quot;desc&quot;, sort.by.groups = FALSE, x.text.angle = 90, main = &quot;Descending Bar Plot of Population by Sex&quot;, subtitle = &quot;Using ggpubr&quot; ) #use the ggpar function to customise plot barplot_descending2 &lt;- ggpar(barplot_descending, xlab = &quot;Subject ID&quot;, ylab = &quot;Population Count&quot;, legend = &quot;right&quot;, legend.title = &quot;Gender&quot;, font.label = list(size = 15, face = &quot;bold&quot;, color = &quot;red&quot;), font.x = c(16), font.y = c(16)) barplot_descending2 #ascending order barplot_ascending &lt;- ggbarplot(age_long, x = &quot;ID&quot;, y = &quot;Pop&quot;, fill = &quot;Sex&quot;, color = &quot;white&quot;, palette = &quot;jco&quot;, sort.val = &quot;asc&quot;, sort.by.groups = FALSE, x.text.angle = 90, main = &quot;Ascending Bar Plot of Population by Sex&quot;, subtitle = &quot;Using ggpubr&quot; ) #use the ggpar function to customise plot barplot_ascending2 &lt;- ggpar(barplot_ascending, xlab = &quot;Subject ID&quot;, ylab = &quot;Population Count&quot;, legend = &quot;right&quot;, legend.title = &quot;Gender&quot;, font.label = list(size = 15, face = &quot;bold&quot;, color = &quot;red&quot;), font.x = c(16), font.y = c(16)) barplot_ascending2 #rotate plots ggbarplot(age_long, x = &quot;ID&quot;, y = &quot;Pop&quot;, fill = &quot;Sex&quot;, # change fill colour by mpg_level color = &quot;white&quot;, # Set bar border colours to white palette = &quot;jco&quot;, # jco journal colour palette. see ?ggpar sort.val = &quot;desc&quot;, # Sort the value in descending order sort.by.groups = FALSE, # Don&#39;t sort inside each group x.text.angle = 90, # Rotate vertically x axis texts ylab = &quot;Population Count&quot;, legend.title = &quot;Sex&quot;, rotate = TRUE, ggtheme = theme_minimal(), main = &quot;Ascending Bar Plot of Population by Sex&quot;, subtitle = &quot;Using ggpubr&quot; ) 2.3.2 Box plots A box plot is the simplest plot for displaying continuous (numerical) data and can show the numerical data by category (for example, separate box plots for male vs female). #plot the box plot using base R boxplot(Pop_data$pop~Pop_data$region, ylab = &quot;Population&quot;, xlab = &quot;Region&quot;, main = &quot;Box Plot of Population&quot;) mtext(side = 3, &quot;Using base R&quot;) #plot the box plot using ggplot ggplot(Pop_data, aes(x = region, y = pop, group = region, fill = region)) + geom_boxplot(fill = c(&quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F0E442&quot;, &quot;#0072B2&quot;), alpha = 0.75) + labs(x = &quot;Region&quot;, y = &quot;Population&quot;, subtitle = &quot;Using ggplot2&quot;) + ggtitle(&quot;Box Plot of Population&quot;) 2.3.3 Histograms Alternatively, a histogram can be utilised, displaying the frequency distribution of a variable. #plot the histogram using base R, adding titles in the same way as for bar plots, #and changing the width of the bars. hist(Pop_data$pop, main = &quot;Histogram of Population&quot;, xlab = &quot;Population&quot;) mtext(side = 3, &quot;Using base R&quot;) #plot the histogram using ggplot ggplot(Pop_data, aes(x = pop)) + geom_histogram(bins = 10, color = &quot;white&quot;, fill = &quot;#004C92&quot;) + scale_fill_manual(values = &quot;#004C92&quot;) + labs(x = &quot;Population&quot;, y = &quot;Frequency&quot;, subtitle = &quot;Using ggplot2&quot;) + ggtitle(&quot;Histogram of Population&quot;) The \\(x\\)-axis is split into ‘bins’ (or ‘cells’) and R counts the amount of times the variable is in each bin, with the width of each bar on the histogram changing depending on how many ‘bins’ there are. Some ‘bins’ are not visible due to being empty, for example, on the ggplot2 histogram above, there are 10 ‘bins’ but 2 are empty so only 8 are visible. Sometimes, the width of each of the bars is too wide or too narrow to be able to interpret the results effectively. In these instances, you can use the arguments breaks = for plotting with base R, and bins = for plotting with ggplot2. #plot the histogram using base R, changing the width of the bars. hist(Pop_data$pop, breaks = 20, main = &quot;Histogram of Population&quot;, xlab = &quot;Population&quot;) mtext(side = 3, &quot;Using base R&quot;) #plot the histogram using ggplot ggplot(Pop_data, aes(x = pop)) + geom_histogram(bins = 20, color = &quot;white&quot;, fill = &quot;#004C92&quot;) + scale_fill_manual(values = &quot;#004C92&quot;) + labs(x = &quot;Population&quot;, y = &quot;Frequency&quot;, subtitle = &quot;Using ggplot2&quot;) + ggtitle(&quot;Histogram of Population&quot;) The histograms do not just have to display the frequency of the variable of interest. By adding the argument freq = FALSE to the hist function, the density of the variable of interest is plotted instead. #plot the histogram using base R for density hist(Pop_data$pop, freq = FALSE, breaks = 20, main = &quot;Histogram of Population&quot;, xlab = &quot;Population&quot;) mtext(side = 3, &quot;Using base R&quot;) #import height dataset Height_data &lt;- read.csv(file = paste0(data_path, &quot;Height_data.csv&quot;), header = TRUE) # Make histogram plot hist1 &lt;- gghistogram(Height_data, x = &quot;Height&quot;, add = &quot;mean&quot;, rug = TRUE, color = &quot;Sex&quot;, fill = &quot;Sex&quot;, palette = c(&quot;#FF0000&quot;, &quot;#FFC600&quot;), bins = 15, xlab = &quot;Height (cm)&quot;, ylab = &quot;Frequency&quot;, main = &quot;Histogram of Height by Sex&quot;, subtitle = &quot;Using ggpubr&quot;) hist1 2.3.4 Density plots Density plots are smoothed histograms. In base R, the function density() can be used to find the density of the variable of interest and be inserted straight into the plot() function. #plot the density plot using base R plot(density(Pop_data$pop), main = &quot;Density plot of Population&quot;, xlab = &quot;Population&quot;, ylab = &quot;Density&quot;) mtext(side = 3, &quot;Using base R&quot;) #plot the density plot using ggplot ggplot(Pop_data, aes(x = pop)) + geom_density(fill = &quot;#004C92&quot;, alpha = 0.2, color = &quot;#004C92&quot;) + scale_color_manual(values = &quot;#004C92&quot;) + labs(x = &quot;Population&quot;, color = &quot;Density&quot;, subtitle = &quot;Using ggplot2&quot;) + ggtitle(&quot;Density Plot of Population&quot;) The function ggdensity within ggpubr can also be used for plotting the density of a variable, including plotting the density categorised by another variable, for example plotting the density of a population by ‘Sex’. #make density plot density1 &lt;- ggdensity(Height_data, x = &quot;Height&quot;, add = &quot;mean&quot;, rug = TRUE, color = &quot;Sex&quot;, fill = &quot;Sex&quot;, palette = c(&quot;#FF0000&quot;, &quot;#FFC600&quot;), xlab = &quot;Height (cm)&quot;, ylab = &quot;Density&quot;, main = &quot;Density Plot of Height by Sex&quot;, subtitle = &quot;Using ggpubr&quot;) density1 2.3.5 Scatter plots Scatter plots are useful for showing multivariate data, for investigating the relationship between two continuous variables. A line of best fit can be added to the scatter plot, aiding in the visualisation of the relationship through using the functions lm() to model the relationship and abline() to add the line to the plot in base R. Alternatively, the function geom_smooth() can be used with argument method = \"lm\" for plotting with ggplot2. Modelling the relationship between variables is discussed further and in more detail in Module 4. #scatter plot of temperature and rainfall plot(Pop_data$temperature_mean, Pop_data$precipitation_mean, xlab = &quot;Temperature&quot;, ylab = &quot;Rainfall&quot;, main = &quot;Scatter Plot of Temperature and Rainfall&quot;, pch = 20) mtext(side = 3, &quot;Using base R&quot;) abline(lm(precipitation_mean ~ temperature_mean, data = Pop_data)) #plot the scatter plot and fit a line using ggplot ggplot(Pop_data, aes(x = temperature_mean, y = precipitation_mean)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;red&quot;) + labs(x = &quot;Temperature&quot;, y = &quot;Rainfall&quot;, subtitle = &quot;Using ggplot2&quot;) + ggtitle(&quot;Scatter Plot of Temperature and Rainfall&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; #points with regression line ggplot(data = Pop_data, aes(x = elevation, y = precipitation_mean))+ geom_point(col = &quot;red&quot;)+ geom_smooth(method = &quot;lm&quot;)+ theme_bw()+ labs(x = &quot;Elevation&quot;, y = &quot;Mean Precipitation&quot;, subtitle = &quot;Using ggplot2&quot;)+ ggtitle(&quot;Scatter Plot of Elevation and Mean Precipitation&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; 2.3.6 Violin plots Violin plots are similar to box plots and are useful for the comparison of probability distributions through showing the different probability densities at different values #violin plot ggplot(data = Height_data, aes(x = Sex, y = Height))+ geom_violin(aes(fill = Sex), alpha = 0.75)+ fill_palette(c(&quot;#FF0000&quot;, &quot;#FFC600&quot;)) + labs(y = &quot;Height (cm)&quot;, subtitle = &quot;Using ggplot2&quot;)+ ggtitle(&quot;Violin Plot of Height by Sex&quot;)+ theme_bw() 2.3.7 Line graphs #points connected with line ggplot(data = Pop_data, aes(x = elevation, y = precipitation_mean))+ geom_point(col = &quot;red&quot;)+ geom_line()+ theme(axis.text.x = element_text(size = 15), axis.text.y = element_text(size = 15), axis.line = element_line(color = &quot;black&quot;))+ labs(x = &quot;Elevation&quot;, y = &quot;Mean Precipitation&quot;, subtitle = &quot;Using ggplot2&quot;)+ ggtitle(&quot;Line Graph of Elevation and Mean Precipitation&quot;) 2.3.8 Population pyramids Population pyramids are an effective plot for comparing the structures of different populations, for example, age structure by gender. #create population pyramid for age by sex pyramid1 &lt;- ggplot(Age_data) + geom_col(aes(x = age_groups, y = females, fill = &quot;red&quot;)) + geom_col(aes(x = age_groups, y = -males, fill = &quot;blue&quot;)) + scale_y_continuous(breaks = seq(-200000, 200000, 50000), labels = paste0(as.character(c(seq(200, 0, -50), seq(50, 200, 50))))) + coord_flip() + scale_fill_discrete(name = &quot;Gender&quot;, labels = c(&quot;Female&quot;, &quot;Male&quot;))+ labs(x = &quot;Age groups&quot;, y = &quot;Population (in 1000s)&quot;, subtitle = &quot;Using ggplot2&quot;)+ ggtitle(&quot;Population Pyramid for Age by Sex&quot;)+ theme_minimal() #use the ggpar function to customise plot pyramid2 &lt;- ggpar(pyramid1, legend = &quot;right&quot;, legend.title = &quot;Gender&quot;, font.label = list(size = 16, face = &quot;bold&quot;, color =&quot;red&quot;), font.x = c(16), font.y = c(16), subtitle = &quot;Using ggplot2 and ggpubr&quot;, xtickslab.rt = 45, ytickslab.rt = 45) pyramid2 2.3.9 Lollipop plots A lollipop plot is a subset of bar plots where instead of bars, the data is represented by a line and a dot. #create lollipop plot for population lollipop1 &lt;- ggdotchart(age_long, x = &quot;ID&quot;, y = &quot;Pop&quot;, color = &quot;age_groups&quot;, #palette = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;), sorting = &quot;ascending&quot;, add = &quot;segments&quot;, dot.size = 10, #large dot size label = round(age_long$Pop/10000), font.label = list(color = &quot;white&quot;, size = 12, vjust = 0.5), ggtheme = theme_pubr(), subtitle = &quot;Using ggpubr&quot;, main = &quot;Lollipop Plot for Population&quot; ) #adjust lollipop plot with ggpar lollipop2 &lt;- ggpar(lollipop1, xlab = &quot;Subject ID&quot;, ylab = &quot;Population Count&quot;, legend = &quot;right&quot;, legend.title = &quot;Age groups&quot;, font.label = list(size = 15, face = &quot;bold&quot;, color =&quot;red&quot;), font.x = c(16), font.y = c(16), xtickslab.rt = 45, ytickslab.rt = 45) #change size of legend points and axis text to fit in plot lollipop2 + guides(color = guide_legend(override.aes = list(size = 1.5))) + theme(axis.text = element_text(size = 6)) 2.3.10 Plots with an image background #create a plot with an image background img1 &lt;- png::readPNG(paste0(pic_path,&quot;worldpoplogo.png&quot;)) ggplot(Pop_data, aes(x = pop)) + background_image(img1)+ geom_histogram(bins = 20, color = &quot;white&quot;, fill = &quot;black&quot;, alpha = 0.75) + scale_fill_manual(values = &quot;black&quot;) + labs(x = &quot;Population&quot;, y = &quot;Frequency&quot;, subtitle = &quot;Using ggplot2&quot;) + ggtitle(&quot;Histogram of Population&quot;) 2.3.11 Viewing and saving plots You can view current and previous plots by looking to pane 3 on RStudio, navigating to the Plots tab and using the arrows to flip between plots. Figure 2.5: View plots You can also view multiple plots on one window/page by using the function par() with argument mfrow which divides the plot window into rows and columns. #have a window with 4 plots, 2 in each row and 2 in each column par(mfrow = c(2,2)) boxplot(Pop_data$pop~Pop_data$region, ylab = &quot;Population&quot;, xlab = &quot;Region&quot;, main = &quot;Box Plot of Population by Region&quot;) hist(Pop_data$pop, breaks = 20, main = &quot;Histogram of Population&quot;, xlab = &quot;Population&quot;) barplot(regional_population, ylab = &quot;Population Total&quot;, xlab = &quot;Region&quot;, col = &quot;#004C92&quot;, main = &quot;Bar Plot of Population by Region&quot;, names.arg = c(1:5)) plot(Pop_data$temperature_mean, Pop_data$precipitation_mean, xlab = &quot;Temperature&quot;, ylab = &quot;Rainfall&quot;, main = &quot;Scatter Plot of Temperature and Rainfall&quot;, pch = 1) Also using the function par(), the size of the margins of the plots can be changed, using the argument mar = c(bottom, left, top, right), giving the argument a numerical vector with one number for each side of the plot. #adjust margins of plots par(mfrow = c(2,2), mar = c(2,3,3,2)) boxplot(Pop_data$pop~Pop_data$region, ylab = &quot;Population&quot;, xlab = &quot;Region&quot;, main = &quot;Box Plot of Population by Region&quot;) hist(Pop_data$pop, breaks = 20, main = &quot;Histogram of Population&quot;, xlab = &quot;Population&quot;) barplot(regional_population, ylab = &quot;Population Total&quot;, xlab = &quot;Region&quot;, col = &quot;#004C92&quot;, main = &quot;Bar Plot of Population by Region&quot;, names.arg = c(1:5)) plot(Pop_data$temperature_mean, Pop_data$precipitation_mean, xlab = &quot;Temperature&quot;, ylab = &quot;Rainfall&quot;, main = &quot;Scatter Plot of Temperature and Rainfall&quot;, pch = 1) Similarly, the function ggarrange from ggpubr can also be used to display multiple plots on one window/page. #arrange multiple plots on one page ggarrange(hist1, density1 + rremove(&quot;x.text&quot;), labels = c(&quot;A&quot;, &quot;B&quot;), ncol = 1, nrow = 2) To return the plot window back to its default settings of margins and number of plots per window, use the function dev.off() with no arguments. To save plots as either an image or as a PDF, on pane 3 follow Plots &gt; Export &gt; Save as [Image or PDF]. Figure 2.6: Save plots 2.4 End of module exercises 1. How many cars have a miles per gallon (mpg) of less than 15 in the mtcars data? 2. How many cars have exactly 4 cylinders (cyl) in the mtcars data? 3. What is the mean value of horsepower (hp) to 2 decimal places in the mtcars data? 4. What car has the lowest miles per gallon (mpg) in the mtcars data? 5. What is the median miles per gallon (mpg) value for cars with 8 cylinders (cyl) in the mtcars data? 6. What car has the highest weight (wt) for each amount of cylinders (cyl) in the mtcars data? 7. Create a bar plot that shows the number of cars with each gear type (gear) in the mtcars data. 8. Create a bar plot that shows the number of cars with each gear type (gear) with the distribution of cylinders (cyl) for each type in the mtcars data. Add a legend for the cylinders. 9. Create a scatter plot to show the relationship between weight (wt) and miles per gallon (mpg) in the mtcars data. 10. Create a scatter plot to show the relationship between weight (wt) and miles per gallon (mpg) by cylinder (cyl) in the mtcars data. 2.5 Useful resources R packages: datacamp Importing data: intro2r Basic data wrangling: Data Science in R: A Gentle Introduction Data visualisation: R for Data Science Data visualisation: The R Graph Gallery #drive path for windows data_path &lt;- &quot;data/&quot; "],["working-with-spatial-data-in-r.html", "3 Working with Spatial Data in R 3.1 Spatial data: Overview and types 3.2 Basic GIS concepts 3.3 Using GIS in R 3.4 Visualisation of spatial data 3.5 Interactive maps 3.6 Basic geoprocessing 3.7 End of module exercises 3.8 Useful resources", " 3 Working with Spatial Data in R This module introduces spatial data, providing fundamental knowledge on the different types of spatial data, the key data sources and how to work with them. Module 3 also includes some basic GIS/goeprocessing methods in R. The data visualisation methods explored in Module 2 are expanded upon, accounting for the spatial data. 3.1 Spatial data: Overview and types Spatial data is a term used to describe data collected at and representative of a specific and identifiable geographic location on the surface of Earth. It is sometimes referred to as geospatial data. The type of spatial data is dependent on the storing technique. 3.1.1 Points data One of the fundamental types of spatial data is points data, representing discrete locations or point features on the Earth’s surface, such as the locations of trees in a forest, the position of buildings in a city, or the coordinates of sampling sites in an environmental study. Points data are characterised by having precise geographic coordinates that are often expressed in terms of longitude and latitude. This data type is crucial for analysing spatial patterns, identifying spatial relationships and making informed decisions in various domains including, but not limited to, environmental science, urban planning and epidemiology. 3.1.2 Areal data Within spatial statistics and geospatial analysis, areal (or lattice) data represent aggregated information within a predefined region, where there is only one value per region. Examples of this data include census tracts/districts, administrative regions (provinces, districts, etc.) and grid cells. This data captures the characteristics and attributes that are associated with specific geographic areas, rather than individual points, providing insight into spatial patterns and phenomena at regional and macroscopic scales. Areal data is important for understanding socio-economic disparities, environmental gradients and land-use patterns in particular, with abrupt changes between regions clearly identifiable. From this data, researchers and policy-makers are enabled to explore spatial relationships and make informed decisions on resource allocation, resource planning and public policy interventions. 3.1.3 Vector data Vector data is used for discrete data to graphically represent the real world, objects comprised of points, (poly)lines and polygons. Points are used to mark specific locations, like the position of a landmark. Polylines represent features such as roads, rivers, or boundaries. Polygons enclose areas and represent features like land parcels or administrative boundaries. Vector data is widely used for mapping, spatial analysis, and decision-making in fields like urban planning, environmental management, and transportation. 3.1.3.1 Points Points are given by a singular coordinate pair relating to a specific geographical location, such as a street address. Given that they are represented by a single coordinate pair, they do not have any length or area, meaning that they technically cannot be “seen”. Having points that cannot be seen is not practical, so for mapping purposes, points are represented by symbols, for example, circles or squares, which have both area and shape for the points to be seen. An example of how points look on a map can be seen in the Figure below, with the pink circles representing coordinates for specific place names. Figure 3.1: Points representing named places in pink 3.1.3.2 Polylines Polylines (or lines) are comprised of two or more coordinate pairs, where the coordinate pairs are called vertices. The difference between a vertex (singular of vertices) and a point, is that a vertex is defined by the relationship with neighbouring vertices, and each vertex is connected to at least one other vertex. Similarly to points, polylines have no area, so cannot be “seen”. For mapping purposes, polylines are represented by lines which have area. These symbolising lines can vary in their style, for example, they can be solid, dashed, or dotted. Examples of what polylines represent include rivers, railways and road networks, with the Figure below representing road networks in a specific area with white lines. Figure 3.2: Polylines representing road networks in white 3.1.3.3 Polygons Polygons (or lattices or areas) are comprised of three or more line segments that share the same start and end coordinate pairs to form a shape that encloses an area in the “inside environment”. Given that they have both area and length (of the perimeter), polygons are visible without additional the symbolisation needed for points and polylines. Examples of what polygons represent include buildings, lakes, postcode areas and counties, with the Figure below including polygons in white representing settlement boundaries. Figure 3.3: Polygon representing settlement boundaries in white outlines 3.1.4 Raster data Raster data is a continuous representation (for continuous data) of the Earth’s surface, comprised of regular grid cells identified with an associated number or by the row and column. Individual grid cells are grouped together to represent an image. There are two main types of grid cell data structure: Binary (categorical or thematic) - e.g. land cover, soils data, … Continuous - e.g. elevation, temperature, … The cell size (or spatial resolution) determines the area of a grid, where smaller grid cells lead to a finer spatial resolution. For example, a grid of 1km x 1km cells means that a single value represents an area of 1km x 1km An example of raster data can be seen in the Figure below, with the different colours representing the difference in average population in 100m by 100m grid cells. Figure 3.4: Raster showing the average population in 100m by 100m grid cells Figure 3.5: Grid cell in continuous raster 3.1.5 Sources of spatial data Multiple sources of geospatial data exist, for example: Field data collection - e.g. using a GPS Remote sensing and satellite imageries Digitisation (digitising features) - e.g. roads, OpenStreetMap (OSM) Feature/components extraction (automatic detection of features) - e.g. building footprints Spectral analysis (computing parameters for each cell) - e.g. vegetation indices, slope, elevation Classification (grouping cells into classes) - e.g. land cover classification Change detection (comparing cell values over time) - e.g. loss or gain of forest 3.1.6 Geospatial covariates Geospatial covariates can be described as supporting datasets that don’t directly measure the population but (may) correlate to the population, used as proxies of variation in population density. If the covariates correlate to the population, they can provide important context for the population data, explaining unexplained variations and consequently improving the model accuracy. Geospatial covariates act as predictor values in producing a weighting layer, related to the population density and associated with both the built and natural environment. The weighting layer is used to spatially disaggregate the enumeration areas (EA) population. Geospatial covariates are also used as proxies of variation in population density. Street data, building footprints and vegetation data are all examples of geospatial covariates where typically, building footprints and the corresponding residential information are strong predictors. 3.2 Basic GIS concepts A Geographic Information System (GIS) is the most common way of processing and analysing spatial data through being a multi-component environment which creates, manages, manipulates, visualises and analyses data. 3.2.1 Geoprocessing In order to analyse the data available, the dataset must be in raster format. If it is not already in raster format, each geographic variable/geospatial covariate must be converted to raster data, where each variable must have the same spatial resolution with the cells spatially aligned in a continuous and coherent mesh. For example, vector data with points representing building footprints can be transformed to raster data via grid cells, where cells having at least one building (defined by the centroid of the building footprint) are classified as residential. From this, raster data can be derived on building counts, building area, building length and distance to nearest buildings. Figure 3.6: Deriving raster data from vector If the data is already in raster format, geoprocessing involves re-sampling, aligning the dataset to a grid reference layer and delineation of the study area. Alternatively, for binary or categorical data, a grid can be created based on the proportion covered, for example, it is possible to calculate the distance to a single value or the density of a feature. An example of this data conversion can be seen in the Figure below, converting binary data on the loss of forest area to the proportion of each grid cell with forest loss. Figure 3.7: Cell proportions from binary data 3.3 Using GIS in R In R, Geographic Information Systems (GIS) capabilities are facilitated through various packages such as sp, raster, sf . These packages provide functions for reading, manipulating, analysing, and visualizing spatial data within the R environment. 3.3.1 Importing spatial data in R A basic vector data format is Shapefiles, which is made up of several files sharing the same name but different extensions: Basic: .shp: stores the map object information .dbf: stores the attribute information of the features .shx: stores the index of the feature geometry Additional .prj: projection information (ESRI) .xml: metadata associated with file QGIS specific .qpj: project information (QGIS) .qml: style information .qix: spatial index To import vector spatial data, the functions st_read() or read_sf() from the sf package are required. #install sf package install.packages(&quot;sf&quot;) #load the sf package library(sf) #import vector data health_facilities &lt;- st_read(paste0( data_path, &quot;GRID3_Nigeria_-_Health_Care_Facilities.shp&quot;)) road_network &lt;- st_read(paste0(data_path, &quot;nga_rds_1m_dcw.shp&quot;)) states &lt;- st_read(paste0(data_path, &quot;Admin2_states.shp&quot;)) country &lt;- st_read(paste0(data_path, &quot;nga_polbnda_adm0_1m.shp&quot;)) To import raster spatial data, the function raster() from the raster package is required. #install the raster package install.packages(&quot;raster&quot;) #load the raster package library(raster) #read in the raster data pop_raster &lt;- raster(paste0(data_path, &quot;NGA_population_v1_2_gridded.tif&quot;)) 3.3.2 Handling spatial data in R Most spatial processing can be done with the following packages: sf raster tmap terra leaflet mapview units exactextractr Additionally, tidyverse functions such as merge(), subset() and extract() can be used for manipulating the attribute tables of the spatial data. install.packages(&quot;tmap&quot;) install.packages(&quot;terra&quot;) install.packages(&quot;leaflet&quot;) install.packages(&quot;mapview&quot;) install.packages(&quot;units&quot;) install.packages(&quot;tidyverse&quot;) install.packages(&quot;RColorBrewer&quot;) install.packages(&quot;exactextractr&quot;) library(tmap) library(terra) library(leaflet) library(mapview) library(units) library(tidyverse) library(RColorBrewer) library(exactextractr) # tmap_options(check.and.fix = TRUE) #fix potential issues during the processing 3.3.2.1 Converting between vectors and rasters in R Vector data can be converted to raster data through using the function rasterize() in R from the package raster() with arguments: x: the shape file you wish to rasterise y: the raster object field: the values (codes) you wish to transfer over to raster object #rasterise the vector data health_facilities_raster &lt;- rasterize(health_facilities, pop_raster, field=&#39;lga_code&#39;) #plot the raster data - axis limits added for clarity of plot plot(health_facilities_raster, xlim=c(3,6), ylim=c(6,8)) To stack raster data, the function stack() can be utilised, including functions for each of the raster datasets you wish to stack. #stack raster data stack(pop_raster, health_facilities_raster) ## class : RasterStack ## dimensions : 11546, 14413, 166412498, 2 (nrow, ncol, ncell, nlayers) ## resolution : 0.0008333333, 0.0008333333 (x, y) ## extent : 2.66875, 14.67958, 4.270417, 13.89208 (xmin, xmax, ymin, ymax) ## crs : +proj=longlat +datum=WGS84 +no_defs ## names : NGA_population_v1_2_gridded, layer ## min values : 0.2034, 101.0000 ## max values : 1240.079, 35016.000 Raster data can be converted to a polygon via the function as.polygons() from the terra package with an argument for the raster data you wish to convert. #create a simple example raster dataset example_raster &lt;- rast(ncols=3, nrows=3) values(example_raster) &lt;- 1:ncell(example_raster) #convert raster data to polygon example_rast_to_poly &lt;- as.polygons(example_raster) example_rast_to_poly ## class : SpatVector ## geometry : polygons ## dimensions : 9, 1 (geometries, attributes) ## extent : -180, 180, -90, 90 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat WGS 84 (CRS84) (OGC:CRS84) ## names : lyr.1 ## type : &lt;int&gt; ## values : 1 ## 2 ## 3 Using the function st_as_sf() with a polygon dataset as an argument, a polygon can be converted to a sf object. #convert polygon created above example_poly_to_sf &lt;- st_as_sf(example_rast_to_poly) example_poly_to_sf ## Simple feature collection with 9 features and 1 field ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: -180 ymin: -90 xmax: 180 ymax: 90 ## Geodetic CRS: WGS 84 (CRS84) ## lyr.1 geometry ## 1 1 POLYGON ((-180 90, -180 30,... ## 2 2 POLYGON ((-60 90, -60 30, 6... ## 3 3 POLYGON ((60 90, 60 30, 180... ## 4 4 POLYGON ((-180 30, -180 -30... ## 5 5 POLYGON ((-60 30, -60 -30, ... ## 6 6 POLYGON ((60 30, 60 -30, 18... ## 7 7 POLYGON ((-180 -30, -180 -9... ## 8 8 POLYGON ((-60 -30, -60 -90,... ## 9 9 POLYGON ((60 -30, 60 -90, 1... 3.3.2.2 Checking the resolution and number of cells As previously explained, the spatial resolution of raster data determines the area of a grid, defined by the size of the cells in the raster dataset. The resolution of the dataset can be checked in R by using the function res(), inputting the chosen raster dataset as an argument, returning the x and y resolution of the raster object. #check the resolution of the population raster dataset res(pop_raster) ## [1] 0.0008333333 0.0008333333 To return just the x or just the y resolution of a raster object, the functions xres() and yres() can be used respectively, also inputting the raster dataset as an argument. #check the x resolution xres(pop_raster) ## [1] 0.0008333333 #check the y resolution yres(pop_raster) ## [1] 0.0008333333 Additionally, the resolution of a raster object can be updated by assigning new values to the resolution. #create an example raster example_raster2 &lt;- raster(ncol=18, nrow=18) res(example_raster2) ## [1] 20 10 #update the resolution of the example raster res(example_raster2) &lt;- 1/120 res(example_raster2) ## [1] 0.008333333 0.008333333 To check how many cells are in a raster object, the function ncell() can be used, inputting the chosen raster dataset as an argument. #check number of cells in population raster data ncell(pop_raster) ## [1] 166412498 The functions ncol() and nrow() discussed in previous modules can also be used for raster objects to check the number of columns and number of rows respectively. #check the number of columns in the population raster data ncol(pop_raster) ## [1] 14413 #check the number of rows in the population raster data nrow(pop_raster) ## [1] 11546 3.3.2.3 Exporting spatial data in various formats As in Module 2, exporting data from R requires different functions for different data types. Data type Function Package required .csv write.csv() none (in base) .shp, shape file st_write() sf .shp, shape file from polygon write_sf() sf .tif, raster writeRaster() raster #export a .csv file write.csv(st_drop_geometry(sample_data), file = &quot;exported_data.csv&quot;, row.names = FALSE) #export a .shp file st_write(sample_data, &quot;exported_data.shp&quot;) #export a .shp file write_sf(sample_data, &quot;exported_data.shp&quot;) #export a .tif file writeRaster(sample_raster, file = &quot;exported_raster.tif&quot;) 3.4 Visualisation of spatial data Visualizing spatial data is essential for understanding spatial phenomena and communicating findings effectively. Visualizations of static and interactive maps play an important role in spatial analysis and geospatial data interpretation. Static maps provide a snapshot of spatial patterns and relationships of geographic information. They are often used for publication purposes, presentations, and reports, providing a clear visual summary of analytical findings. In R, static maps can be created using packages like ggplot2 and tmap, which offer powerful capabilities for designing aesthetically pleasing and informative maps. On the other hand, interactive maps offer dynamic exploration capabilities, allowing users to interactively explore spatial data layers, zoom in/out, pan across the map, and query specific features. Interactive maps enhance engagement and facilitate deeper understanding by enabling users to customize their viewing experience and interact with spatial data in real-time. For interactive mapping in R, packages like leaflet and mapview are commonly used. 3.4.1 Static maps Visualisation with base R The plot() function from base R can be used for visualising spatial data, as it reads both for vector and raster data. With vector data, plot() defaults to plotting the attributes of the dataset. #plot the states plot(states) However, through indexing the desired attribute, a single attribute can be plotted. #plot just the shape area attribute plot(states[&#39;region&#39;], main = &quot;region&quot;, pal = rev(heat.colors(10))) To display a raster dataset with plot(), only the dataset name is required as an argument. #plot the raster data plot(pop_raster) To take a quick look at a vector, the geometry of the dataset can be extracted with the function st_geometry(), and calling the function plot(). #plot the states st_geometry(states) %&gt;% plot() Alternatively, the function st_geometry() can be called inside the function plot(). #plot the states plot(st_geometry(states)) Layers can be combined by using the argument add = TRUE in the plot() function, however, this argument only works with the st_geometry() function. #add layers for the states to the country plot plot(st_geometry(country)) plot(st_geometry(states), add = TRUE, pch = 16, col = &quot;#004C92&quot;, cex = 0.5) Vector and raster data can be plotted at the same time by combining the plot layers with the argument add = TRUE in the plot() function. plot(pop_raster) plot(st_geometry(states), add = TRUE, pch = 16, cex = 0.5) Visualisation with tmap The tmap package has more options and possibilities for mapping and allows for both static and interactive views. Through using the function tm_shape() with the dataset, the vector data can be plotted, mapping the shape of the country. #plot the country shapefile tm_shape(country) + tm_fill(&quot;Terr_Name&quot;) Points can be added to the map of the country using the function tm_shape() with the dataset as the argument in conjunction with the function tm_dots() with arguments for the size and title. In this case, the points represent health facilities. #plot the country with points for health facilities tm_shape(country) + tm_fill(&quot;Terr_Name&quot;)+ tm_shape(health_facilities)+ tm_dots(size = 0.2, col = &quot;type&quot;, title = &quot;Health facilities&quot;) Polylines can be added to the map using the function tm_shape() with the dataset that includes the polylines as the argument. In this case, the polylines represent the road networks. #plot the country with a layer for road networks tm_shape(country) + tm_fill(&quot;Terr_Name&quot;)+ tm_shape(road_network)+ tm_lines(size = 0.5) Polygons can be added to the map using the function tm_fill(), including the variable name that identifies the shapes/polygons as the argument. In this case, the polygons represent the states of the country. #plot the country with a layer for the states tm_shape(states) + tm_fill(&quot;statename&quot;)+ tm_layout(legend.show = TRUE) Choropleth maps can be created with tmap through adding the attribute of choice as an argument to the polygon element. #create choropleth map tm_shape(states) + tm_polygons(&quot;Area&quot;) To add a colour palette (or change the default colour scheme) the argument pal= can be added. For example, to make the choropleth map have a blue colour palette, add the argument pal=\"Blues\" to the tm_polygons() function. #change colour palette of choropleth map tm_shape(states) + tm_polygons(&quot;Area&quot;, pal=&quot;Blues&quot;) To reverse the colours within the palette, a hyphen, -, can be added in front of the colours chosen, for example pal=\"-Blues\". #reverse colours in choropleth map tm_shape(states) + tm_polygons(&quot;Area&quot;, pal=&quot;-Blues&quot;) The function tm_shape() can also be used to plot raster data, however, it can take a long time to plot due to the large size of the dataset. #displaying raster data tm_shape(country) + tm_borders()+ tm_shape(pop_raster) + tm_raster() ## stars_proxy object shown at 14413 by 11546 cells. Similarly to using the plot() function, multiple layers can be included based on one input. #add multiple layers to plot tm_shape(states) + tm_polygons() + tm_dots(size = 0.5, col = &quot;pink&quot;) + tm_text(&quot;statename&quot;, col = &quot;purple&quot;, size = 0.75) Alternatively, multiple different layers can be included using multiple shapes. #plot multiple shapes and layers tm_shape(states) + tm_polygons() + tm_shape(health_facilities) + tm_dots(size = 0.01) Exercise: How can you add multiple layers, such as polygons for states and points for health facilities, to a single map using tmap? By using tm_shape() and tm_fill() By using tm_shape() and tm_dots() By using multiple tm_shape() functions with different elements By using tm_shape() and tm_lines() As with standard data visualisation, you can save the plots by assigning them to a given name, and similar to how with ggplot2 plots can be saved and added to with recall, tmap allows for parts of the map to be saved and then reused. #save and reuse plot mymap &lt;- tm_shape(states) + tm_polygons() mymap + tm_dots(size = 0.75, col=&quot;pink&quot;) + tm_text(&quot;statename&quot;, col = &quot;purple&quot;) Similarly to how plots can be saved as explained in Module 2, maps produced in R can also be saved. In order to save maps, use the function tmap_save() with arguments for the plot name and what you want to name the file. #save mymap as a .png file tmap_save(mymap, &quot;mymap.png&quot;) ## Map saved to F:\\Study\\WorldPop Training Materials\\GitBook\\wp_training_manual\\mymap.png ## Resolution: 2329.98 by 1892.72 pixels ## Size: 7.766599 by 6.309068 inches (300 dpi) As introduced in Module 2, the function arrange() can be used to display multiple different plots at the same time, effectively combining multiple plots into one image. #create three maps m1 &lt;- tm_shape(country) + tm_polygons() m2 &lt;- tm_shape(states) + tm_polygons() m3 &lt;- tm_shape(health_facilities) + tm_dots() #arrange them in one image tmap_arrange(m1, m2, m3, nrow = 1) 3.5 Interactive maps Visualisation with tmap The maps produced so far are static map. tmap allows us to also do an interactive map where we can zoom-in and out to see the spatial distribution of objects. We use the tmap_mode() function to toggle between interactive and static maps. The default option is tmap_mode(\"plot\"), and is a static mapping mode. Alternatively, tmap_mode(\"view\") is used for interactive map. tmap_mode(&quot;view&quot;) ## tmap mode set to interactive viewing #plot the states tm_shape(states)+ tm_borders(col=&#39;brown&#39;, lwd=3)+ tm_shape(states)+ tm_borders()+ tm_basemap(&#39;OpenStreetMap&#39;) tmap_mode(&quot;view&quot;) ## tmap mode set to interactive viewing # plot road networks tm_shape(road_network)+ tm_lines(size = 0.5)+ tm_basemap(&#39;OpenStreetMap&#39;)+ tm_shape(states)+ tm_borders(lwd=1) Visualisation with leaflet leaflet is another package that is used for mapping, primarily to create dynamic maps. For this package, the function leaflet() is called, using the piper operator %&gt;% to add layers to the map, where layers are added using the function add*() where the asterisk * is replaced by different words corresponding to different layer types. For example, the function addPolygons() adds a polygon layer to the map and the function addTiles() adds a basemap (a base layer of a map) to visualise the country, area or region being mapped. The code below demonstrates how to plot a polygons representing the country’s states to a basemap using the leaflet package. #plot the states states_map &lt;- leaflet() %&gt;% addTiles() %&gt;% addPolygons(data = states, color = &quot;brown&quot;, weight = 3, fillOpacity = 0) #display the map states_map Points can be added to interactive plots with leaflet using the addCircleMarkers() function. To demonstrate the plotting of points, a plot the healthcare facilities with leaflet, differentiating the different types by different colours can be mapped. First the different types of healthcare facilities need to be set as a factor. #convert &#39;type&#39; column to factor health_facilities$type &lt;- as.factor(health_facilities$type) To distinguish between the different types of healthcare facilities, a colour palette can be created through identifying each of the unique types with the function unique() and then creating the palette with the function palette(). #define unique types unique_types &lt;- unique(health_facilities$type) #define colour palette color_pal &lt;- colorFactor(palette = &quot;Accent&quot;, domain = unique_types) Once the variable is set as a factor and the colour palette is created, the points can be mapped. The below code creates a map with a base layer for the country’s map, points for each of the healthcare facilities with the different types given by different colour points and polygons to identify each of the country’s states. Additionally, to help identify the different healthcare facility types, a legend layer is added using the function addLegend(). #create Leaflet map health_facilities_map &lt;- leaflet(health_facilities) %&gt;% addTiles() %&gt;% addCircleMarkers(lng = ~longitude, lat = ~latitude, stroke = TRUE, color = &quot;black&quot;, weight = 0.5, fill = TRUE, fillColor = ~color_pal(type), radius = 2, fillOpacity = 0.5) %&gt;% addPolygons(data = states, color = &quot;orange&quot;, fillColor = &quot;transparent&quot;, weight = 1.5) %&gt;% addLegend(position = &quot;bottomright&quot;, pal = color_pal, values = ~type, title = &quot;Type&quot;) #display the map health_facilities_map Visualisation with mapview mapview is an another package that can be used to create maps, originally created with the purpose of creating quick interactive maps. To create a simple map the function mapview() can be used, including the dataset as a necessary argument, with further optional arguments such as col.regions = to assign specific colours to polygon (region) shapes. In this case, since the states data contains information on the states of the country, including their boarders, mapping states as the dataset will result in an interactive map with polygons for the country’s states. The same function mapview() can be used to plot points, used in the same way but with the addition of a dataset which contains points data, such as the health_facilities data. As with leaflet, maps can be layered with mapview, but instead of using the piper operator, a plus sign + is used. #plot the health facilities with a layer for states mapview(health_facilities, zcol = &#39;type&#39;, cex = 2, col.regions=brewer.pal(7, &quot;Accent&quot;), alpha = 0.5) + mapview(states, color = &#39;orange&#39;, lwd = 2, col.regions = &quot;transparent&quot;, alpha = 1) 3.6 Basic geoprocessing A vector layer can be joined with tabular data using a common identifier. The population dataset in .csv format contains predicted population figures for all of the states. This .csv file can be joined with the shapefile for the states and mapped. #import the population dataset pop_estimate &lt;- read.csv(paste0(data_path, &quot;Population.csv&quot;)) #view the population size estimates view(pop_estimate) The variables (or attributes) of the states shapefile can be viewed to help identify a common field which would allow for joining the vector layer with the tabular data. #view the attributes of the states vector data view(states) There are multiple common fields to both datasets that can be used to join the two datasets, however, the variable “statename” will be used. The function innter_join() will join the two datasets, keeping only the observations in x that possess a matching ‘key’ in y. #join the states vector data and the population estimates tabular data together pop_states &lt;- states %&gt;% inner_join(pop_estimate, by = &quot;statename&quot;) The function fulljoin() can be used to join the two datasets, however, it keeps all observations in x and y. #join the states vector data and the population estimates tabular data together pop_states_full &lt;- full_join(states, pop_estimate, by=&quot;statename&quot;) Alternatively, the function merge() can be used to join two datasets based on a unique ID. #merge two datasets pop_merged &lt;- merge(states, pop_estimate, by=&quot;statename&quot;) Once the datasets are joined, the population distribution by state can be visualised using the spatial visualisation methods previously discussed. #visualize population distribution by state # tm_shape(pop_states)+ # tm_polygons(col=&#39;total&#39;, lwd=2)+ # tm_shape(states)+ # tm_borders()+ # tm_basemap(&#39;OpenStreetMap&#39;) # Set tmap mode to static plot tmap_mode(&quot;plot&quot;) ## tmap mode set to plotting # Create static map tm_shape(pop_states) + tm_polygons(col = &quot;total&quot;, title = &quot;total&quot;, lwd = 2) + tm_shape(states) + tm_borders() + tm_layout(frame = FALSE, legend.position = c(&quot;right&quot;, &quot;bottom&quot;), main.title = &quot;&quot;) # tm_scale_bar() + # Display scale bar # tm_compass() # Display north arrow 3.6.1 Coordinate reference systems (CRS) Given that the Earth is a globe, it cannot be visualised in itself all at once. In other words, you can only see one side of the Earth at a time. To circumvent this issue when mapping, projected maps are used, a two-dimensional flattened out version of the Earth which can be viewed all at one time. A simplified example of this concept is through visualising an orange. You cannot see all sides of the orange peel at one time, given that it is spherical, like the Earth. However, through peeling the orange, and flattening the peel out on a surface, the entire orange peel can be seen at once. The idea of this process has been done by cartographers for centuries, in order to effectively map out the globe. Coordinate reference systems, also commonly referred to as the abbreviation CRS, provide a standardised method that uses coordinates to define how these two-dimensional projected maps relate to real locations on Earth. Within GIS, there are two types of coordinate reference systems, Projected Coordinate Systems and Geographic Coordinate Systems, where the latter is the commonly used method where position on the Earth’s surface as given in latitude and longitude coordinates. To extract coordinate system information from vector data or raster data in R, the function st_crs() can be used. #extract CS information from healthcare_facilities vector data st_crs(health_facilities) ## Coordinate Reference System: ## User input: WGS 84 ## wkt: ## GEOGCRS[&quot;WGS 84&quot;, ## DATUM[&quot;World Geodetic System 1984&quot;, ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## CS[ellipsoidal,2], ## AXIS[&quot;latitude&quot;,north, ## ORDER[1], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## AXIS[&quot;longitude&quot;,east, ## ORDER[2], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## ID[&quot;EPSG&quot;,4326]] #extract CS information from pop_raster raster data st_crs(pop_raster) ## Coordinate Reference System: ## User input: +proj=longlat +datum=WGS84 +no_defs ## wkt: ## GEOGCRS[&quot;unknown&quot;, ## DATUM[&quot;World Geodetic System 1984&quot;, ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]], ## ID[&quot;EPSG&quot;,6326]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8901]], ## CS[ellipsoidal,2], ## AXIS[&quot;longitude&quot;,east, ## ORDER[1], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433, ## ID[&quot;EPSG&quot;,9122]]], ## AXIS[&quot;latitude&quot;,north, ## ORDER[2], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433, ## ID[&quot;EPSG&quot;,9122]]]] If there is no coordinate system definition for an object, or you wish to overwrite the existing information, the function st_set_crs() can be used to assign coordinate system information to the object. 3.6.2 Converting between Lat/Long and the Universal Transverse Mercator (UTM) coordinate system The Universal Transverse Mercator (UTM) coordinate system is a map projection based on the Transverse Mercator projection, a modified version of the standard Mercator cylindrical projections which has been around since the 1500s. The standard Mercator projections of the Earth distort area leading to the sizes of many countries being massively misrepresented, and this approach uses an upright cylinder for its map projection. The Transverse Mercator projection uses a cylinder that has been rotated 90 degrees, used to form the UTM projection through placing the cylinder 60 times, each occasion having a different central line leading to 60 UTM zones each 6 degrees of longitude wide in order to minimise distortion in each segment. Given that distortion is minimised for each zone, it is a poor choice of map projection when multiple zones are projected together. Therefore, UTM is most suitable for narrow regions, rather than world map projections. To convert between latitude/longitude coordinates and the UTM coordinate system in R, the following code can be used. Firstly, the function st_crs() is used to check the current coordinate reference system. #check the current coordinate reference system of the states shapefile current_crs &lt;- st_crs(states) print(current_crs) ## Coordinate Reference System: ## User input: WGS 84 ## wkt: ## GEOGCRS[&quot;WGS 84&quot;, ## DATUM[&quot;World Geodetic System 1984&quot;, ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## CS[ellipsoidal,2], ## AXIS[&quot;latitude&quot;,north, ## ORDER[1], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## AXIS[&quot;longitude&quot;,east, ## ORDER[2], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## ID[&quot;EPSG&quot;,4326]] mapview::mapview(states) The results show that the current coordinate system is on latitude/longitude, so will be converted to the UTM coordinate system. However, before converting, the UTM code (identifying which zone the region is in) and corresponding EPSG code (a unique reference number for identifying regions in coordinate systems) need to be found, this can be done through searching the internet for the regions UTM code and then searching for the corresponding EPSG code. A good resource for converting between the two codes is the following website up42.com. The states data is from Nigeria. Nigeria has 3 UTM codes as it is in zones 31, 32 and 33. However, for this example, the middle zone will be used, leading to the UTM code 32N. The corresponding EPSG code is then 32632. To convert between the coordinate systems, the function st_transform() can be used, including the shapefile and the corresponding EPSG code as arguments. #convert the states projection system to UTM states_utm &lt;- st_transform(states, 32632) #check the new coordinate reference system new_crs &lt;- st_crs(states_utm) print(new_crs) ## Coordinate Reference System: ## User input: EPSG:32632 ## wkt: ## PROJCRS[&quot;WGS 84 / UTM zone 32N&quot;, ## BASEGEOGCRS[&quot;WGS 84&quot;, ## ENSEMBLE[&quot;World Geodetic System 1984 ensemble&quot;, ## MEMBER[&quot;World Geodetic System 1984 (Transit)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G730)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G873)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G1150)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G1674)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G1762)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G2139)&quot;], ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]], ## ENSEMBLEACCURACY[2.0]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## ID[&quot;EPSG&quot;,4326]], ## CONVERSION[&quot;UTM zone 32N&quot;, ## METHOD[&quot;Transverse Mercator&quot;, ## ID[&quot;EPSG&quot;,9807]], ## PARAMETER[&quot;Latitude of natural origin&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8801]], ## PARAMETER[&quot;Longitude of natural origin&quot;,9, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8802]], ## PARAMETER[&quot;Scale factor at natural origin&quot;,0.9996, ## SCALEUNIT[&quot;unity&quot;,1], ## ID[&quot;EPSG&quot;,8805]], ## PARAMETER[&quot;False easting&quot;,500000, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8806]], ## PARAMETER[&quot;False northing&quot;,0, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8807]]], ## CS[Cartesian,2], ## AXIS[&quot;(E)&quot;,east, ## ORDER[1], ## LENGTHUNIT[&quot;metre&quot;,1]], ## AXIS[&quot;(N)&quot;,north, ## ORDER[2], ## LENGTHUNIT[&quot;metre&quot;,1]], ## USAGE[ ## SCOPE[&quot;Navigation and medium accuracy spatial referencing.&quot;], ## AREA[&quot;Between 6°E and 12°E, northern hemisphere between equator and 84°N, onshore and offshore. Algeria. Austria. Cameroon. Denmark. Equatorial Guinea. France. Gabon. Germany. Italy. Libya. Liechtenstein. Monaco. Netherlands. Niger. Nigeria. Norway. Sao Tome and Principe. Svalbard. Sweden. Switzerland. Tunisia. Vatican City State.&quot;], ## BBOX[0,6,84,12]], ## ID[&quot;EPSG&quot;,32632]] mapview::mapview(states_utm) To convert back to the original coordinate reference system, the function st_transform() can be used again, with the UTM data and original coordinate reference system as arguments. #convert states back to latitude/longitude coordinate reference system states_original_crs &lt;- st_transform(states_utm, current_crs) #check the coordinate reference system after converting back crs_after_conversion &lt;- st_crs(states_original_crs) print(crs_after_conversion) ## Coordinate Reference System: ## User input: WGS 84 ## wkt: ## GEOGCRS[&quot;WGS 84&quot;, ## DATUM[&quot;World Geodetic System 1984&quot;, ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## CS[ellipsoidal,2], ## AXIS[&quot;latitude&quot;,north, ## ORDER[1], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## AXIS[&quot;longitude&quot;,east, ## ORDER[2], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## ID[&quot;EPSG&quot;,4326]] 3.6.3 Subsetting, clipping and masking Using the function filter(), a vector layer can be subset, with the function qtm() from the package tmap plotting a quick thematic plot. #vector layer using a simple query pop_kano &lt;- pop_states %&gt;% filter(statename==&#39;Kano&#39;) # extracting Kano states qtm(pop_kano) The population raster can be clipped using the Kano state and the polygon extent. #clip the population raster r_clipped &lt;- crop(pop_raster, pop_kano) The clipped population raster can then be plotted, creating a map of the Kano state. #create map of Kano state tm_shape(r_clipped)+ tm_raster(title=&quot;Population&quot;)+ tm_shape(pop_kano)+ tm_basemap(&#39;OpenStreetMap&#39;)+ tm_borders(lwd = 3) Another method of clipping is to use the function st_intersection() where a vector layer can be clipped based on another layer. #clip country based on states states_clipped &lt;- st_intersection(country, states) ## Warning: attribute variables are assumed to be spatially constant throughout all geometries Again using the filter() function, states with a population of more than 5 million people can be subset, then plotted with the tmap package. #subset population state_5million &lt;- pop_states %&gt;% filter(total &gt; 5000000) #give names of states that have over 5 million people state_5million$statename ## [1] &quot;Kaduna&quot; &quot;Ogun&quot; &quot;Oyo&quot; &quot;Katsina&quot; &quot;Imo&quot; &quot;Rivers&quot; &quot;Jigawa&quot; &quot;Kano&quot; &quot;Sokoto&quot; &quot;Niger&quot; ## [11] &quot;Bauchi&quot; &quot;Borno&quot; &quot;Lagos&quot; #plot the states with a population of over 5 million people tm_shape(state_5million)+ tm_polygons(col = &quot;total&quot;, title = &quot;Population&quot;)+ tm_basemap(&#39;OpenStreetMap&#39;)+ tm_borders(lwd=4) Masking is an important part of raster processing, typically done to remove low-quality observations from the raster through setting particular cells to the NoData value. In R, this can be done through using the function mask(). To demonstrate masking, the following example is used with a simulated dataset, where values of the second raster below 0.4 are set to be NA. #simulate some raster data raster1 &lt;- raster(ncol=20, nrow=20) raster2 &lt;- raster(ncol=20, nrow=20) values(raster1) &lt;- runif(ncell(raster1)) * 10 values(raster2) &lt;- runif(ncell(raster1)) raster2[raster2 &lt; 0.4] &lt;- NA To visually demonstrate the missing values, the two rasters can be plotted side-by-side, where the white spaces in the second raster correspond to the NA values. #plot the two rasters side-by-side par(mfrow=c(1,2)) plot(raster1, main=&quot;Raster 1&quot;) plot(raster2, main=&quot;Raster 2&quot;) The mask() function can then be used with arguments for the two rasters, where the raster with the missing data is put second as follows. #mask the raster masked_raster &lt;- mask(raster1, raster2) plot(masked_raster) 3.6.4 Buffer analysis Buffer analysis is a crucial step within spatial statistics because it enables the aggregation of the output data based on another layer and within a specific area, for example, aggregating the population within a 1km radius around a health post or summation of pixel values at the sub-district administrative level. To demonstrate how to perform buffer analysis in R, a dataset containing local government area (LGA) boundaries (in Nigeria) is used. For the analysis, only the health facilities in the Abuja local government area (municipal area council) are of interest. #filter to have only the Abuja LGA Abuja_lga &lt;- lga %&gt;% filter(lga_name_x == &quot;Municipal Area Council&quot;) #filter to select only the health facilities in the Abuja LGA health_facilities_Abuja &lt;- health_facilities %&gt;% filter(lga_name==&#39;Municipal Area Council&#39;) To calculate the amount of health facilities are in Abuja, the function tally() can be used. #the number of health facilities in Abuja is given by &quot;n&quot;, in this case, n=226 tally(health_facilities_Abuja) ## Simple feature collection with 1 feature and 1 field ## Geometry type: MULTIPOINT ## Dimension: XY ## Bounding box: xmin: 7.205538 ymin: 8.635901 xmax: 7.587964 ymax: 9.130127 ## Geodetic CRS: WGS 84 ## n geometry ## 1 226 MULTIPOINT ((7.299316 9.130... The number of specific points, health facilities in this example, can be counted and visualised for a given region. Counting is done using the functions group_by() and summarise(), with functions from the ggplot2 package used for visualising the counts (grouped by type) in a bar plot. #count health facilities in Abuja and create bar plot health_facilities_Abuja %&gt;% group_by(type) %&gt;% summarise(count= n()) %&gt;% ggplot(aes(x = type, y = count)) + geom_bar(stat = &quot;identity&quot;) + geom_text(aes(label = count), vjust = -0.5, size = 4) + xlab(&quot;Type of Health Facility&quot;) + ylab(&quot;Number&quot;) + ggtitle(&quot;Number of Health Facilities in Abuja&quot;)+ scale_fill_brewer(type = &quot;qual&quot;, palette = &quot;Greens&quot;) The plotting methods discussed above can be used to visualise the points by the group, in this case, health facilities by type. #plot health facilities tm_shape(health_facilities_Abuja)+ tm_dots(col=&#39;type&#39;, size=0.07, id=&#39;primary_na&#39;, popup.vars=c(&#39;category&#39;,&#39;functional&#39;,&#39;source&#39;))+ tm_basemap(&#39;OpenStreetMap&#39;)+ tm_shape(Abuja_lga)+ tm_borders(lwd=4) The table() function can be used to count the number of points, and subset the group that you want to count for. For example, to find out the number of tertiary health care facilities there are in Abuja can be done using the following code. #create a table to count points table(health_facilities_Abuja$type) ## ## Health Post Dispensary Primary Primary Health Center Primary Health Clinic ## 0 197 0 0 ## Secondary Tertiary Unknown ## 23 6 0 The crop() function can be used to create a geographical subset in the raster data. #subset the raster data for the Abuja region pop_Abuja &lt;- crop(pop_raster, Abuja_lga) #plot the cropped raster plot(pop_Abuja) #map the health facilities with the cropped raster data on the same plot tm_shape(health_facilities_Abuja)+ tm_dots(col=&#39;type&#39;, size=0.07, id=&#39;primary_na&#39;, popup.vars=c(&#39;category&#39;,&#39;functional&#39;,&#39;source&#39;))+ tm_basemap(&#39;OpenStreetMap&#39;)+ tm_shape(pop_Abuja)+ tm_raster()+ tm_shape(Abuja_lga)+ tm_borders(lwd=4) Buffering points can be computed using the function st_buffer() with arguments for the data and dist for the desired size (e.g. 1km around each of the health facilities). #select the buffer points to be 1km around the health facilities health_facilities_Abuja_buffered &lt;- st_buffer(health_facilities_Abuja, dist=set_units(1, km)) #add the buffer points to the map tm_shape(pop_Abuja)+ tm_raster()+ tm_shape(health_facilities_Abuja_buffered)+ tm_borders()+ tm_shape(health_facilities_Abuja)+ tm_dots( size=0.08, id=&#39;primary_na&#39;)+ tm_shape(Abuja_lga)+ tm_borders(lwd=4)+ tm_basemap(&#39;OpenStreetMap&#39;) The function as() can be used to convert the merged buffer points into a SpatialPolygonsDataFrame object through including both the buffer points data and the type of data object (in this case, “Spatial”) as arguments. #convert the merged buffers to a SpatialPolygonsDataFrame object health_facilities_Abuja_buffered &lt;- as(health_facilities_Abuja_buffered, &quot;Spatial&quot;) The population size can be computed through using the function extract() from the raster package, identifying the cropped raster data, buffer points data and function (in this case, sum) as arguments, in addition to changing the defaults of the arguments na.rm and df to TRUE. This computes the population within the buffer points (1km radius around health care facilities in Abuja) through extracting the corresponding populations from the raster data. #extract the population of those in the buffer point regions from the raster data health_facilities_Abuja_pop &lt;- raster::extract(pop_Abuja, health_facilities_Abuja_buffered, fun=sum, na.rm=T,df=T) #rename extracted population health_facilities_Abuja_pop &lt;- health_facilities_Abuja_pop %&gt;% rename(pop = NGA_population_v1_2_gridded) The overall population within the given regions can then be computed by using the function sum() with the desired population as the argument. #Total number of people living within a 1km radius of a health centre in Abuja sum(health_facilities_Abuja_pop$pop) ## [1] 4294393 3.7 End of module exercises 1. What is the primary purpose of the .prj file in a shapefile set, and which package is commonly used to handle this file type in R? Stores map object information, handled by raster package Stores attribute information, handled by sp package Stores projection information, handled by sf package Stores metadata information, handled by terra package 2. Create a map of Nigeria that combines the raster layer for building count and the vector layer for region. 3. Create a choropleth map that shows the variation in population density for Nigeria at region level. Export the results as a .png file. 4. How do you plot a raster dataset using tmap, ensuring it can handle large datasets efficiently? tm_shape(raster_data) + tm_fill() tm_shape(raster_data) + tm_raster() tm_shape(raster_data) + tm_polygons() tm_shape(raster_data) + tm_dots() 3.8 Useful resources Types of spatial data: Spatial Statistics For Data Science: Theory and Practice with R Basic GIS concepts: esri UK GIS in R: Nick Eubank Using spatial data in R: Using Spatial Data with R Visualisation of spatial data: Spatial Data Science Visualisation of spatial data: Making Maps with R Interactive maps with mapview: mapview Basic geoprocessing: Geoprocessing in R Coordinate reference systems: QGIS Documentation "],["introduction-to-statistical-modelling-with-implementation-in-r.html", "4 Introduction to Statistical Modelling with Implementation in R 4.1 Concept of statistical modelling 4.2 Simple regression 4.3 Multiple regression 4.4 Generalised linear regression 4.5 Model predictions 4.6 Model selection 4.7 Stepwise regression 4.8 Cross-validation 4.9 Hierarchical regression 4.10 Useful resources", " 4 Introduction to Statistical Modelling with Implementation in R This module contains an introduction to the concepts of statistical modelling, covering the key types of models, starting with simple linear regression and progressing to more complex models with implementation in R. Module 4 also introduces various methodologies for model selection, model prediction and cross-validation. 4.1 Concept of statistical modelling 4.1.1 Overview of statistical modelling Statistical modelling, also known as regression analysis, can be described as a statistical technique used for exploring the relationship between a dependent variable (also called the outcome or response variable) and one or more independent variables (also called the explanatory variable or covariates). Data is viewed as being generated by some random process from which conclusions can be drawn. Statistical models help to understand these processes in greater depth, something that can be of interest for multiple reasons. Future predictions can be made from understanding the random process Decisions can be made based on the inference from the random process The random process itself may be of scientific interest For example, statistical modelling can be used to find out about the relationship between rainfall and crop yields, or the relationship between unemployment and poverty. Suppose for \\(i=1,\\cdots,n\\) observations we have the observed responses \\[ \\boldsymbol{y}=\\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix}, \\] where each \\(y_i\\) has an associated vector of values for \\(p\\) independent variables \\[ x_i=\\begin{pmatrix} x_{i1} \\\\ x_{i2} \\\\ \\vdots \\\\ x_{in} \\end{pmatrix}. \\] The observed responses \\(\\boldsymbol{y}\\) are assumed to be realisations of the random variables denoted by \\[ \\boldsymbol{Y}=\\begin{pmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{pmatrix}. \\] Alternatively, the random variable \\(Y_i\\) is the predicted (or fitted) value of the observed response \\(y_i\\). The use of either upper or lower case is important in statistical modelling as the case used indicates to whether the variable is fixed (e.g. the observed responses \\(\\boldsymbol{y}\\) ) or random (e.g. the predicted responses \\(\\boldsymbol{Y}\\) ). The variability in the independent variable(s) is not modelled, therefore, it is treated as a fixed (not random) variable, hence given in lower case, \\(x_i\\). 4.1.2 Correlation The relationship between variables can be determined by the extent to which they are related, also known as correlation. Correlation can either be positive, negative, or it does not exist (no correlation). What each type of correlation means is as follows: Positive: as the value of one variable increases, the value of another increases Negative: as the value of one variable decreases, the value of another decreases No correlation: the value of one variable does not affect the value of another variable A visual example of correlation can be seen in the figure below. (#fig:image correlation)Visual examples of the different types of correlation Exercise: Identify the dependent and independent variables in the following sentence: A researcher investigates the effects of school density on the grades of its pupils. 4.1.3 Scaled covariates When using more than one covariate in the modelling, it is important that they are given in the same units for comparison purposes. In the cases where they are not in the same units, covariates can be scaled to ensure the results are comparable. The two main methods for scaling are as follows: Centring: subtract each value from the mean value Z-score: subtract each value from the mean value and divide by the standard deviation 4.1.4 Uncertainty Uncertainty and error is unavoidable when estimating and predicting. The less error in your predictions, the more reliable the results are, therefore, it is important to measure the error margin when modelling. This can be done by measuring how close (or far) the predicted value is from the mean value. Alternatively, confidence intervals (in frequentest/classical statistics) or credible intervals (in Bayesian statistics) can be used. In this chapter, the focus is on frequentest statistics so confidence intervals will be used, Bayesian statistics will be introduced in Module 6. 4.2 Simple regression There are some important assumptions required for simple regression modelling, given as follows. Normality of the response/residuals (e.g. histograms, Q-Q plot) Linear relationship between response and predictors (e.g. scatter plot, residuals vs fitted plot) Homoscedasticity - constant variance of the residuals (e.g. spread-location plot) Independence between predictors - no multicollinearity (e.g. use Corr(X1, X2) function, should be approximately equal to 1) To check that the response follows the normality assumption, plots can be used. If the normality assumption holds, the shape of the histogram will resemble the shape of a bell curve, as seen in the example below using the birth data. This dataset has the dependent variable Weight for the birth weight (in grams) of 24 newborn babies with 2 independent variables, Sex for the sex and Age for the gestational age (in weeks) of the babies, where Sex is a categorical variable with values \\[\\text{Sex} = \\begin{cases} 1 &amp; \\text{if male, or} \\\\ 2 &amp; \\text{if female}. \\end{cases}\\] #create birth weight dataset birth &lt;- data.frame( Sex= c(1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2), Age=c(40,38,40,35,36,37,41,40,37,38,40,38,40,36,40,38,42,39,40,37,36,38,39,40), Weight=c(2968,2795,3163,2925,2625,2847,3292,3473,2628,3176,3421,2975,3317, 2729,2935,2754,3210,2817,3126,2539,2412,2991,2875,3231)) #plot a histogram of the birth weights hist(birth$Weight, breaks=10, main=&quot;Histogram of Birth Weight&quot;, xlab = &quot;Birth Weight&quot;) Alternatively, a Q-Q (quantile-quantile) plot can be used to assess the validity of the normality assumption, which plots the theoretical quantiles against the sample quantiles. If the normality assumption holds, the points on the plot will approximately follow a straight line. To plot this in R, the function qqnorm() can be used with an argument for the response that is being assessed, with the function qqline() being used with the response as an argument to add a reference line, making it easier to see whether the relationship is in fact linear. This is demonstrated below with the birth dataset, where it can be seen that the normality assumption holds given that the points on the Q-Q plot approximately follow the straight line given. However, in the case where the normality assumption does not hold, transformations such as the log-transformation can be used. If transformations are not appropriate, such as when the data is count or binary, alternative methods of modelling are required, leading to generalised linear modelling discussed later in this module. #qq plot of the birth weight data qqnorm(birth$Weight) #add a reference line to the plot qqline(birth$Weight, col=&quot;blue&quot;) Using the plotting techniques discussed in Module 2, exploratory analysis can be conducted on the data, checking the assumption of a linear relationship between the response and predictors. Through using the plot() function with the response and the predictor you wish to test as arguments, if the resulting scatter plot has a linear trend, then this assumption is met. #scatter plot for linear assumption plot(birth$Age, birth$Weight, xlab = &quot;Gestational age (in weeks)&quot;, ylab = &quot;Birth weight (in grams)&quot;, main = &quot;Gestational age vs birth weight&quot;) The plot of gestational age vs birth weight shows that there is a positive correlation between the two variables, indicating that as gestational age increases, the birth weight also increases, where the points roughly follow a straight line indicating that the assumption of linearity between the response and predictor is met. Another way to test this assumption is to use the pairs() function in R, where when the dataset of interest is included as an argument, it provides a matrix of scatter plots that show the relationship of each combination of variables available in the data. #use the pairs function for the birth dataset pairs(birth) The scatter plots for Age and Weight indicate that the relationship between the variables is approximately linear and therefore the assumption is met. However, if this assumption was not upheld, then appropriate transformations could be used, for example, a log-transformation. This is discussed more in the non-linear regression modelling section. Alternative ways of checking assumptions require for the model to be fitted first, these methods will be discussed in the following sections. 4.2.1 Linear regression Simple linear regression is a regression model which has a linear relationship due to the dependent variable depends only on one independent variable, alternatively, the independent variable is conditioned only on one dependent variable. A key concept of the simple linear regression model is that it is assumed each response follows a normal distribution, \\(Y_i \\sim N(\\mu_i, \\sigma^2)\\). The simple linear regression model can be written as \\[ Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i,\\] where \\(\\beta_0\\) is the \\(y-\\)intercept, \\(\\beta_1\\) is the slope and the errors are independent and identically distributed as \\(\\epsilon\\sim N(0, \\sigma^2)\\) for \\(i=1,...,n\\). The error can be described as the random difference between the value of \\(Y_i\\) and \\(\\beta_0 +\\beta_1 x_i\\) which is the value of its conditional mean. An example of a simple linear regression can be seen below with the birth dataset. The birth weight of each baby can be modelled using either the gestational age or sex of the individual for a simple linear regression model. In this example we will focus on predicting the birth weight of a baby using the gestational age. \\[\\text{Weight}_i = \\beta_0 + \\beta_1\\text{Age}_i + \\epsilon_i\\] where \\(\\epsilon_i \\sim Normal(0, \\sigma)\\). To do linear modelling in R, the function lm() can be used with an argument for the model formula. When fitting a simple linear regression model, the function glm(), used for fitting generalised linear models, yields identical results to the function lm(). Generalised linear models will be discussed later in the module. #fit a linear model for birth weight by gestational age birth_simple_lm &lt;- lm(Weight ~ Age, data = birth) birth_simple_lm ## ## Call: ## lm(formula = Weight ~ Age, data = birth) ## ## Coefficients: ## (Intercept) Age ## -1485.0 115.5 #fit the linear model using the function glm birth_simple_glm &lt;- glm(Weight ~ Age, data = birth) birth_simple_glm #results are identical to the results from the lm function ## ## Call: glm(formula = Weight ~ Age, data = birth) ## ## Coefficients: ## (Intercept) Age ## -1485.0 115.5 ## ## Degrees of Freedom: 23 Total (i.e. Null); 22 Residual ## Null Deviance: 1830000 ## Residual Deviance: 816100 AIC: 324.5 (#fig:image lm summary)Reading the coefficients from the model output The output of the model gives the resulting coefficients. (Intercept) corresponds to the \\(y-\\)intercept \\(\\beta_0\\), in this case \\(\\beta_0=-1485.0\\) meaning that if \\(\\text{Age}=0\\) then the birth weight would be -1485.0g. The coefficient Age corresponds to the slope \\(\\beta_1\\), in this case \\(\\beta_1=115.5\\), meaning that for each (1) unit increase in Age, the birth weight will increase by 115.5g. Including the linear model as an argument in the function summary() in R provides summary statistics for the linear model. These statistics include the previously given coefficients, as well as the corresponding standard errors and p-values (probability values) for the coefficients (among other information). This function is beneficial for exploring whether the independent variables are statistically significant and improve the model, as a small p-value (typically chosen to be less than 0.05) indicates that there is a statistically significant relationship between variables. #print summary for linear model summary(birth_simple_lm) ## ## Call: ## lm(formula = Weight ~ Age, data = birth) ## ## Residuals: ## Min 1Q Median 3Q Max ## -262.03 -158.29 8.35 88.15 366.50 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1485.0 852.6 -1.742 0.0955 . ## Age 115.5 22.1 5.228 3.04e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 192.6 on 22 degrees of freedom ## Multiple R-squared: 0.554, Adjusted R-squared: 0.5338 ## F-statistic: 27.33 on 1 and 22 DF, p-value: 3.04e-05 (#fig:image coef summary)Interpreting the summary output In this case, it can be seen that the p-value for the Age covariate is \\(&lt;0.05\\) and is therefore statistically significant to the 5% level and improves the model (and therefore should remain in the model). A line of best fit using the linear model can be added to the above plot through using the function abline(), adding the entire linear model as an argument of the function. #plot birth weight against gestational age plot(birth$Weight ~ birth$Age, xlab = &quot;Gestational age (in weeks)&quot;, ylab = &quot;Birth weight (in grams)&quot;, main = &quot;Gestational age vs birth weight&quot;) #add line for linear model abline(birth_simple_lm) To create diagnostic plots for the linear model to check the required assumptions, the function plot() can be used with the model as the argument. This creates four plots, each having a different purpose. To produce only one of the four plots, add an argument for the plot index you wish to show (the fourth plot is combined of Cook’s distance and Residuals vs Leverage plots, to index the fourth plot, include 5 as an argument). #model diagnostic plot par(mfrow = c(2, 2)) plot(birth_simple_lm) The first plot is the Residuals vs Fitted plot, used to check the linearity assumption. #residuals plot plot(birth_simple_lm, 1) In this plot, there should be no pattern and the red line should be approximately horizontal at zero for the normality assumption to hold. The residuals plot for the birth data is quite horizontal and is based around zero, however, there is a slight indication of a pattern meaning that there could be some problem with the linear model. This problem could be many things, possibly indicating that the relationship is not linear and instead quadratic for example. The second plot is the Normal Q-Q plot, similar to that produced by the qqnorm() function discussed previously, but now is used to check the normality assumption of the residuals. #normal Q-Q plot plot(birth_simple_lm, 2) The plot for the birth data does show some problems with the normality assumption given that the points to not all approximately fall on the reference line. This means that the normality assumption cannot be assumed. The third plot is the Scale-Location plot, or the Spread-Location plot, used to verify the homoscedasticity (homogeneity of variance). For the assumption to be upheld, the line should be approximately horizontal with the points equally dispersed. #scale-location plot plot(birth_simple_lm, 3) In this case, the points are approximately equally spread out with the reference line being mostly horizontal, indicating that the homoscedasticity assumption may be upheld. Finally, the fourth plot is the Residuals vs Leverage plot, used for identifying outlier points that have high leverage. These are points that may impact the results of the regression analysis if they are included or excluded, although not all outliers are influential to alter the results. This plot identifies the 3 most extreme values. #residuals vs leverage plot plot(birth_simple_lm, 5) This plot identifies the 3 most extreme points (#4, #8 and #21), where point number 4 is identified as influential through using the measure of Cook’s distance. There is evidence that this point will alter the results of the regression analysis so there should be some consideration whether to include this point or not. 4.2.2 Polynomial regression The data is not always best described by a linear relationship between the dependent and independent variables, for example, there could be a quadratic relationship between the variables. The following are examples of polynomial regression models: A quadratic function: \\[ Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\epsilon_i.\\] A polynomial of degree 4: \\[Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3 + \\beta_4 x_i^4+ \\epsilon_i.\\] For example, to fit a simple polynomial regression model with a quadratic function to the birth weight dataset used above, the quadratic term needs to be created and added to the data, then the function lm() can be used inputting the model formula. Alternatively, the quadratic term can be included in the formula within the function I() which lets R know to include that term as a separate term within the model. Another option is to use the function poly() with arguments for the independent variable and the degree of polynomial wanted, making the code more efficient when higher order polynomials are used in particular, instead of typing out a long equation with many terms. #create the quadratic term and add to data before modelling birth$Age2 &lt;- birth$Age^2 birth_quad_lm1 &lt;- lm(Weight ~ Age + Age2, data = birth) summary(birth_quad_lm1) ## ## Call: ## lm(formula = Weight ~ Age + Age2, data = birth) ## ## Residuals: ## Min 1Q Median 3Q Max ## -276.86 -156.62 5.87 99.72 340.27 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7288.668 17631.783 0.413 0.684 ## Age -342.639 919.905 -0.372 0.713 ## Age2 5.969 11.980 0.498 0.624 ## ## Residual standard error: 196 on 21 degrees of freedom ## Multiple R-squared: 0.5592, Adjusted R-squared: 0.5173 ## F-statistic: 13.32 on 2 and 21 DF, p-value: 0.0001837 #alternatively, include quadratic term within I() birth_quad_lm2 &lt;- lm(Weight ~ Age + I(Age^2), data = birth) summary(birth_quad_lm2) #produces the same model ## ## Call: ## lm(formula = Weight ~ Age + I(Age^2), data = birth) ## ## Residuals: ## Min 1Q Median 3Q Max ## -276.86 -156.62 5.87 99.72 340.27 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7288.668 17631.783 0.413 0.684 ## Age -342.639 919.905 -0.372 0.713 ## I(Age^2) 5.969 11.980 0.498 0.624 ## ## Residual standard error: 196 on 21 degrees of freedom ## Multiple R-squared: 0.5592, Adjusted R-squared: 0.5173 ## F-statistic: 13.32 on 2 and 21 DF, p-value: 0.0001837 #alternatively, include quadratic term within poly() birth_quad_lm3 &lt;- lm(Weight ~ poly(x = Age, degree = 2, raw = TRUE), data = birth) summary(birth_quad_lm3) #produces the same model ## ## Call: ## lm(formula = Weight ~ poly(x = Age, degree = 2, raw = TRUE), ## data = birth) ## ## Residuals: ## Min 1Q Median 3Q Max ## -276.86 -156.62 5.87 99.72 340.27 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7288.668 17631.783 0.413 0.684 ## poly(x = Age, degree = 2, raw = TRUE)1 -342.639 919.905 -0.372 0.713 ## poly(x = Age, degree = 2, raw = TRUE)2 5.969 11.980 0.498 0.624 ## ## Residual standard error: 196 on 21 degrees of freedom ## Multiple R-squared: 0.5592, Adjusted R-squared: 0.5173 ## F-statistic: 13.32 on 2 and 21 DF, p-value: 0.0001837 The output from the summary() function works the same for non-linear models as it does for linear, with the coefficient estimates corresponding to the values of \\(\\alpha\\) for the intercept and the \\(\\beta\\) value(s) for the covariate(s). In this case, it can be seen that adding the quadratic term for age does not improve upon the linear model given that the p-value (Pr(&gt;|t|)) is not statistically significant. This conclusion is reasonable given that the line of best fit for the linear model fits the birth weight data well and the data does not show a quadratic trend. The welding dataset below contains information from the Welding Institute in Abingdon, providing \\(n=21\\) measurements of currents in amps with the corresponding minimum diameter of the weld. Given that the diameter of the weld depends on the amount of current, Current is the independent variable and Diameter is the dependent variable. #create welding dataset welding &lt;- data.frame(Current = c(7.82, 8.00, 7.95, 8.07, 8.08, 8.01, 8.33, 8.34, 8.32, 8.64, 8.61, 8.57, 9.01, 8.97, 9.05, 9.23, 9.24, 9.24, 9.61, 9.60, 9.61), Diameter = c(3.4, 3.5, 3.3, 3.9, 3.9, 4.1, 4.6, 4.3, 4.5, 4.9, 4.9, 5.1, 5.5, 5.5, 5.6, 5.9, 5.8, 6.1, 6.3, 6.4, 6.2)) The welding data can be modelled in the same way as the birth weight dataset, using the function lm(), as seen in the example below. #simple linear model weld_simple_lm &lt;- lm(Diameter ~ Current, data = welding) summary(weld_simple_lm) ## ## Call: ## lm(formula = Diameter ~ Current, data = welding) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.42623 -0.07282 0.01637 0.08269 0.34586 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -9.45427 0.65526 -14.43 1.09e-11 *** ## Current 1.65793 0.07531 22.01 5.53e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2012 on 19 degrees of freedom ## Multiple R-squared: 0.9623, Adjusted R-squared: 0.9603 ## F-statistic: 484.6 on 1 and 19 DF, p-value: 5.529e-15 #quadratic model weld_quad_lm &lt;- lm(Diameter ~ Current + I(Current^2), data = welding) summary(weld_quad_lm) ## ## Call: ## lm(formula = Diameter ~ Current + I(Current^2), data = welding) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.31023 -0.10023 -0.00496 0.09880 0.35197 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -41.5662 9.9392 -4.182 0.000560 *** ## Current 9.0430 2.2833 3.960 0.000917 *** ## I(Current^2) -0.4227 0.1306 -3.236 0.004589 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1644 on 18 degrees of freedom ## Multiple R-squared: 0.9761, Adjusted R-squared: 0.9735 ## F-statistic: 368.3 on 2 and 18 DF, p-value: 2.501e-15 Unlike with the birth weight quadratic model, the addition of a quadratic term to the model for the welding data improves the fit of the model, given that both the linear and quadratic terms are statistically significant at the 5% significance level. This improved fit can be demonstrated graphically using the function predict() with arguments for the model you wish to predict from and a new dataset with a range of values you wish to predict the values of the dependent variable from (typically a sequence of evenly spaced numbers from the minimum to maximum values of your independent variable). #made a new dataset weld.new &lt;- data.frame(Current = seq(from = min(welding$Current), to = max(welding$Current), length.out = 100)) #use the predict function pred_simple_lm &lt;- predict(weld_simple_lm, newdata = weld.new) pred_quad_lm &lt;- predict(weld_quad_lm, newdata = weld.new) #basic plot for relationship between variables plot(Diameter ~ Current, data = welding) #add lines for each of the sets of predicted values lines(pred_simple_lm ~ weld.new$Current, col = &quot;blue&quot;, lty = 2, lwd = 2) lines(pred_quad_lm ~ weld.new$Current, col = &quot;red&quot;, lty = 1, lwd = 2) #add a legend for clarity legend(&quot;topleft&quot;, c(&quot;Linear&quot;, &quot;Quadratic&quot;), col = c(&quot;blue&quot;, &quot;red&quot;), lty = c(2,1), lwd = 2) The plot showing lines of best fit for both the simple linear model and the quadratic model demonstrate the improved fit of the quadratic model, with the added flexibility of the curve matching the trend of the data better. This process can be extended for including higher degrees of polynomials in the regression models, although it is important to be wary of overfitting the model to the data as this risks the model only having use for inference to the original dataset. 4.2.3 Non-linear regression The relationships being explored are not always best described by a linear relationship. In these cases, the data can be transformed, for example using logarithms, square roots and exponentials, to fit a non-linear model which is more flexible, potentially explaining the relationship between variables better. For a regression model to be non-linear, \\(Y\\) must be a non-linear function of the parameters (e.g. \\(\\beta_0\\) and \\(\\beta_1\\)), however, \\(Y\\) can still be a linear function of the covariates \\(x\\). The following are examples of non-linear regression models: Squared value of the \\(\\beta\\) coefficient: \\[Y_i = \\beta_0 + \\beta_1^2x_i + \\epsilon_i.\\] Logarithmic: \\[\\log(Y_i) = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\] which implies \\[Y_i = \\exp(\\beta_0 + \\beta_1 x_i + \\epsilon_i)=\\exp(\\beta_0)\\exp(\\beta_1 x_i)\\exp(\\epsilon)\\] a relationship which is multiplicative, meaning that a unit increase in \\(x_i\\) corresponds to \\(Y_i\\) being multiplied by a value of \\(\\exp(\\beta x_i)\\), instead of an additive effect of \\(\\beta x_i\\) like with a linear model. Square root: \\[Y_i^{1/2}=\\beta_0 + \\beta_1 x_i + \\epsilon_i.\\] Negative reciprocal: \\[-\\frac{1}{Y_i}= \\beta_0 + \\beta_1 x_i + \\epsilon_i.\\] The function nls() can be used for non-linear regression models and estimate the parameters via a non-linear least squares approach (a non-linear approach to finding the line of best fit for the given data). To demonstrate this approach, the Michaelis-Menten equation for kinetics given below can be used, given that there is a non-linear relationship between the dependent variable and the parameters. \\[ Y_i = \\frac{\\beta_0 x_i}{\\beta_1+x_i} \\] #simulate some data set.seed(100) x&lt;-1:100 y&lt;-((runif(1,20,30)*x)/(runif(1,0,20)+x)) + rnorm(100,0,1) #model the data using the function nls(), if no start values are given, a #warning may occur, but R will just choose the start values itself instead nonlinear_mod &lt;- nls(y ~ a*x/(b+x)) ## Warning in nls(y ~ a * x/(b + x)): No starting values specified for some parameters. ## Initializing &#39;a&#39;, &#39;b&#39; to &#39;1.&#39;. ## Consider specifying &#39;start&#39; or using a selfStart model #summary of the non-linear model summary(nonlinear_mod) ## ## Formula: y ~ a * x/(b + x) ## ## Parameters: ## Estimate Std. Error t value Pr(&gt;|t|) ## a 22.9682 0.1928 119.11 &lt;2e-16 *** ## b 4.9492 0.3010 16.44 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.023 on 98 degrees of freedom ## ## Number of iterations to convergence: 6 ## Achieved convergence tolerance: 2.029e-07 The summary function works in the same way as for the linear models, providing the estimated values of the model parameters, in this case, \\(\\beta_0=6.4946\\) and \\(\\beta_1 =1.0765\\). To visualise this equation with the non-linear regression model fitted, the function plot() can be used as with the linear models, with the addition of the function lines() as used with the polynomial regression models with an argument for the x-axis values and the predicted values. #plot the data plot(x,y) #add a line of best fit lines(x, predict(nonlinear_mod), col = &quot;red&quot;, lty = 1, lwd = 2) #add a linear regression line for comparison purposes abline(lm(y ~ x), col = &quot;blue&quot;, lty = 2, lwd = 2) #add a legend for clarity legend(&quot;bottomright&quot;, c(&quot;Non-linear&quot;, &quot;Linear&quot;), col = c(&quot;red&quot;, &quot;blue&quot;), lty = c(1,2), lwd = 2) It can be seen in the plot that the non-linear line fits the data very well, much better than the simple linear regression model added to the plot for comparison purposes. 4.3 Multiple regression Multiple regression can be described as an extension of simple regression where you still only have one dependent variable but there are multiple independent variables. For \\(p\\) independent variables, the model can be written as \\[ Y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + ... + \\beta_p x_{ip} + \\epsilon_i,\\] where \\(\\epsilon_i \\sim N(0, \\sigma^2)\\) for \\(i=1,...,n\\). An important assumption of multiple regression modelling is multicollinearity, meaning that it is assumed that the independent variables are not highly correlated with one another. If this assumption is not met, it can make identifying which variables better explain the dependent variable better much more challenging. The birth dataset can be used to demonstrate multiple linear regression given that there are 2 independent variables included in the data, Sex and Age, modelled as follows. \\[ Weight_i =\\beta_0 + \\beta_1 Sex_i + \\beta_2 Age_i + \\epsilon_i\\] To perform multiple linear regression in R, the function lm() can be used in the same way as for simple linear regression, however, with the additional variables given in the formula as in the code below. #multiple linear regression birth_multi_lm1 &lt;- lm(Weight ~ Sex + Age, data = birth) summary(birth_multi_lm1) ## ## Call: ## lm(formula = Weight ~ Sex + Age, data = birth) ## ## Residuals: ## Min 1Q Median 3Q Max ## -257.49 -125.28 -58.44 169.00 303.98 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1447.24 784.26 -1.845 0.0791 . ## Sex -163.04 72.81 -2.239 0.0361 * ## Age 120.89 20.46 5.908 7.28e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 177.1 on 21 degrees of freedom ## Multiple R-squared: 0.64, Adjusted R-squared: 0.6057 ## F-statistic: 18.67 on 2 and 21 DF, p-value: 2.194e-05 The output from the summary function shows that both independent variables are statistically significant at the 5% significance level, and hence birth weight depends on both sex and gestational age of the baby. Interpreting the results is done in the same way as for simple models, with the estimates corresponding to the coefficients as follows: \\(\\alpha\\)=-1447.24 \\(\\beta_1\\)=-163.04 \\(\\beta_2\\)=120.89 Through using the function update(), you can add or remove variables from a model without needing to re-fit the model yourself. This is particularly useful when you have a model with many parameters, where instead of needing to type out the model again with each of the parameters, you can simply update the existing model to either add another parameter or remove a parameter if it is not needed. To remove a variable from a model, you use the function in the form update(model, ~. - term). For example, to update the model given above to remove the covariate Age, you would use the below code. #remove the Age covariate from the multiple linear regression model birth_multi_lm2 &lt;- update(birth_multi_lm1, ~. - Age) birth_multi_lm2 ## ## Call: ## lm(formula = Weight ~ Sex, data = birth) ## ## Coefficients: ## (Intercept) Sex ## 3136.7 -112.7 Alternatively, if you wish to add a variable, you use the formula in the form update(model, ~. + term). For example, to add the term for Age back into the model, you would use the below code, which results in the same model as originally fitted. #remove the Age covariate from the multiple linear regression model update(birth_multi_lm2, ~. + Age) ## ## Call: ## lm(formula = Weight ~ Sex + Age, data = birth) ## ## Coefficients: ## (Intercept) Sex Age ## -1447.2 -163.0 120.9 The update() function also allows for the data being modelled to be updated through adding an argument for data =. This is demonstrated in the code below. #create an example dataset y &lt;- c(1:20) x1 &lt;- y^2 z1 &lt;- y*3 update_example1 &lt;- data.frame(x = x1, y = y, z = z1) #fit linear model example_mod1 &lt;- lm(y ~ x + z, data = update_example1) summary(example_mod1) ## Warning in summary.lm(example_mod1): essentially perfect fit: summary may be unreliable ## ## Call: ## lm(formula = y ~ x + z, data = update_example1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.774e-15 -4.318e-16 4.340e-17 5.935e-16 3.439e-15 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4.046e-15 1.160e-15 -3.488e+00 0.00282 ** ## x -3.823e-17 1.177e-17 -3.249e+00 0.00472 ** ## z 3.333e-01 8.480e-17 3.931e+15 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.559e-15 on 17 degrees of freedom ## Multiple R-squared: 1, Adjusted R-squared: 1 ## F-statistic: 1.368e+32 on 2 and 17 DF, p-value: &lt; 2.2e-16 #create new dataset x2 &lt;- y^3 z2 &lt;- y*4 update_example2 &lt;- data.frame(x = x2, y = y, z = z2) #update the dataset in the model example_mod2 &lt;- update(example_mod1, data = update_example2) summary(example_mod2) ## Warning in summary.lm(example_mod2): essentially perfect fit: summary may be unreliable ## ## Call: ## lm(formula = y ~ x + z, data = update_example2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.681e-16 -1.410e-18 3.702e-17 6.826e-17 1.084e-16 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.000e+00 7.773e-17 0.000e+00 1 ## x 0.000e+00 2.849e-20 0.000e+00 1 ## z 2.500e-01 3.016e-18 8.289e+16 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.204e-16 on 17 degrees of freedom ## Multiple R-squared: 1, Adjusted R-squared: 1 ## F-statistic: 2.293e+34 on 2 and 17 DF, p-value: &lt; 2.2e-16 4.4 Generalised linear regression For simple regression modelling, there is the assumption of normality for the dependent variable, however, this assumption is not always met, for example, with count data (e.g. number of people with a disease) which is often modelled with a Poisson distribution, or binary data (e.g. beetles killed or not killed) which is often modelled with a Bernoulli distribution. In these cases of non-normal data, alternative models are required, which is where generalised linear modelling is beneficial with its relaxed distributional assumption. The generalised linear model is written in the form \\[g(\\mu_i) = \\eta_i = \\boldsymbol{x}_i^T \\boldsymbol{\\beta},\\] where \\(\\mu_i=E(Y_i)\\) is the expected value of \\(Y_i\\), \\(\\eta_i\\) is the linear predictor and \\(g(\\mu_i)\\) is the link function between the distribution of \\(\\boldsymbol{Y}\\) and the linear predictor. An important assumption for generalised linear regression is that the dependent variable \\(\\boldsymbol{Y}\\) is assumed to be independent and a member of the exponential family (e.g. normal, Poisson, Bernoulli, geometric, exponential, …). The link function depends on the distribution of the data type and the dependent variable, where the table below provides the three main link functions and the corresponding data types and distributions. Data type Response family Link Name Continuous Normal/Gaussian/log-normal/gamma \\(g(\\mu)=\\mu\\) Identity Count Poisson \\(g(\\mu)=\\log(\\mu)\\) Log Binary Bernoulli/binomial \\(g(\\mu)=\\log\\left(\\frac{p}{1-p}\\right)\\) Logit To fit generalised linear models in R, the function glm() can be used, in a very similar way to the function lm() seen in earlier sections, but with the addition of a (exponential) family argument. The default family is normal, hence why if no family is specified, the functions glm() and lm() produce identical models. However, if the data is not normal, the exponential family that the data is a member of must be specified. When dealing with count data, the Poisson log-linear model is most commonly used, taking the following form. \\[ Y_i \\sim Poisson(\\mu_i), \\text{ } \\log(\\mu_i)=\\boldsymbol{x}_i^T \\boldsymbol{\\beta}\\] It is important to note that the Poisson distribution assumes that the mean and variance are equal. If over-dispersion (the variance is greater than the mean) is present, a negative-binomial model may be preferable. To demonstrate generalised linear modelling with count data, the ccancer dataset from the package GLMsData can be utilised. This dataset gives the count of deaths (Count) due to cancer within three different regions of Canada (Region), providing additional covariates for the gender of each individual (Gender) and the site of the cancer (Site). More information on this dataset can be found through using the help function. #install GLMsData package install.packages(&quot;GLMsData&quot;) #load the GLMsData package library(GLMsData) #import ccancer dataset data(ccancer) head(ccancer) ## Count Gender Region Site Population ## 1 3500 M Ontario Lung 11874400 ## 2 1250 M Ontario Colorectal 11874400 ## 3 0 M Ontario Breast 11874400 ## 4 1600 M Ontario Prostate 11874400 ## 5 540 M Ontario Pancreas 11874400 ## 6 2400 F Ontario Lung 11874400 To model this data, the function glm() can be used again, but with specifying the family argument as family = poisson as follows, where the following model is an intercept only model, including a 1 instead of any independent variables. #fit the glm for the ccancer data to explore the effect of gender on the #count of cancer deaths ccancer_glm1 &lt;- glm(Count ~ 1, family = &quot;poisson&quot;, data = ccancer) summary(ccancer_glm1) ## ## Call: ## glm(formula = Count ~ 1, family = &quot;poisson&quot;, data = ccancer) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 6.702984 0.006396 1048 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 35187 on 29 degrees of freedom ## Residual deviance: 35187 on 29 degrees of freedom ## AIC: 35380 ## ## Number of Fisher Scoring iterations: 6 Given that there are covariates included in this dataset, it is important to explore the relationship they may have with the response. The following model contains a main effect for gender, exploring the relationship between the gender of individuals and cancer deaths. #fit the glm for the ccancer data to explore the effect of gender on the #count of cancer deaths ccancer_glm2 &lt;- glm(Count ~ Gender, family = &quot;poisson&quot;, data = ccancer) summary(ccancer_glm2) ## ## Call: ## glm(formula = Count ~ Gender, family = &quot;poisson&quot;, data = ccancer) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 6.621406 0.009422 702.77 &lt;2e-16 *** ## GenderM 0.157000 0.012831 12.24 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 35187 on 29 degrees of freedom ## Residual deviance: 35037 on 28 degrees of freedom ## AIC: 35232 ## ## Number of Fisher Scoring iterations: 6 It can be seen in this model that the inclusion of the covariate Gender is statistically significant, therefore there is a relationship between the gender of an individual and the count of cancer deaths in Canada. Exercise: Fit a Poisson GLM using the ccancer dataset exploring the relationship between the count of cancer deaths and the covariates for the cancer site and region in Canada. Is the relationship between the dependent and independent variables significant? Another example of generalised linear modelling with count data can be seen as follows using the hodgkins dataset which contains information on 583 patients with Hodgkin’s disease. Within this information is the number of patients (count) with each combination of the histological type of disease (type) and the response to the treatment (rtreat). hodgkins &lt;- data.frame(count = c(74, 18, 12, 68, 16, 12, 154, 54, 58, 18, 10, 44), type = c(&quot;Lp&quot;, &quot;Lp&quot;, &quot;Lp&quot;, &quot;Ns&quot;, &quot;Ns&quot;, &quot;Ns&quot;, &quot;Mc&quot;, &quot;Mc&quot;, &quot;Mc&quot;, &quot;Ld&quot;, &quot;Ld&quot;, &quot;Ld&quot;), rtreat = c(&quot;positive&quot;, &quot;partial&quot;, &quot;none&quot;, &quot;positive&quot;, &quot;partial&quot;, &quot;none&quot;, &quot;positive&quot;, &quot;partial&quot;, &quot;none&quot;, &quot;positive&quot;, &quot;partial&quot;, &quot;none&quot;)) The information on the patients has been cross-classified, where the covariates type and rtreat are categorical variables with multiple levels each. #fit a glm to the hodgkins dataset including both covariates in the model hodgkins_glm1 &lt;- glm(count ~ type + rtreat, family = poisson, data = hodgkins) summary(hodgkins_glm1) ## ## Call: ## glm(formula = count ~ type + rtreat, family = poisson, data = hodgkins) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 2.8251 0.1413 19.993 &lt;2e-16 *** ## typeLp 0.3677 0.1533 2.399 0.0165 * ## typeMc 1.3068 0.1328 9.837 &lt;2e-16 *** ## typeNs 0.2877 0.1559 1.845 0.0650 . ## rtreatpartial -0.2513 0.1347 -1.866 0.0621 . ## rtreatpositive 0.9131 0.1055 8.659 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 367.247 on 11 degrees of freedom ## Residual deviance: 68.295 on 6 degrees of freedom ## AIC: 143.66 ## ## Number of Fisher Scoring iterations: 5 Alternatively, a generalised linear model can be fitted with both the main effects and an interaction. This model is known as the full or saturated model, where the interaction term can be included in addition to the main effects using a colon : between the variables you wish to include an interaction term for. This method is beneficial for when you just want to include an interaction, not necessarily the corresponding main effects, however, if you want to include both the interaction and corresponding main effects to a model, an asterisk * can be used between the chosen covariates. Both these methods are demonstrated below and return the same model. #fit the saturated Poisson GLM to the hodgkins dataset with a colon hodgkins_glm2 &lt;- glm(count ~ type + rtreat + type:rtreat, family = poisson, data = hodgkins) summary(hodgkins_glm2) ## ## Call: ## glm(formula = count ~ type + rtreat + type:rtreat, family = poisson, ## data = hodgkins) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3.7842 0.1508 25.101 &lt; 2e-16 *** ## typeLp -1.2993 0.3257 -3.990 6.62e-05 *** ## typeMc 0.2763 0.1999 1.382 0.167031 ## typeNs -1.2993 0.3257 -3.990 6.62e-05 *** ## rtreatpartial -1.4816 0.3503 -4.229 2.34e-05 *** ## rtreatpositive -0.8938 0.2798 -3.195 0.001400 ** ## typeLp:rtreatpartial 1.8871 0.5115 3.689 0.000225 *** ## typeMc:rtreatpartial 1.4101 0.3981 3.542 0.000397 *** ## typeNs:rtreatpartial 1.7693 0.5182 3.414 0.000640 *** ## typeLp:rtreatpositive 2.7130 0.4185 6.483 9.00e-11 *** ## typeMc:rtreatpositive 1.8703 0.3194 5.856 4.75e-09 *** ## typeNs:rtreatpositive 2.6284 0.4199 6.260 3.86e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 3.6725e+02 on 11 degrees of freedom ## Residual deviance: 1.9540e-14 on 0 degrees of freedom ## AIC: 87.363 ## ## Number of Fisher Scoring iterations: 3 #fit the saturated Poisson GLM to the hodgkins dataset with an asterisk hodgkins_glm3 &lt;- glm(count ~ type*rtreat, family = poisson, data = hodgkins) summary(hodgkins_glm3) ## ## Call: ## glm(formula = count ~ type * rtreat, family = poisson, data = hodgkins) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3.7842 0.1508 25.101 &lt; 2e-16 *** ## typeLp -1.2993 0.3257 -3.990 6.62e-05 *** ## typeMc 0.2763 0.1999 1.382 0.167031 ## typeNs -1.2993 0.3257 -3.990 6.62e-05 *** ## rtreatpartial -1.4816 0.3503 -4.229 2.34e-05 *** ## rtreatpositive -0.8938 0.2798 -3.195 0.001400 ** ## typeLp:rtreatpartial 1.8871 0.5115 3.689 0.000225 *** ## typeMc:rtreatpartial 1.4101 0.3981 3.542 0.000397 *** ## typeNs:rtreatpartial 1.7693 0.5182 3.414 0.000640 *** ## typeLp:rtreatpositive 2.7130 0.4185 6.483 9.00e-11 *** ## typeMc:rtreatpositive 1.8703 0.3194 5.856 4.75e-09 *** ## typeNs:rtreatpositive 2.6284 0.4199 6.260 3.86e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 3.6725e+02 on 11 degrees of freedom ## Residual deviance: 1.9540e-14 on 0 degrees of freedom ## AIC: 87.363 ## ## Number of Fisher Scoring iterations: 3 It can be seen from looking at the results from both the main effects model and the saturated model, particularly the p-values, that the terms in the saturated model are more statistically significant, indicating that the saturated model is a better fit for the data. Additionally, through testing whether the interaction term is needed, you are able to test whether the covariates are independent from one another or whether they are correlated/associated. In this case, the inclusion of the interaction term improves the model and all interaction terms are statistically significant, therefore there is evidence that the covariates type and rtreat are independent from one another. If the data is binary, meaning that there are only two possible outcomes, the family can be specified as family = binomial to fit a binomial logistic regression model with a logit link, the default link for a binomial family which assumes that the errors follow a logistic distribution. Alternatively, a probit link can be used, through changing the family argument to be family = binomial(link=\"probit\"), which instead assumes that the errors follow a normal distribution, however, this is used less frequently. The form of a binomial logistic regression model is given as \\[Y_i|n_i, p_i \\sim Binomial(n_i, p_i), \\text{ logit}(p_i)=\\log\\left(\\frac{p_i}{1-p_i}\\right).\\] This can be seen in the code below using the beetles dataset, \\(Y_i\\) is the number of beetles killed, \\(n_i\\) is the number of beetles exposed and \\(\\boldsymbol{x}_i\\) is the dose. beetles &lt;- data.frame(dose = c(1.6907, 1.7242, 1.7552, 1.7842, 1.8113, 1.8369, 1.861, 1.8839), exposed = c(59, 60, 62, 56, 63, 59, 62, 60), killed = c(6, 13, 18, 28, 52, 53, 61, 60)) There are two ways to fit the binomial logistic regression model. Firstly, is to model the proportion of “successes” (in this example, it is the proportion of beetles killed) and weight by the number of trials (in this example, it is the number of beetles exposed). #compute the proportion of beetles killed beetles$prop_killed &lt;- beetles$killed / beetles$exposed #fit a binomial logistic regression model beetles_glm_props &lt;- glm(prop_killed ~ dose, data = beetles, family = binomial, weights = exposed) summary(beetles_glm_props) ## ## Call: ## glm(formula = prop_killed ~ dose, family = binomial, data = beetles, ## weights = exposed) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -60.717 5.181 -11.72 &lt;2e-16 *** ## dose 34.270 2.912 11.77 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 284.202 on 7 degrees of freedom ## Residual deviance: 11.232 on 6 degrees of freedom ## AIC: 41.43 ## ## Number of Fisher Scoring iterations: 4 Alternatively, the independent variable can be given as a matrix with two columns, one for the number of “successes” and the other for “failures” (in this example, a success is a beetle killed and a failure is a beetle not killed). #fit a binomial logistic regression model with two columns to response beetles_glm_matrix &lt;- glm(cbind(killed, exposed - killed) ~ dose, data = beetles, family = binomial) summary(beetles_glm_matrix) ## ## Call: ## glm(formula = cbind(killed, exposed - killed) ~ dose, family = binomial, ## data = beetles) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -60.717 5.181 -11.72 &lt;2e-16 *** ## dose 34.270 2.912 11.77 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 284.202 on 7 degrees of freedom ## Residual deviance: 11.232 on 6 degrees of freedom ## AIC: 41.43 ## ## Number of Fisher Scoring iterations: 4 As seen from the summaries for the binomial logistic regression models from each approach, the approaches yield identical results, so it is unimportant which approach is taken. 4.5 Model predictions 4.5.1 Predictions with the formula and coefficients Values of the dependent variable can be predicted through inputting the coefficient estimates found from the model summaries into the model formulae, given values of the independent variable(s). To demonstrate predictions using just the model formula and the resulting coefficient estimates, firstly the simple linear regression model with the birth dataset will be used. As a reminder, the model was given as follows, fitted with the lm() function in R. \\[\\text{Weight}_i = \\beta_0 + \\beta_1\\text{Age}_i + \\epsilon_i\\] The coefficients from the linear model given by the summary() function can then be used to predict the value of the birth weight for a baby at a given gestational age. #summary of the birth weight simple linear regression model summary(birth_simple_lm) ## ## Call: ## lm(formula = Weight ~ Age, data = birth) ## ## Residuals: ## Min 1Q Median 3Q Max ## -262.03 -158.29 8.35 88.15 366.50 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1485.0 852.6 -1.742 0.0955 . ## Age 115.5 22.1 5.228 3.04e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 192.6 on 22 degrees of freedom ## Multiple R-squared: 0.554, Adjusted R-squared: 0.5338 ## F-statistic: 27.33 on 1 and 22 DF, p-value: 3.04e-05 Given the coefficient estimates from the model, the birth weight of a baby for a given gestational age can be predicted using the following formula. \\[ \\text{Weight}_i = -1485.0 + 115.5 \\times \\text{Age}_i\\] Therefore, for example, a baby of gestational age 37.5 weeks, the birth weight is predicted as \\[Weight_i=-1485.0 + 115.5 \\times 37.5 = 2846.25g.\\] This method of prediction does not just work on simple linear regression models but can be used for regression modelling in general. For example, the multiple regression model for the birth dataset birth_multi_lm1, which has the following formula. \\[ Weight_i =\\beta_0 + \\beta_1 Sex_i + \\beta_2 Age_i + \\epsilon_i\\] If for example, you wanted to predict the birth weight of a baby girl at a gestational age of 38 weeks, you would use the following formula. \\[Weight = -1447.24 -163.04\\times 1+120.89\\times 38 = 2983.54g\\] Exercise: What is the expected birth weight for a baby boy at a gestational age of 39.5 weeks? This method of prediction also works with generalised linear models using the results from a model fitted with the glm() function. To demonstrate this, the Poisson GLM fitted to the ccancer dataset exploring the relationship between gender and counts of cancer deaths is used. As a reminder, the model summary output is as follows. #GLM for ccancer data gender vs count summary(ccancer_glm2) ## ## Call: ## glm(formula = Count ~ Gender, family = &quot;poisson&quot;, data = ccancer) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 6.621406 0.009422 702.77 &lt;2e-16 *** ## GenderM 0.157000 0.012831 12.24 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 35187 on 29 degrees of freedom ## Residual deviance: 35037 on 28 degrees of freedom ## AIC: 35232 ## ## Number of Fisher Scoring iterations: 6 Using the resulting coefficient estimates from this model in the formula below to estimate the count of cancer deaths for a given gender. Since the log-link was used for this model, the coefficients require exponentiation in order to transform the log-count to just the count. \\[ \\begin{aligned} \\log(\\hat{\\mu}) &amp;= 6.621406 + 0.157000 \\times x_1 \\\\ \\hat{\\mu} &amp;= \\exp( 6.621406 + 0.157000 \\times x_1) \\end{aligned} \\] where \\(x_1=0\\) if the individual is female and \\(x_1=1\\) if the individual is male. Using this formula, the expected count of cancer deaths for women in from the dataset is \\(\\hat{\\mu}=exp( 6.621406 + 0.157000)=879\\) and for men is \\(\\hat{\\mu}=exp( 6.621406)=751\\). 4.5.2 Predictions with functions As mentioned in previous sections, you can also predict values using functions, particularly the predict() function. To use this, include the model you wish to predict from as an argument, in addition to values of data you wish to predict the values of the dependent variable from as newdata. The use of this function is demonstrated in the code below, where for the birth dataset, birth weight is predicted for a range of values from the quadratic model, birth_quad_lm2, fitted with the lm() function. Firstly, a new dataset needs to be created. This dataset needs to contain the value(s) for which the dependent variable will be predicted using. If for example you wish to estimate the birth weight of a baby with gestational ages 36.5, 37.5 and 38.5, you can create a new dataset containing just these values, as in the code given below. #made a new dataset birth.new1 &lt;- data.frame(Age = c(36.5, 37.5, 38.5)) birth.new1 ## Age ## 1 36.5 ## 2 37.5 ## 3 38.5 Then, the predict function can be used, including the name of the model, birth_quad_lm2, and the new dataset to predict from, birth.new1. #use the predict function birth_predict_quad1 &lt;- predict(birth_quad_lm2, newdata = birth.new1) The values of these predictions can be given as a table for ease of viewing using the data.table() function in the data.table package. To use this function, include the variables you wish to display as arguments and assign the variables names in the function for clarity. #install data.table package install.packages(&quot;data.table&quot;) #load the data.table package library(data.table) #create a table for the predictions data.table(birth.new1, Weight = birth_predict_quad1) ## Age Weight ## &lt;num&gt; &lt;num&gt; ## 1: 36.5 2733.896 ## 2: 37.5 2832.927 ## 3: 38.5 2943.894 Alternatively, a range of values can be given as the new dataset for assessing the fit of a given model. To demonstrate this, the birth weight will be predicted for a sequence of gestational age values, starting from the minimum observed age to the maximum observed age with 50 values in total. #made a new dataset birth.new2 &lt;- data.frame(Age=seq(from = min(birth$Age), to = max(birth$Age), length.out = 50)) head(birth.new2) ## Age ## 1 35.00000 ## 2 35.14286 ## 3 35.28571 ## 4 35.42857 ## 5 35.57143 ## 6 35.71429 Then, the predict function can be used, including the name of the model, birth_quad_lm2, and the new dataset to predict from, birth.new2. #use the predict function birth_predict_quad2 &lt;- predict(birth_quad_lm2, newdata = birth.new2) birth_predict_quad2 ## 1 2 3 4 5 6 7 8 9 10 11 ## 2607.732 2618.590 2629.693 2641.038 2652.627 2664.460 2676.537 2688.857 2701.421 2714.228 2727.279 ## 12 13 14 15 16 17 18 19 20 21 22 ## 2740.574 2754.112 2767.894 2781.919 2796.188 2810.701 2825.457 2840.457 2855.700 2871.187 2886.918 ## 23 24 25 26 27 28 29 30 31 32 33 ## 2902.892 2919.110 2935.572 2952.277 2969.226 2986.418 3003.854 3021.534 3039.457 3057.624 3076.034 ## 34 35 36 37 38 39 40 41 42 43 44 ## 3094.688 3113.586 3132.727 3152.112 3171.741 3191.613 3211.729 3232.088 3252.691 3273.537 3294.628 ## 45 46 47 48 49 50 ## 3315.961 3337.539 3359.360 3381.424 3403.733 3426.284 To demonstrate the fit of this model visually, create a plot depicting the relationship between the two variables being explored, with the predicted values added to the plot. If the model fits the data well, the predicted values line should match the trend of the data well. For comparative purposes, a line is added for the simple linear regression for this data to demonstrate the difference in fit of the two models and how the quadratic model fits the data better. #basic plot for relationship between variables plot(Weight ~ Age, data = birth) #add lines for each of the sets of predicted values lines(birth_predict_quad2 ~ birth.new2$Age, col = &quot;red&quot;, lty = 1, lwd = 2) lines(predict(birth_simple_lm, newdata = birth.new2) ~ birth.new2$Age, col = &quot;blue&quot;, lty = 2, lwd = 2) #add a legend for clarity legend(&quot;topleft&quot;, c(&quot;Quadratic&quot;, &quot;Linear&quot;), col = c(&quot;red&quot;, &quot;blue&quot;), lty = c(1,2), lwd = 2) As with the other method for predictions, this method also works with generalised linear models fitted with the glm() function. To demonstrate this, the binomial logistic regression model fitted to the beetles dataset is used. For example, predicting in the same way as before but for doses of 1.7 and 1.8, the following code would be used. #create a new data frame to predict with doses of 1.7 and 1.8 beetles_newdata1 &lt;- data.frame(dose = c(1.7, 1.8)) #predict function predict(beetles_glm_props, newdata = beetles_newdata1) ## 1 2 ## -2.4579008 0.9691318 However, these predicted values are not what is expected for the probability of death. This is due to the default type of the predict function being type = link which returns predicted values of \\(\\text{logit}(p(x))\\). To return the predicted values of the probabilities instead, type = \"response\" needs to be added as an argument, as follows. #predict the probabilities beetles_predict_glm &lt;- predict(beetles_glm_props, newdata = beetles_newdata1, type = &quot;response&quot;) beetles_predict_glm ## 1 2 ## 0.07886269 0.72494641 These predictions are much more what you would expect for probabilities, given that they are between 0 and 1. For more information on probability, see Module 5. The fit of this GLM can be assessed in the same way as for the models fitted using the lm() function, through predicting values for a wider range of values and fitting the predicted values to a plot. To demonstrate the goodness-of-fit of the GLM and why it is important to fit a GLM over a simple linear regression model, a linear model is fitted with weights added for the number of beetles exposed, with the line of best fit of this linear regression model also added to the plot. #fit a simple linear regression model for the beetles data beetles_simple_lm &lt;- lm(prop_killed ~ dose , data = beetles, weights = exposed) #create a new dataset beetles_newdata2 &lt;- data.frame(dose = seq(min(beetles$dose) - .2, max(beetles$dose) + .2, length = 100)) #plot the relationship between the dosage and proportion of beetles killed plot(prop_killed ~ dose, data = beetles, ylim = c(-0.5, 1.5), xlim = c(1.6, 2), ylab = &quot;Proportion killed&quot;, xlab = &quot;dose&quot;) #add a line of best fit for the simple linear regression model abline(beetles_simple_lm, col = &quot;blue&quot;, lwd = 2, lty = 2) #add a line for the predicted values from the GLM lines(beetles_newdata2$dose, predict(beetles_glm_props, newdata = beetles_newdata2, type = &quot;response&quot;), lwd = 2, col = &quot;red&quot;) abline(h = c(0,1), lty = 2) legend(&quot;topleft&quot;, c(&quot;Linear model&quot;, &quot;Logistic GLM&quot;), lty = c(2,1), lwd = 2, col = c(&quot;blue&quot;, &quot;red&quot;)) It is clear to see from this plot that the GLM fits much better than the linear model, given that for the linear model, at a dosage of 1.6, the proportion of beetles killed is actually negative, and that for a dosage of 1.9, the proportion of beetles killed is greater than 1, neither are logistically possible proportions. However, for the GLM, the proportion of beetles killed is always between the values of 0 and 1. For example, for the GLM, at a dosage of 1.6, instead of having a negative proportion, the proportion is just very close to 0 meaning that it is unlikely for any beetles to be killed, with the opposite occurring at a dosage of 1.9 where the proportion is very close to 1. Beyond the linear model not being realistically feasible for certain values of dosage, the line of best fit also does not fit the trend of the observed points as well as the GLM. 4.6 Model selection 4.6.1 Accuracy and precision One of the main goals when modelling is to produce the best fitting models with the least amount of error. One type of error is observational error, made up of accuracy and precision, and can be used to measure results. Accuracy can be defined as the distance between the observed/estimated results and the true values, and precision can be defined as the spread of the observed/estimated results. In an ideal situation, you would want the observations to be close together and close to the true values. A visual representation of accuracy and precision can be seen in the figure below, where the graphs depict the 4 different combinations of the observational error types. (#fig:image accuracy and precision)Visual examples of accuracy and precision There are multiple ways of testing the accuracy and precision of a model, for example the Akaike information criterion which assesses the goodness-of-fit of a model, which could be described as assessing the comparative accuracy when paired with at least one other model. This method and others are be discussed in this module. 4.6.2 Akaike information criterion The Akaike information criterion (AIC) estimates the proportional quality of one model compared to another through assessing the quality predictions. It uses the bias-variance trade-off to identify which model is preferred for the given data, taking into consideration the accuracy of the model’s predictions via the log-likelihood and the complexity of the model via a penalty term for the number of parameters within the model. Formally, the AIC is given as \\[\\text{AIC}=-2\\ell + 2p,\\] where \\(\\ell\\) is the log-likelihood of a model and \\(p\\) is the number of parameters in the model. Given that the AIC statistic is essentially a measure of both bias and variance, when comparing two models fitted to the same data, the model with the smallest AIC value is the better fitting model for the data. To find the AIC statistic in R for a given model, the value can found using the function AIC() with the desired model included as an argument, or if fitting a GLM, the function summary() provides the AIC statistic in the output. (#fig:image AIC)Reading the AIC statistic from a GLM sumamry To demonstrate the AIC() function, the AIC statistics can be found for both the linear and quadratic regression models for the birth weight data as follows. #AIC value of linear regression model AIC(birth_simple_lm) ## [1] 324.53 #AIC value of quadratic regression model AIC(birth_multi_lm1) ## [1] 321.3909 The AIC statistic for the more complex model is smaller, supporting previously made conclusions that the multiple regression model is a better fitting regression model for the birth weight data compared to the simple linear regression model. 4.6.3 Bayesian information criterion The Bayesian information criterion (BIC), is an alternative information to the AIC,and also uses the bias-variance trade-off. Unlike the AIC however, it uses the number of observations in the dataset in the computation of the penalty term. As a result of this difference, the BIC has a larger penalty term than the AIC and penalises complex models more than the AIC. The formula for the BIC is given below. \\[\\text{BIC}=-2\\ell + p \\times \\log(n),\\] where \\(\\ell\\) is the log-likelihood of a model, \\(p\\) is the number of parameters in the model and \\(n\\) is the number of observations in the dataset. To compute the BIC statistic in R for a given model, the BIC() function can be used in much the same way as for computing the AIC statistic. Demonstrated below with the birth weight data. #BIC value of linear regression model BIC(birth_simple_lm) ## [1] 328.0642 #BIC value of quadratic regression model BIC(birth_multi_lm1) ## [1] 326.1031 These results agree with previous conclusions, that the more complex model, the multuple regression model, fits the data best given that the BIC value for this model is smaller. Given the differences in the AIC and BIC, for a smaller dataset, the AIC might be more appropriate since it doesn’t penalise the more complex models as harshly. However, if the dataset is large, then the BIC may be more appropriate for preventing overfitting. 4.6.4 R-squared statistic The \\(R^2\\) or R-squared statistic, also called the coefficient of determination, is a measure of goodness-of-fit of a given regression model through measuring the proportion of variance from the dependent variable that is explained by the independent variable(s). There are two main components of the R-squared statistic, the sum of squares of the residuals and total sum of squares, both of which measure variation in the data, where squared values are used to account for fitted values being both above and below the true values. To compute these measures of variation, let \\(\\boldsymbol{y}=y_1,...,y_n\\) be a dataset with corresponding fitted values \\(\\boldsymbol{\\hat{y}}=\\hat{y}_1,...,\\hat{y}_n,\\). The residuals can be described as the estimates of unobservable error, or the difference between the observed and fitted values, then given as \\(r_i= y_i - \\hat{y}_i\\). The sum of squares of the residuals (the sum of the squared distance between the observed values and the fitted values) is computed through summing the squared values of the residuals as follows: \\[RSS = \\sum_{i=1}^n (y_i-\\hat{y}_i)^2 = \\sum_{i=1}^n r_i^2.\\] The total sum of squares (the sum of the squared distance between the observed values and the overall mean) follows the same structure as follows: \\[TSS = \\sum_{i=1}^n (y_i - \\bar{y})^2,\\] where \\(\\bar{y}= \\sum_{i=1}^n y_i\\) is the overall mean of the observed values. The R-squared statistic is then computed using the following formula: \\[ R^2 = 1-\\frac{RSS}{TSS},\\] and takes a value between \\(0\\) and \\(1\\). Given that the better fitting a model is, the smaller the difference between the observed and fitted values is and hence the smaller the RSS value is, a better fitting model will have a larger R-squared value, with the perfectly fitting model having an RSS value of 0 and an R-squared value of 1. Therefore, when comparing the goodness-of-fit of two or more models, the model with the R-squared statistic value closest to 1 is the better fitting model for the given data. The values for the R-squared statistics from models in R can be found directly through using the summary() function using the code summary()$r.squared. For example, this function can be used with the linear and quadratic regression models for the welding data to find out which model fits the data better. #R-squared statistic for the linear regression model summary(weld_simple_lm)$r.squared ## [1] 0.9622726 #R-squared statistic for the quadratic regression model summary(weld_quad_lm)$r.squared ## [1] 0.9761461 Whilst both of the R-squared statistics are high, the value for the quadratic model is slightly higher, supporting previous conclusions that the quadratic model fits the data better and hence the quadratic model should be used. 4.6.5 Analysis of variance Analysis of variance (ANOVA) is a statistical test used to assess the relationship between the dependent variable and one or more independent variables. To produce an ANOVA table, the function anova() can be used in R. If only one model is given as the argument, it will indicate as to whether the terms in the given model are significant. This is demonstrated below with the simple linear model for the welding data. #produce an ANOVA table for simple model for welding data anova(weld_simple_lm) ## Analysis of Variance Table ## ## Response: Diameter ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Current 1 19.6203 19.6203 484.61 5.529e-15 *** ## Residuals 19 0.7692 0.0405 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (#fig:image ANOVA 1)Understanding the ANOVA table It can be seen from the results that the p-value is very small, and much smaller than the standard 5% significance level indicating that current does have an impact on diameter, and therefore the term for current should remain in the model. However, you can also test which model fits the data best by including multiple models as arguments. This is demonstrated in the R code below, with a comparison between the simple linear model and quadratic model for the welding dataset. #produce an ANOVA table for the simple and quadratic models for the welding data anova(weld_simple_lm,weld_quad_lm) ## Analysis of Variance Table ## ## Model 1: Diameter ~ Current ## Model 2: Diameter ~ Current + I(Current^2) ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 19 0.76924 ## 2 18 0.48637 1 0.28287 10.469 0.004589 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (#fig:image ANOVA 2)Understanding the ANOVA table The p-value given in the ANOVA table is smaller than \\(0.05\\), which is the typical significance value chosen, indicating that there is evidence that the addition of the quadratic term is significant. Therefore, there is evidence that the quadratic model better explains the relationship between the diameter and current comparatively to the simple model. 4.6.6 Likelihood ratio testing Likelihood ratio testing can be used to compare the fit of two models when the models are nested. In other words, if one model is a special case of the other where at least one parameter is removed from the model. The likelihood ratio test (LRT) helps to decide whether to reject the null hypothesis or not where the null hypothesis assumes that the nested model is at least as good as the more complex model. The likelihood ratio test (LRT) statistic can be computed using the following formula \\[ \\lambda = - 2 \\times (\\ell(\\text{model 1}) - \\ell(\\text{model 2})),\\] where model 1 is nested in model 2 and \\(\\hat{\\ell}()\\) is the log-likelihood for the model given in the brackets. To perform the likelihood ratio test, a constant, \\(c\\), is chosen to determine the significance level of the test. If the corresponding p-value to \\(\\lambda\\) is less than \\(c\\), then there is evidence to reject the null hypothesis and the complex model is preferred, otherwise, if the p-value is greater than or equal to \\(c\\), there is evidence to not reject the null hypothesis and the nested model is preferred. The LRT can be conducted manually, through first computing the value of \\(\\lambda\\) through using the function logLik() in the equation with an argument for the chosen model to compute the log-likelihood of the chosen model. #compute lambda llmod1 &lt;- logLik(birth_simple_lm) llmod2 &lt;- logLik(birth_multi_lm1) lambda &lt;- -2*(llmod1 - llmod2) The corresponding p-value can then be computed through using the function pchisq() which computes the chi-squared distribution function for the arguments included. In this case, to compute the p-value, input the value of \\(\\lambda\\), the degrees of freedom (the difference between the degrees of freedom for the nested and complex models, given by the logLik() function) and set lower.tail = FALSE as arguments. #compute the corresponding p-value pchisq(lambda[1], df = 1, lower.tail = FALSE) ## [1] 0.02339251 Given that the resulting p-value is less than \\(0.05\\), the most common significance level, there is evidence that the more complex model is preferred and that the null hypothesis should be rejected. Alternatively, the function lrtest() within the package lmtest can be used to perform a likelihood ratio test. An example of this with the birth dataset is as follows. #install lmtest package install.packages(&quot;lmtest&quot;) #load the lmtest package library(lmtest) #perform a likelihood ratio test on the simple and multiple regression models lrtest(birth_simple_lm, birth_multi_lm1) ## Likelihood ratio test ## ## Model 1: Weight ~ Age ## Model 2: Weight ~ Sex + Age ## #Df LogLik Df Chisq Pr(&gt;Chisq) ## 1 3 -159.26 ## 2 4 -156.69 1 5.1391 0.02339 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 It can be seen that the results from the function lrtest() are the same as computing the LRT manually, with the p-values being the same. Once again, there is therefore evidence that the null hypothesis should be rejected and that the more complex, multiple regression model is preferred, supporting the evidence of the AIC results. The final most common way that a LRT can be done in R is through using the anova() function again, but this time specifying test = \"LRT\" as an additional argument. #likelihood ratio test with anova anova(birth_simple_lm, birth_multi_lm1, test = &quot;LRT&quot;) ## Analysis of Variance Table ## ## Model 1: Weight ~ Age ## Model 2: Weight ~ Sex + Age ## Res.Df RSS Df Sum of Sq Pr(&gt;Chi) ## 1 22 816074 ## 2 21 658771 1 157304 0.02514 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 This method is conducted slightly differently, hence the slight variation in the p-value, however, the test is also valid and also indicates that the more complex model is preferred. 4.7 Stepwise regression It is always important to find out which model fits the data best. When the data only has one or two covariates, using the methods discussed so far this module, it can be a simple process of fitting each of the models and using the evaluation methods to select the best fitting model. However, once the data has more than a couple covariates available, this process becomes more lengthy. This is when stepwise regression comes in useful, although it does not guarantee to select the best model. This regression is a step-by-step iterative regression that looks at how the fit of a model changes when a variable is added or removed (depending on which direction you go in), testing the significance of the variable, in an automated process to select the best model with the data available. There are three approaches that can be taken with stepwise regression, the first being the way that people typically take manually and that is forward stepwise regression. For the forward approach, the process starts with the intercept-only model, adding one term at a time, testing its significance and keeping that term in the model if it is significant. The second approach is backward stepwise regression. For the backward approach, the same idea is used but the process starts with the saturated (full) model, removing terms one at a time and testing whether that term was significant through its impact on the model and the model’s fit. Finally is the bidirectional, or both-ways stepwise regression, which is a combination of both forward and backward regression to test which terms should be included or excluded. This is done by starting with the intercept-only model and adding sequentially adding terms that are deemed statistically significant, and after each new term is added, any terms which are no longer statistically significant are removed. There are multiple ways of conducting stepwise regression in R, however, the most common approach is to use either the function stepAIC() from the MASS package or the function step() from the stats package, both functions used in the same way (although step() is a simplified version of stepAIC()). These methods of stepwise regression use the AIC by default to choose the best fitting model, with a model (either a lm() or glm() object) inputted as the object argument and the direction used chosen by adding the argument direction = and inputting one of \"both\", \"backward\" or \"forward\" as the direction of choice. #install MASS package install.packages(&quot;MASS&quot;) #load the MASS package library(MASS) To fit the full or saturated model in R, instead of needing to type out each of the terms manually, after the tilde (~), you can put a full stop (.) in place of the terms which will add main effects for each covariate available in the data. The mtcars dataset from the datasets package used introduced in Module 2 will be used to demonstrate the different approaches to stepwise regression given that there are many covariates available in the data. To find out more about the dataset itself, search for the mtcars help file with ?mtcars. #&#39;mtcars&#39; is a data set available in the &#39;datasets&#39; package with data on #11 different aspects of auto mobiles for 32 auto mobiles from the 1974 Motor #Trend US magazine library(datasets) #information on the dataset in the &#39;Help&#39; pane ?mtcars #load data and assign to &#39;cars_data&#39; cars_data &lt;- mtcars 4.7.1 Forward stepwise regression To perform forward stepwise regression, you need to start from the intercept-only model as terms are added sequentially. Therefore, the first step is to fit the intercept-only model to the data. #fit the saturated model cars_initial &lt;- lm(mpg ~ 1, data = cars_data) summary(cars_initial) ## ## Call: ## lm(formula = mpg ~ 1, data = cars_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.6906 -4.6656 -0.8906 2.7094 13.8094 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 20.091 1.065 18.86 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.027 on 31 degrees of freedom Then, to use the stepAIC() function, the direction argument needs to be specified as direction = \"forward\" and the range of models (lower and upper) to be assessed specified in the scope argument. If no scope argument is specified, the initial model is used as upper model, so to explore more than just the intercept-only model with forward stepwise regression, the initial model should be included as the lower scope and the saturated model as the upper scope. Therefore, the saturated model should also be fitted prior to performing the stepwise regression #fit the saturated model cars_saturated &lt;- lm(mpg ~ ., data = cars_data) summary(cars_saturated) ## ## Call: ## lm(formula = mpg ~ ., data = cars_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.4506 -1.6044 -0.1196 1.2193 4.6271 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 12.30337 18.71788 0.657 0.5181 ## cyl -0.11144 1.04502 -0.107 0.9161 ## disp 0.01334 0.01786 0.747 0.4635 ## hp -0.02148 0.02177 -0.987 0.3350 ## drat 0.78711 1.63537 0.481 0.6353 ## wt -3.71530 1.89441 -1.961 0.0633 . ## qsec 0.82104 0.73084 1.123 0.2739 ## vs 0.31776 2.10451 0.151 0.8814 ## am 2.52023 2.05665 1.225 0.2340 ## gear 0.65541 1.49326 0.439 0.6652 ## carb -0.19942 0.82875 -0.241 0.8122 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.65 on 21 degrees of freedom ## Multiple R-squared: 0.869, Adjusted R-squared: 0.8066 ## F-statistic: 13.93 on 10 and 21 DF, p-value: 3.793e-07 #fit the forwards stepwise regression cars_forward &lt;- stepAIC(cars_initial, direction = &quot;forward&quot;, scope = list(lower = cars_initial, upper = cars_saturated)) ## Start: AIC=115.94 ## mpg ~ 1 ## ## Df Sum of Sq RSS AIC ## + wt 1 847.73 278.32 73.217 ## + cyl 1 817.71 308.33 76.494 ## + disp 1 808.89 317.16 77.397 ## + hp 1 678.37 447.67 88.427 ## + drat 1 522.48 603.57 97.988 ## + vs 1 496.53 629.52 99.335 ## + am 1 405.15 720.90 103.672 ## + carb 1 341.78 784.27 106.369 ## + gear 1 259.75 866.30 109.552 ## + qsec 1 197.39 928.66 111.776 ## &lt;none&gt; 1126.05 115.943 ## ## Step: AIC=73.22 ## mpg ~ wt ## ## Df Sum of Sq RSS AIC ## + cyl 1 87.150 191.17 63.198 ## + hp 1 83.274 195.05 63.840 ## + qsec 1 82.858 195.46 63.908 ## + vs 1 54.228 224.09 68.283 ## + carb 1 44.602 233.72 69.628 ## + disp 1 31.639 246.68 71.356 ## &lt;none&gt; 278.32 73.217 ## + drat 1 9.081 269.24 74.156 ## + gear 1 1.137 277.19 75.086 ## + am 1 0.002 278.32 75.217 ## ## Step: AIC=63.2 ## mpg ~ wt + cyl ## ## Df Sum of Sq RSS AIC ## + hp 1 14.5514 176.62 62.665 ## + carb 1 13.7724 177.40 62.805 ## &lt;none&gt; 191.17 63.198 ## + qsec 1 10.5674 180.60 63.378 ## + gear 1 3.0281 188.14 64.687 ## + disp 1 2.6796 188.49 64.746 ## + vs 1 0.7059 190.47 65.080 ## + am 1 0.1249 191.05 65.177 ## + drat 1 0.0010 191.17 65.198 ## ## Step: AIC=62.66 ## mpg ~ wt + cyl + hp ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 176.62 62.665 ## + am 1 6.6228 170.00 63.442 ## + disp 1 6.1762 170.44 63.526 ## + carb 1 2.5187 174.10 64.205 ## + drat 1 2.2453 174.38 64.255 ## + qsec 1 1.4010 175.22 64.410 ## + gear 1 0.8558 175.76 64.509 ## + vs 1 0.0599 176.56 64.654 The output from the stepwise regression demonstrates the process clearly, starting with no covariates in the model and adding each of the covariates to the intercept-only model one at a time, selecting the covariate which results in the lowest AIC value, in this case wt. The process is then repeated, adding each of the remaining covariates one at a time to the intercept and main effect for weight model, selecting the model which results in the lowest AIC value, in this case cyl. Repeating this process until adding any more covariates no longer improves the fit of the model results in the final model. It can be seen that the resulting model from the forward stepwise regression includes the covariates wt, cyl and hp as terms in the linear model, meaning that the weight, number of cylinders and the horsepower of the car all improve the fit of the model according to the AIC when exploring the relationship between the covariates and the response, miles per gallon. #summary of the forward stepwise regression model summary(cars_forward) ## ## Call: ## lm(formula = mpg ~ wt + cyl + hp, data = cars_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.9290 -1.5598 -0.5311 1.1850 5.8986 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 38.75179 1.78686 21.687 &lt; 2e-16 *** ## wt -3.16697 0.74058 -4.276 0.000199 *** ## cyl -0.94162 0.55092 -1.709 0.098480 . ## hp -0.01804 0.01188 -1.519 0.140015 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.512 on 28 degrees of freedom ## Multiple R-squared: 0.8431, Adjusted R-squared: 0.8263 ## F-statistic: 50.17 on 3 and 28 DF, p-value: 2.184e-11 The output from the summary of the forward stepwise regression model indicates that whilst this model produced the lowest AIC value, not all of the terms are statistically significant using the p-values at the 95% significance level. It is important to explore this when using stepwise regression as you do not want to include covariates unnecessarily in your model. 4.7.2 Backward stepwise regression To perform backward stepwise regression, use the stepAIC() function, adding the argument direction = \"backward\". Backward stepwise regression starts from the saturated model so this model should be inputted as the object, and since the saturated is the upper model, there is no need to add an argument for scope. #fit the backward stepwise regression cars_backward &lt;- stepAIC(cars_saturated, direction = &quot;backward&quot;) ## Start: AIC=70.9 ## mpg ~ cyl + disp + hp + drat + wt + qsec + vs + am + gear + carb ## ## Df Sum of Sq RSS AIC ## - cyl 1 0.0799 147.57 68.915 ## - vs 1 0.1601 147.66 68.932 ## - carb 1 0.4067 147.90 68.986 ## - gear 1 1.3531 148.85 69.190 ## - drat 1 1.6270 149.12 69.249 ## - disp 1 3.9167 151.41 69.736 ## - hp 1 6.8399 154.33 70.348 ## - qsec 1 8.8641 156.36 70.765 ## &lt;none&gt; 147.49 70.898 ## - am 1 10.5467 158.04 71.108 ## - wt 1 27.0144 174.51 74.280 ## ## Step: AIC=68.92 ## mpg ~ disp + hp + drat + wt + qsec + vs + am + gear + carb ## ## Df Sum of Sq RSS AIC ## - vs 1 0.2685 147.84 66.973 ## - carb 1 0.5201 148.09 67.028 ## - gear 1 1.8211 149.40 67.308 ## - drat 1 1.9826 149.56 67.342 ## - disp 1 3.9009 151.47 67.750 ## - hp 1 7.3632 154.94 68.473 ## &lt;none&gt; 147.57 68.915 ## - qsec 1 10.0933 157.67 69.032 ## - am 1 11.8359 159.41 69.384 ## - wt 1 27.0280 174.60 72.297 ## ## Step: AIC=66.97 ## mpg ~ disp + hp + drat + wt + qsec + am + gear + carb ## ## Df Sum of Sq RSS AIC ## - carb 1 0.6855 148.53 65.121 ## - gear 1 2.1437 149.99 65.434 ## - drat 1 2.2139 150.06 65.449 ## - disp 1 3.6467 151.49 65.753 ## - hp 1 7.1060 154.95 66.475 ## &lt;none&gt; 147.84 66.973 ## - am 1 11.5694 159.41 67.384 ## - qsec 1 15.6830 163.53 68.200 ## - wt 1 27.3799 175.22 70.410 ## ## Step: AIC=65.12 ## mpg ~ disp + hp + drat + wt + qsec + am + gear ## ## Df Sum of Sq RSS AIC ## - gear 1 1.565 150.09 63.457 ## - drat 1 1.932 150.46 63.535 ## &lt;none&gt; 148.53 65.121 ## - disp 1 10.110 158.64 65.229 ## - am 1 12.323 160.85 65.672 ## - hp 1 14.826 163.35 66.166 ## - qsec 1 26.408 174.94 68.358 ## - wt 1 69.127 217.66 75.350 ## ## Step: AIC=63.46 ## mpg ~ disp + hp + drat + wt + qsec + am ## ## Df Sum of Sq RSS AIC ## - drat 1 3.345 153.44 62.162 ## - disp 1 8.545 158.64 63.229 ## &lt;none&gt; 150.09 63.457 ## - hp 1 13.285 163.38 64.171 ## - am 1 20.036 170.13 65.466 ## - qsec 1 25.574 175.67 66.491 ## - wt 1 67.572 217.66 73.351 ## ## Step: AIC=62.16 ## mpg ~ disp + hp + wt + qsec + am ## ## Df Sum of Sq RSS AIC ## - disp 1 6.629 160.07 61.515 ## &lt;none&gt; 153.44 62.162 ## - hp 1 12.572 166.01 62.682 ## - qsec 1 26.470 179.91 65.255 ## - am 1 32.198 185.63 66.258 ## - wt 1 69.043 222.48 72.051 ## ## Step: AIC=61.52 ## mpg ~ hp + wt + qsec + am ## ## Df Sum of Sq RSS AIC ## - hp 1 9.219 169.29 61.307 ## &lt;none&gt; 160.07 61.515 ## - qsec 1 20.225 180.29 63.323 ## - am 1 25.993 186.06 64.331 ## - wt 1 78.494 238.56 72.284 ## ## Step: AIC=61.31 ## mpg ~ wt + qsec + am ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 169.29 61.307 ## - am 1 26.178 195.46 63.908 ## - qsec 1 109.034 278.32 75.217 ## - wt 1 183.347 352.63 82.790 The output from the backward stepwise regression also demonstrates the process, starting with the saturated model and removing one covariate at a time, selecting the model which results in the lowest AIC value, repeating this process until removing another covariate from the model no longer decreases the AIC value. The resulting model from backward stepwise regression includes the covariates wt, qsec and am, meaning that weight, 1/4 mile time and transmission all improve the fit of the model according to the AIC value. This is a different model produced than when using forward stepwise regression, which is also why it is important to still look at the summary output for the model to assess whether the terms themselves are statistically significant and hence whether they have a notable impact on the response. #summary of backward stepwise model summary(cars_backward) ## ## Call: ## lm(formula = mpg ~ wt + qsec + am, data = cars_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.4811 -1.5555 -0.7257 1.4110 4.6610 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.6178 6.9596 1.382 0.177915 ## wt -3.9165 0.7112 -5.507 6.95e-06 *** ## qsec 1.2259 0.2887 4.247 0.000216 *** ## am 2.9358 1.4109 2.081 0.046716 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.459 on 28 degrees of freedom ## Multiple R-squared: 0.8497, Adjusted R-squared: 0.8336 ## F-statistic: 52.75 on 3 and 28 DF, p-value: 1.21e-11 In this case, the summary output indicates that each of the terms included in the backward stepwise regression model are all statistically significant at the 95% level and hence should be included in the model as they impact the response variable. 4.7.3 Both ways stepwise regression To perform both ways stepwise regression, use the stepAIC() function, adding the argument direction = \"both\". #fit the both ways stepwise regression cars_both &lt;- stepAIC(cars_saturated, direction = &quot;both&quot;) ## Start: AIC=70.9 ## mpg ~ cyl + disp + hp + drat + wt + qsec + vs + am + gear + carb ## ## Df Sum of Sq RSS AIC ## - cyl 1 0.0799 147.57 68.915 ## - vs 1 0.1601 147.66 68.932 ## - carb 1 0.4067 147.90 68.986 ## - gear 1 1.3531 148.85 69.190 ## - drat 1 1.6270 149.12 69.249 ## - disp 1 3.9167 151.41 69.736 ## - hp 1 6.8399 154.33 70.348 ## - qsec 1 8.8641 156.36 70.765 ## &lt;none&gt; 147.49 70.898 ## - am 1 10.5467 158.04 71.108 ## - wt 1 27.0144 174.51 74.280 ## ## Step: AIC=68.92 ## mpg ~ disp + hp + drat + wt + qsec + vs + am + gear + carb ## ## Df Sum of Sq RSS AIC ## - vs 1 0.2685 147.84 66.973 ## - carb 1 0.5201 148.09 67.028 ## - gear 1 1.8211 149.40 67.308 ## - drat 1 1.9826 149.56 67.342 ## - disp 1 3.9009 151.47 67.750 ## - hp 1 7.3632 154.94 68.473 ## &lt;none&gt; 147.57 68.915 ## - qsec 1 10.0933 157.67 69.032 ## - am 1 11.8359 159.41 69.384 ## + cyl 1 0.0799 147.49 70.898 ## - wt 1 27.0280 174.60 72.297 ## ## Step: AIC=66.97 ## mpg ~ disp + hp + drat + wt + qsec + am + gear + carb ## ## Df Sum of Sq RSS AIC ## - carb 1 0.6855 148.53 65.121 ## - gear 1 2.1437 149.99 65.434 ## - drat 1 2.2139 150.06 65.449 ## - disp 1 3.6467 151.49 65.753 ## - hp 1 7.1060 154.95 66.475 ## &lt;none&gt; 147.84 66.973 ## - am 1 11.5694 159.41 67.384 ## - qsec 1 15.6830 163.53 68.200 ## + vs 1 0.2685 147.57 68.915 ## + cyl 1 0.1883 147.66 68.932 ## - wt 1 27.3799 175.22 70.410 ## ## Step: AIC=65.12 ## mpg ~ disp + hp + drat + wt + qsec + am + gear ## ## Df Sum of Sq RSS AIC ## - gear 1 1.565 150.09 63.457 ## - drat 1 1.932 150.46 63.535 ## &lt;none&gt; 148.53 65.121 ## - disp 1 10.110 158.64 65.229 ## - am 1 12.323 160.85 65.672 ## - hp 1 14.826 163.35 66.166 ## + carb 1 0.685 147.84 66.973 ## + vs 1 0.434 148.09 67.028 ## + cyl 1 0.414 148.11 67.032 ## - qsec 1 26.408 174.94 68.358 ## - wt 1 69.127 217.66 75.350 ## ## Step: AIC=63.46 ## mpg ~ disp + hp + drat + wt + qsec + am ## ## Df Sum of Sq RSS AIC ## - drat 1 3.345 153.44 62.162 ## - disp 1 8.545 158.64 63.229 ## &lt;none&gt; 150.09 63.457 ## - hp 1 13.285 163.38 64.171 ## + gear 1 1.565 148.53 65.121 ## + cyl 1 1.003 149.09 65.242 ## + vs 1 0.645 149.45 65.319 ## + carb 1 0.107 149.99 65.434 ## - am 1 20.036 170.13 65.466 ## - qsec 1 25.574 175.67 66.491 ## - wt 1 67.572 217.66 73.351 ## ## Step: AIC=62.16 ## mpg ~ disp + hp + wt + qsec + am ## ## Df Sum of Sq RSS AIC ## - disp 1 6.629 160.07 61.515 ## &lt;none&gt; 153.44 62.162 ## - hp 1 12.572 166.01 62.682 ## + drat 1 3.345 150.09 63.457 ## + gear 1 2.977 150.46 63.535 ## + cyl 1 2.447 150.99 63.648 ## + vs 1 1.121 152.32 63.927 ## + carb 1 0.011 153.43 64.160 ## - qsec 1 26.470 179.91 65.255 ## - am 1 32.198 185.63 66.258 ## - wt 1 69.043 222.48 72.051 ## ## Step: AIC=61.52 ## mpg ~ hp + wt + qsec + am ## ## Df Sum of Sq RSS AIC ## - hp 1 9.219 169.29 61.307 ## &lt;none&gt; 160.07 61.515 ## + disp 1 6.629 153.44 62.162 ## + carb 1 3.227 156.84 62.864 ## + drat 1 1.428 158.64 63.229 ## - qsec 1 20.225 180.29 63.323 ## + cyl 1 0.249 159.82 63.465 ## + vs 1 0.249 159.82 63.466 ## + gear 1 0.171 159.90 63.481 ## - am 1 25.993 186.06 64.331 ## - wt 1 78.494 238.56 72.284 ## ## Step: AIC=61.31 ## mpg ~ wt + qsec + am ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 169.29 61.307 ## + hp 1 9.219 160.07 61.515 ## + carb 1 8.036 161.25 61.751 ## + disp 1 3.276 166.01 62.682 ## + cyl 1 1.501 167.78 63.022 ## + drat 1 1.400 167.89 63.042 ## + gear 1 0.123 169.16 63.284 ## + vs 1 0.000 169.29 63.307 ## - am 1 26.178 195.46 63.908 ## - qsec 1 109.034 278.32 75.217 ## - wt 1 183.347 352.63 82.790 The results from the output of the both ways stepwise regression are the same as that from the backward stepwise regression in this instance, indicating that the model with the covariates wt, qsec and am results in the lowest AIC value. #summary of both ways stepwise model summary(cars_both) ## ## Call: ## lm(formula = mpg ~ wt + qsec + am, data = cars_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.4811 -1.5555 -0.7257 1.4110 4.6610 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.6178 6.9596 1.382 0.177915 ## wt -3.9165 0.7112 -5.507 6.95e-06 *** ## qsec 1.2259 0.2887 4.247 0.000216 *** ## am 2.9358 1.4109 2.081 0.046716 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.459 on 28 degrees of freedom ## Multiple R-squared: 0.8497, Adjusted R-squared: 0.8336 ## F-statistic: 52.75 on 3 and 28 DF, p-value: 1.21e-11 Whilst the results are the same as previously seen, it is still important to check the significance of the covariates in the final model to ensure that you are not including covariates which are not significant at the 95% (or otherwise chosen) significance level as this would indicate that the covariate does not have a significant impact on the response variable. In this case, the summary output indicates that each of the terms are statistically significant and therefore should not be removed from the model. 4.8 Cross-validation As mentioned previously, when choosing a model, it is important to avoid overfitting. This occurs when the model of interest is fitted too well on the given data and does not perform well on any other, unseen data. Cross-validation (CV) is a resampling method that is used to assess the performance of the model of interest on unseen data. The general idea using the validation approach is to divide the available dataset into 2 sets, the training set and the validation set. The former is used to fit the model, essentially training the model using the training dataset, where the resulting fitted values are used to predict responses for the observations that are in the validation dataset. These predictions are then used to test the generalised fit of the model of interest. This results in a much more generalised assessment of the model’s performance which gives a better idea of how it will perform with a new unseen dataset, instead of just the dataset used to train the model. There are multiple ways to perform cross-validation, in this section, k-fold cross-validation and leave-one-out cross-validation (LOOCV) will be discussed. 4.8.1 k-fold cross-validation One of the more widely-used approaches to cross-validation is that of the k-fold approach. To perform k-fold CV, the available dataset is partitioned into \\(K\\) approximately equal-sized, non-overlapping (sub)sets, known as folds. Commonly, the choice of \\(K\\) is either 5 or 10. Once the data is partitioned, for each \\(k=1,\\cdots K\\), fold \\(k\\) is removed from the overall data and the model is fitted using the training set which is made up of the remaining \\(K-1\\) folds Once the model has been fitted, the predictions can be found using the validation set which is the \\(k\\)th fold that was originally removed. After this process is repeated for each of the \\(K\\) folds, the results are combined to find the cross-validation error where the lower the value of the CV error the better the generalised fit of the model. The process is demonstrated visually in the below figure, where the complete dataset is partitioned into \\(K=5\\) folds. (#fig:image k-fold)Visual example of k-fold cross-validation To demonstrate the method in R, the Auto dataset from the ISLR package will be used. This dataset contains information on 392 vehicles, where for this example, the covariates for miles per gallon (mpg) and horsepower (horsepower) are of interest. #install ISLR package install.packages(&quot;ISLR&quot;) #load the ISLR package library(ISLR) To compute the k-fold cross-validation error, the function cv.glm() from the boot package can be used, with arguments for the dataset, the model being tested and the number of folds. From the results, the delta[1] coefficient can be subset in order to extract the CV error from the results. Since the partitioning into the folds is a random process, to obtain reproducible results, the seed needs to be set with the function set.seed(). #install boot package install.packages(&quot;boot&quot;) #load the boot package library(boot) In this data, the response (mpg) is assumed to follow a normal distribution and hence a GLM can be fitted to explore the relationship between the miles per gallon and horsepower with the default value for the family argument. Additionally, the number of folds for this example is chosen to be \\(K=10\\). #load the Auto dataset data(Auto) #set the seed set.seed(100) #fit a linear model Auto_simple_glm &lt;- glm(mpg ~ horsepower, data = Auto) #find the k-fold CV error for this model fit kfold_CV_error1 &lt;- cv.glm(Auto, Auto_simple_glm, K = 10)$delta[1] kfold_CV_error1 ## [1] 24.13555 However, since there is no reference point for the value of CV error besides the smaller the error the better, it is important to fit alternative models to select which model has the best generalised fit. Below is code which fits a series of models with the polynomial() function including polynomials for the horsepower argument ranging from degree 1 to degree 5 #set the seed set.seed(100) #create the empty variable kfold_CV_error2 &lt;- c() for (i in 1:5){ Auto_poly_glm &lt;- glm(mpg~poly(horsepower,5), data = Auto) kfold_CV_error2[i] &lt;- cv.glm(Auto, Auto_poly_glm, K = 10)$delta[1] } kfold_CV_error2 ## [1] 18.93963 18.92569 19.06507 19.40166 18.99473 #which model has the lowest error which.min(kfold_CV_error2) ## [1] 2 #plot the results for a visual illustration plot(kfold_CV_error2, xlab = &quot;Polynomial degree&quot;, ylab = &quot;k-fold CV error&quot;) The results from the k-fold cross-validation indicate that the quadratic model has the lowest CV error and is therefore the model that should be used. 4.8.2 Leave-one-out cross-validation Leave-one-out cross-validation can be described as a special case of k-fold cross-validation in the instance where the number of folds chosen is equal to the number of observations in the available dataset, \\(K=n\\). This leads to each subset, or fold, containing a single observation only. This means that instead of removing a set of observations from the dataset each time the model is fitted, a singular observation is removed and the remaining data is used for fitting the model. Given the nature of this approach, it is not appropriate when datasets are very large due to the high computational cost associated to fitting the same number of models as observations in the data. This process is demonstrated visually in the figure below. (#fig:image LOOCV)Visual example of leave-one-out cross-validation To perform LOOCV in R, the process is very similar to that of k-fold CV fitting a GLM and then using the function cv.glm() to perform the cross-validation. However, instead of defining the argument K, this is left blank as the default value is set equal to \\(n\\), the number of observations in the dataset. This is demonstrated below with the Auto dataset again. #set the seed set.seed(100) #fit a linear model Auto_simple_glm &lt;- glm(mpg ~ horsepower, data = Auto) #find the k-fold CV error for this model fit LOOCV_error1 &lt;- cv.glm(Auto, Auto_simple_glm)$delta[1] LOOCV_error1 ## [1] 24.23151 As with the k-fold cross-validation approach, the value of the CV error of one model on its own does not indicate as to which model is best. The same example as given for the k-fold approach is provided below, where the LOOCV error is given for multiple polynomial models ranging from degree 1 to degree 5. #set the seed set.seed(100) #create the empty variable LOOCV_error2 &lt;- c() for (i in 1:5){ Auto_poly_glm &lt;- glm(mpg ~ poly(horsepower, 5), data = Auto) LOOCV_error2[i] &lt;- cv.glm(Auto, Auto_poly_glm)$delta[1] } LOOCV_error2 ## [1] 19.03321 19.03321 19.03321 19.03321 19.03321 #which model has the lowest error which.min(LOOCV_error2) ## [1] 5 #plot the results for a visual illustration plot(LOOCV_error2, xlab = &quot;Polynomial degree&quot;, ylab = &quot;LOOCV error&quot;) The results for this approach indicate that the simple linear regression model is best for this data, however, the plot demonstrates that there is much less of a difference in the error between each of the different polynomial GLMs compared the corresponding k-fold errors. 4.9 Hierarchical regression The data modelled so far this module has all been of the same level, and the assumption of independence between observations is upheld. However, when data is multi-level/hierarchical, clustered or longitudinal, this assumption is not upheld and alternative modelling methods are required. This is where linear mixed modelling (incorporating both fixed and random effects) and hierarchical regression are used, enabling for the relationship between different variables at different levels to be explored. 4.9.1 Data structure A hierarchical data structure is where level 1 is nested in level 2, which is nested in level 3 and so on, where information can be shared between groups (also called clusters). When this information is shared, information on both between-group and within-group variability is provided, where typically the variation between groups is due to factors which are not measured but differ from one group to another. This hierarchical data structure is seen for example, when there is data on people within settlements within regions (3 levels), households within communities (2 levels) or pupils within schools within districts (3 levels). The former example is given in the below figure as a visual example of what a hierarchical data structure looks like. (#fig:image hierarchical school structure)Visual example of a hierarchical data structure It is important to account for the different levels in hierarchical data in order to make reliable and correct inferences. For example, if the vaccine status of individual people within a region is of interest where the people are grouped by settlements, each settlement forms a cluster. It is reasonable to assume that the vaccine status within a settlement (or cluster) is not independent, but is actually correlated. This can be due to a number of factors, for instance it is reasonable to assume that people within the same settlement will have similar (physical) access to the medical centre and therefore vaccines due to having approximately the same distance to a medical centre. Another factor could be that people within the same settlement can be more likely to hold similar beliefs to one another compared to those from another settlement. Another example of this importance can be seen in the school example where pupils are nested within schools within districts. If the response is the pupil’s performance in school, this will not only depend on the individual pupil’s own characteristics and intelligence, but also the school they are in and the district that the school is in. For example, private schools and grammar schools typically have better performance than standard state schools, and schools in wealthy districts typically also have better performance than schools in districts with overall lower socio-economic status. This example of pupils being nested within schools is seen in the exam dataset where there is information on different test scores, including GCSE scores (GCSEscore), London Reading test scores (LRTscore), and Verbal Reasoning test score band (VRTsband, levels for top 25%, middle 50% and bottom 25%) for pupils in different schools (SchoolID) in addition to the sex (Sex, levels for male and female) of each pupil and the type of school (sch.type, levels for mixed, male-only and female-only schools). #import the exam dataset exam &lt;- read.table(paste0(data_path,&#39;exam.txt&#39;), sep = &quot; &quot;, header = FALSE, colClasses = c(&quot;factor&quot;,&quot;double&quot;,&quot;double&quot;,&quot;factor&quot;, &quot;factor&quot;,&quot;factor&quot;)) #set column names colnames(exam) &lt;- c(&quot;SchoolID&quot;,&quot;GCSEscore&quot;,&quot;LRTscore&quot;,&quot;Sex&quot;,&quot;VRTband&quot;,&quot;sch.type&quot;) #set levels of factor variables levels(exam$Sex) &lt;- c(&quot;Male&quot;,&quot;Female&quot;) levels(exam$VRTband) &lt;- c(&quot;Top25&quot;,&quot;Middle50&quot;,&quot;Bottom25&quot;) levels(exam$sch.type) &lt;- c(&quot;Mixed&quot;,&quot;Males-only&quot;,&quot;Females-only&quot;) To perform hierarchical modelling in R, functions from the lme4, nlme or glme packages can be used. They both perform similar tasks but do have some differences, detailed below. lme4 lmer(): fits linear mixed-effects models, similar to the lm() function but allows for random effects glmer(): fits generalised linear mixed-effects models, similar to the glm() function but allows for random effects nlmer(): fits non-linear mixed-effects models nlme lme(): fits linear mixed-effects models, similar to the lm() function but allows for random effects nlme(): fits non-linear mixed-effects models -glme -glme(): fits generalised linear mixed-effects models, similar to the glm() function but allows for random effects To begin with, the lme function will be used and has the general structure of lme(y~x, random = ~) where y~x relates to the fixed part of the model and random=~ relates to the random part of the module. The default method of fitting the model is REML, which is a restricted maximum likelihood approach used to have unbiased variance components, however, this can be changed to use the maximum likelihood approach through adding the argument method = 'ML'. #install nlme package install.packages(&quot;nlme&quot;) #load the nlme package library(nlme) 4.9.2 Variance partition coefficient One method of identifying how important group level differences are is looking at the variance partition coefficient (VPC), which identifies how much of the total variance is due to the level 2 variance, being the difference between groups. For the random intercepts modelling, the VPC is computed using the following formula. \\[\\text{VPC} = \\frac{\\text{Level 2 variance}}{\\text{Total variance}}\\] Given that the VPC is a proportion, it must be between 0 and 1, \\(0 \\leq \\text{VPC} \\geq 1\\), where \\[ \\text{VPC} = \\begin{cases} 0 &amp; \\text{ if there is no group effect } (\\sigma^2_u=0), \\\\ 1 &amp; \\text{ if there is no within group differences } (\\sigma^2_e=0) \\end{cases} \\] (#fig:VPC example)Visual example of VPC equalling 0 and 1 respectively 4.9.3 Random intercepts modelling Random intercepts modelling takes into account that observations come from different groups through using a random group level effect, where the general notation for the form of the (2-level) model is as follows. \\[ y_{ij} = u_j +e_{ij}, \\] where \\(i\\) is the index for level 1, \\(j\\) is the index for level 2 and \\(u_j\\): group level effect (random effect) Normally distributed: \\(u_j \\sim N(0, \\sigma^2_u)\\) where \\(\\sigma^2_u\\) is the between-groups variance Independence: \\(\\text{cov}(u_j, u_{k})=0\\) \\(e_{ij}\\): individual level error (random error) Normally distributed: \\(e_{ij} \\sim N(0, \\sigma^2_e)\\) where \\(\\sigma^2_e\\) is the within-groups variance Independence: \\(\\text{cov}(e_{ij}, e_{kj})=0\\) It is also assumed that there is independence between the random effects and random errors (\\(\\text{cov}(e_{ij}, u_{j})=\\text{cov}(e_{ij}, u_{k})=0\\)) and independence between the group level effects (\\(u\\)) and the covariates (\\(X\\)). The random effect for the group level accounts for the difference between groups by “shifting” the intercept up or down by a random amount \\(u_j\\) characteristic of each group. Using the exam dataset, an empty 2-level random intercepts model can be fitted to explore the GCSE scores of pupils by different schools. If no covariates are included, an empty model is fitted, taking the following form. \\[ \\text{GCSE}_{ij} = u_j + e_{ij}.\\] #fit the empty random intercepts model to the exam dataset exam_emptyRI &lt;- lme(GCSEscore ~ -1, random = ~1|SchoolID, method = &#39;ML&#39;, data = exam) summary(exam_emptyRI) ## Linear mixed-effects model fit by maximum likelihood ## Data: exam ## AIC BIC logLik ## 11014.71 11027.33 -5505.355 ## ## Random effects: ## Formula: ~1 | SchoolID ## (Intercept) Residual ## StdDev: 0.4106958 0.9207448 ## ## Fixed effects: GCSEscore ~ -1 ## [1] Value Std.Error DF t-value p-value ## &lt;0 rows&gt; (or 0-length row.names) ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -3.948756027 -0.653647311 0.009080812 0.693656002 3.656266940 ## ## Number of Observations: 4059 ## Number of Groups: 65 (#fig:empty lme summary)Interpreting the summary output To compute the estimated total variance for the two-level random intercepts model the variance components for both the random effects and the error terms must be summed, where both components are provided by the summary output for the model. \\[\\hat{\\sigma}_u^2 +\\hat{\\sigma}_e^2 = 0.411^2 + 0.921^2 = 1.017.\\] To investigate the significance of the group level differences, the VPC can be computed using the above total variance and the level 2 variance as follows. \\[ \\text{VPC} =\\frac{\\sigma^2_u}{\\sigma^2_u+\\sigma^2_e} = \\frac{0.411^2}{1.017} = 0.166,\\] which can be interpreted as the level 2 variability (between schools) accounts for 16.6% of the total variability observed in the GCSE scores of the pupils. For a random intercepts model, the VPC is equal to the expected correlation between 2 randomly selected observations that belong to the same group, also known as the intra-class correlation. This means that a for VPC of 0.166 for the random intercepts model above, the expected correlation between the GCSE scores of 2 randomly selected pupils within the same school is 0.166. 4.9.4 Random intercepts mixed-effects modelling Random intercepts mixed modelling takes into account that observations come from different groups through using both fixed and random effects, where the general notation for the form of the (2-level) model is as follows. \\[ y_{ij} = \\beta_0 + \\beta_1x_{ij} + u_j +e_{ij}, \\] where \\(i\\) is the index for level 1, \\(j\\) is the index for level 2 and \\(\\beta_0\\): overall intercept (fixed effect) \\(\\beta_1\\): overall slope (fixed effect) \\(u_j\\): group level effect (random effect) Normally distributed: \\(u_j \\sim N(0, \\sigma^2_u)\\) where \\(\\sigma^2_u\\) is the between-groups variance Independence: \\(\\text{cov}(u_j, u_{k})=0\\) \\(e_{ij}\\): individual level error (random error) Normally distributed: \\(e_{ij} \\sim N(0, \\sigma^2_e)\\) where \\(\\sigma^2_e\\) is the within-groups variance Independence: \\(\\text{cov}(e_{ij}, e_{kj})=0\\) It is also assumed that there is independence between the random effects and random errors (\\(\\text{cov}(e_{ij}, u_{j})=\\text{cov}(e_{ij}, u_{k})=0\\)) and independence between the group level effects (\\(u\\)) and the covariates (\\(X\\)). As with the random intercepts only modelling, the random effect for the group level accounts for the difference between groups by “shifting” the intercept up or down by a random amount \\(u_j\\) characteristic of each group. Using the exam dataset, an empty 2-level random intercepts mixed model can be fitted to explore the GCSE scores of pupils by different schools. The empty model that has only an intercept term (no covariates) for the fixed effects and a random intercept for the random effects takes the following form. \\[ \\text{GCSE}_{ij} = \\beta_0 + u_j + e_{ij},\\] which can be re-written as \\[ \\text{GCSE}_i = \\beta_{0j} + e_{i},\\] where \\(\\beta_{0j}=\\beta_0+u_j\\) is the intercept for school \\(j\\). #fit the empty random intercepts model to the exam dataset exam_empty2lvlMRI &lt;- lme(GCSEscore ~ 1, random = ~1|SchoolID, method = &#39;ML&#39;, data = exam) summary(exam_empty2lvlMRI) ## Linear mixed-effects model fit by maximum likelihood ## Data: exam ## AIC BIC logLik ## 11016.65 11035.58 -5505.324 ## ## Random effects: ## Formula: ~1 | SchoolID ## (Intercept) Residual ## StdDev: 0.4106567 0.9207391 ## ## Fixed effects: GCSEscore ~ 1 ## Value Std.Error DF t-value p-value ## (Intercept) -0.01316707 0.05363399 3994 -0.2454986 0.8061 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -3.94711015 -0.64856012 0.01168106 0.69918450 3.65782427 ## ## Number of Observations: 4059 ## Number of Groups: 65 To compute the estimated total variance for the two-level random intercepts mixed-effects model the variance components for both the random effects and the error terms must be summed, where both components are provided by the summary output for the model. \\[\\hat{\\sigma}_u^2 +\\hat{\\sigma}_e^2 = 0.411^2 + 0.921^2 = 1.017,\\] which is the same total variance as found from the empty random intercepts model above, indicating that adding the (fixed effect) intercept does not alter the total variance seen. This means that the VPC is also the same as in the empty RI model. As with the other types of modelling covered in this module, covariates can also be included in a random intercepts model. To see which (continuous) covariates might have a relationship with GCSE score, the pairs() function can be used with the dataset as an argument to produce a matrix of scatter plots. #produce pairs plot for exam data pairs(exam) The resulting scatter plots indicate that there may be a strong relationship between the GCSE score and LRT score of the pupils, so this covariate will be added to the random intercepts model, resulting in the following model. \\[ \\text{GCSE}_{ij} = \\beta_0 + \\beta_1\\text{LRT}_{ij} + u_j + e_{ij},\\] where the mean of GCSE is allowed to vary across schools after adjusting by LRT in addition to the assumption that the effect for LRT is the same for all schools. Given that this is a random intercept model and there is no random slope component, this model creates parallel cluster-specific (school-specific) lines which have different intercepts. To fit this model in R, the lme function can be used again but with the addition of a fixed effect term for LRT. #fit the random intercept model to the exam data with a covariate for LRTscore exam_2lvlMRI1 &lt;- lme(GCSEscore ~ LRTscore, random = ~ 1|SchoolID, method = &#39;ML&#39;, data = exam) summary(exam_2lvlMRI1) ## Linear mixed-effects model fit by maximum likelihood ## Data: exam ## AIC BIC logLik ## 9365.243 9390.478 -4678.622 ## ## Random effects: ## Formula: ~1 | SchoolID ## (Intercept) Residual ## StdDev: 0.303528 0.7521509 ## ## Fixed effects: GCSEscore ~ LRTscore ## Value Std.Error DF t-value p-value ## (Intercept) 0.0023908 0.04003256 3993 0.05972 0.9524 ## LRTscore 0.5633712 0.01246847 3993 45.18365 0.0000 ## Correlation: ## (Intr) ## LRTscore 0.008 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -3.71622343 -0.63042350 0.02866066 0.68442610 3.26797827 ## ## Number of Observations: 4059 ## Number of Groups: 65 The summary output for this model contains the same information as for the empty random intercepts model, with the addition of coefficient and correlation information for the LRT covariate. The coefficient estimate indicates that for a pupil with an average LRT score, the estimated overall mean of standardised GCSE score is 0.002, but for each unit increase in the LRT score, the expected standardised GCSE score of a pupil will increase by 0.563. Looking at the variance for this model, the within-group (within-school) variance has reduced meaning that the heterogeneity (difference) in the reading abilities of the pupils explains some of the heterogeneity in the GCSE scores. The between-group variance is also reduced, leading to the total variance reducing to \\(\\hat{\\sigma}^2_u+\\hat{\\sigma}^2_e = 0.304^2+0.752^2 = 0.658\\). The updated VPC is also reduced slightly, computed as \\[ \\text{VPC} =\\frac{\\sigma^2_u}{\\sigma^2_u+\\sigma^2_e} = \\frac{0.304^2}{0.658} = 0.140,\\] meaning that 14% of the total variability observed in the GCSE scores can be attributed to the variability between schools, once controlled by the London Reading Test scores. The model fit information for this model indicates that it is a better fit than the empty random intercept model, with lower AIC and BIC values. In addition, the variance for both the random effects and random error are smaller due to the addition of a covariate which aids in explaining some of the variability seen in the data. To demonstrate how the school level impacts the intercept of the model, a random selection of schools can be taken and the corresponding lines of best fit can be plotted. To do this, predicted values need to be computed, where from the model above, two types of predicted values can be computed: Population average predicted values: (unconditional inference) beneficial for making predictions for groups not included in the available data since only the coefficient estimates for the fixed part of the model are used. Group-specific predicted values: (conditional inference) beneficial for making predictions for groups included in the available data since the coefficient estimates for both the fixed and random parts of the model are used. To obtain these predicted values, the fitted() function can be used in a similar way to the predict() function introduced earlier in this module, however, instead of including a new dataset in the function, a level argument is included, where level = 0 computes the population average predicted values and level = 1 computes the group-specific predicted values. attach(exam) #identify the unique schools and record their types unique_school &lt;- unique(exam[,c(1,6)]) #sample 6 of the schools set.seed(1) sampled_school &lt;- sample(unique_school$SchoolID, 6) #school-specific predicted values from model with covariate for LRTscore fitted1 &lt;- fitted(exam_2lvlMRI1, level=1) #subset original data for the 6 sampled schools subset_school &lt;- data.frame(exam[SchoolID %in% sampled_school, ], fit1 = fitted1[SchoolID %in% sampled_school]) #plot the GCSE and LRT scores for the sampled schools with lines of best fit plot(subset_school$LRTscore, subset_school$GCSEscore, pch = 16, cex = 0.2, col=seq(1:length(sampled_school)), xlab = &quot;LRT score&quot;, ylab = &quot;GCSE&quot;) for (i in 1:length(sampled_school)){ lines(subset_school$LRTscore[subset_school$SchoolID == sampled_school[i]], subset_school$fit1[subset_school$SchoolID == sampled_school[i]], col = i, lty = 1) } n &lt;- table(exam$SchoolID) #sizes by school legend(&quot;bottomright&quot;,legend=paste(sampled_school, &quot;(&quot;,n[sampled_school], &quot;)&quot;) , col = seq(1:length(sampled_school)), lty = 1, cex = 0.7) The plot provides the school-specific predicted values for 6 randomly selected schools, where it can be seen that the higher the LRT score, the higher the GCSE score for all of the schools, where the slope of each school-specific line is the same. However, the intercept for each school varies due to the random intercept included in the model, where on average, the school with ID number 1 has the highest GCSE scores, and the school with ID number 23 has the lowest GCSE scores. 4.9.5 Random slope mixed-effects modelling Random slope (and intercepts) mixed-effects modelling is an extension of random intercepts modelling that also allows for the relationship between the dependent and independent variables to differ for each group, meaning that not only can the intercept differ by group, but so can the slope. This difference is accounted for through an additional random effect for the slope which acts as an interaction between the group and the independent variable \\(X\\), leading to lines which are not parallel as in the random intercepts modelling. The general notation for the (2-level) model is as follows. \\[ y_{ij} = \\beta_0 + \\beta_1x_{ij} + u_{0j} + u_{1j}x_{ij} + e_{ij}, \\] where \\(i\\) is the index for level \\(1\\), \\(j\\) is the index for level \\(2\\) and \\(\\beta_0\\): overall intercept (fixed effect) \\(\\beta_1\\): overall slope (fixed effect) \\(u_{0j}\\): group level intercept (random effect) Normally distributed: \\(u_{0j} \\sim N(0, \\sigma^2_{u0})\\) where \\(\\sigma^2_u\\) is the between-groups variance Independence (across groups): \\(\\text{cov}(u_{0j}, u_{0jk})=0\\) \\(u_{1j}\\) group level slope (random effect) Normally distributed: \\(u_{1j} \\sim N(0, \\sigma^2_{u1})\\) Independence (across groups): \\(\\text{cov}(u_{1j}, u_{1jk})=0\\) \\(e_{ij}\\): individual level error (random error) Normally distributed: \\(e_{ij} \\sim N(0, \\sigma^2_e)\\) where \\(\\sigma^2_e\\) is the within-groups variance Independence: \\(\\text{cov}(e_{ij}, e_{kj})=\\text{cov}(e_{ij}, u_{0j})=\\text{cov}(e_{ij}, u_{1j})=0\\) It is also assumed that the random intercept and random slope are dependent within a group, \\(\\text{cov}(u_{0j}, u_{1j})=\\sigma_{u01}\\). Additionally, the inclusion of the random slope means that there is heterogeneous variance, meaning that the variance is not constant as it is in the random intercepts modelling (variance in random slopes modelling is a quadratic function of x). Using the exam dataset, the 2-level random slopes model can be fitted to explore the GCSE scores of pupils by different schools, controlling for the LRT score of the pupils and allowing for the relationship between GCSE score and LRT score to differ by school, with the model taking the following form. \\[ GCSE_{ij} = \\beta_0 + \\beta_1 \\times LRTscore_{ij} + u_{0j} + u_{1j}\\times LRTscore_{ij} + e_{ij},\\] which can be rewritten as \\[GCSE_{ij} = \\beta_{0ij} + \\beta_{1j}\\times LRTscore_{ij},\\] where \\(\\beta_{0ij}=\\beta_0 + u_{0j}+e_{ij}\\) and \\(\\beta_{1j}=\\beta_1+u_{1j}\\). To incorporate this random slope into the model in R, the random slope term can be included in the random argument in the lme() function after the random intercept term. This is seen in the R code as follows. #fit the random slope mixed model to the exam dataset exam_2lvlMRS &lt;- lme(GCSEscore ~ LRTscore, random = ~ 1 + LRTscore | SchoolID, method = &#39;ML&#39;, data = exam) summary(exam_2lvlMRS) ## Linear mixed-effects model fit by maximum likelihood ## Data: exam ## AIC BIC logLik ## 9328.871 9366.723 -4658.435 ## ## Random effects: ## Formula: ~1 + LRTscore | SchoolID ## Structure: General positive-definite, Log-Cholesky parametrization ## StdDev Corr ## (Intercept) 0.3007321 (Intr) ## LRTscore 0.1205728 0.497 ## Residual 0.7440807 ## ## Fixed effects: GCSEscore ~ LRTscore ## Value Std.Error DF t-value p-value ## (Intercept) -0.0115037 0.03979185 3993 -0.289097 0.7725 ## LRTscore 0.5567295 0.01994265 3993 27.916526 0.0000 ## Correlation: ## (Intr) ## LRTscore 0.365 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -3.83128151 -0.63248819 0.03403727 0.68320736 3.45615474 ## ## Number of Observations: 4059 ## Number of Groups: 65 Population average (or unconditional) inference can be done simply through interpreting the results from the summary output. For example, over the whole population of the schools, if a pupil has an average LRT score (LRT score = 0), the pupil is expected to get a GCSE score of \\(-0.012\\). Additionally, over the whole population of the schools, for each unit increase in the LRT score, the expected GCSE score of a pupil increases by 0.557. To test if the additional random slope parameter is significant, a likelihood ratio test can be conducted, computing the log-likelihood values with the function logLik() and using the function pchisq() to find the corresponding \\(p\\)-value. #compute lambda llmod1 &lt;- logLik(exam_2lvlMRI1) llmod2 &lt;- logLik(exam_2lvlMRS) lambda &lt;- -2*(llmod1 - llmod2) #compute the corresponding p-value (df = 2 because of the additional terms) pchisq(lambda[1], df = 2, lower.tail = FALSE) ## [1] 1.711123e-09 The resulting \\(p\\)-value is approximately zero, meaning that it is much smaller than the 5% significance level, indicating that the additional term for the random slope is highly significant. The interpretation of this result is that there is evidence that the effect that the LRT scores have on GCSE scores varies across the schools. Alternatively, an ANOVA table can be used to test the significance as follows. #anova of two models anova(exam_2lvlMRI1, exam_2lvlMRS) ## Model df AIC BIC logLik Test L.Ratio p-value ## exam_2lvlMRI1 1 4 9365.243 9390.478 -4678.622 ## exam_2lvlMRS 2 6 9328.871 9366.723 -4658.435 1 vs 2 40.37223 &lt;.0001 Where the results from the ANOVA table also indicate that the addition of the random slope is significant. Given the additional terms involved with random slopes modelling, the VPC formula changes, with the resulting formula as follows. \\[ \\text{VPC}(x) = \\frac{\\text{Level 2 variance}}{\\text{Total variance}} = \\frac{\\text{Var}(u_{0j}+u_{1j}x_{ij})}{\\text{Var}(u_{0j}+u_{1j}x_{ij}+e_{ij})}=\\frac{\\sigma^2_{u0}+2x_{ij}\\sigma_{u01}+x_{ij}^2\\sigma^2_{u1}}{\\sigma^2_{u0}+2x_{ij}\\sigma_{u01}+x_{ij}^2\\sigma^2_{u1}+\\sigma^2_e}, \\] where each of the variance and covariance terms can be found from reading the summary output of the model in the random effects part. The VPC is also dependent on the value of \\(x\\) and a quadratic function of \\(x\\), unlike with the random intercepts models where the VPC is independent of \\(x\\) so is constant across the different values of \\(x\\). (#fig:lme RS summary)Interpreting the summary output \\[\\text{VPC(LRTscore)} = \\frac{\\sigma^2_{u0}+2x\\sigma_{u01}+x^2\\sigma^2_{u1}}{\\sigma^2_{u0}+2x\\sigma_{u01}+x^2\\sigma^2_{u1}+\\sigma^2_e} = \\frac{0.301^2 + 2\\times \\text{LRTscore}_{ij} \\times 0.497 + \\text{LRTscore}_{ij}^2 \\times 0.121^2}{0.301^2 + 2\\times \\text{LRTscore}_{ij} \\times 0.497 + \\text{LRTscore}_{ij}^2 \\times 0.121^2 + 0.744^2},\\] which is dependent on the LRT score. As with the random intercepts mixed-effects model, the results from the random slopes mixed-effects model can be plotted to visually demonstrate the effect that the additional terms have on the models. This is done in the same way as before, through randomly sampling a selection of schools, computing the conditional predicted values with the fitted() function and plotting the results with a line of best fit for each of the sampled schools. #identify the unique schools and record their types unique_school &lt;- unique(exam[,c(1,6)]) #sample 6 of the schools set.seed(1) sampled_school &lt;- sample(unique_school$SchoolID, 6) #school-specific predicted values from model with covariate for LRTscore fitted2 &lt;- fitted(exam_2lvlMRS, level = 1) #subset original data for the 6 sampled schools subset_school2 &lt;- data.frame(exam[SchoolID %in% sampled_school,], fit2 = fitted2[SchoolID %in% sampled_school]) #plot the GCSE and LRT scores for the sampled schools with lines of best fit plot(subset_school2$LRTscore, subset_school2$GCSEscore, pch = 16, cex = 0.2, col=seq(1:length(sampled_school)), xlab = &quot;LRT score&quot;, ylab = &quot;GCSE&quot;) for (i in 1:length(sampled_school)){ lines(subset_school2$LRTscore[subset_school2$SchoolID == sampled_school[i]], subset_school2$fit2[subset_school2$SchoolID == sampled_school[i]], col = i, lty = 1) } n &lt;- table(exam$SchoolID) #sizes by school legend(&quot;bottomright&quot;, legend=paste(sampled_school, &quot;(&quot;,n[sampled_school], &quot;)&quot;) , col = seq(1:length(sampled_school)), lty = 1, cex = 0.7) The plot provides the school-specific predicted values for 6 randomly selected schools, where the same trend between LRT score and GSCE score is seen as before (the higher the LRT score the higher the GCSE score), however, the effect that LRT score has on GCSE score changes depending on the school. For example, the slope for the school with ID number 1 has a steeper slope than that of the school with ID number 23, indicating that for school 1, a higher LRT score has more of an impact on the pupil’s GCSE score than for school 23. 4.9.6 Adding an extra level When there are only 2 levels in a model, it is assumed that the level 2 groups are independent of each other. However, this is not always the case, and this is where it is important to add a level 3 group. When there are three levels, \\(i\\) corresponds to level 1, \\(j\\) corresponds to level 2 and \\(k\\) corresponds to level 3. There are two cases for random structure in 3-level models which are important to distinguish between given that they are specified differently in the lme() function in R. Case 1: The first case for the 3-level mixed-effects model form is that where the models at all 3 of the levels are equal. This can mean that there are either only random intercepts at each level or there are random intercepts and random slopes at each level. The general form of the 3-level random intercepts mixed-effects model where there are only random intercepts at each level is given as follows. \\[y_{ijk} = \\beta_0 + \\beta_1x_{ijk} + v_{0k} + u_{0jk} + e_{0ijk}, \\] which can be rewritten as \\[ y_{ijk} = \\beta_{0ijk} + \\beta_{1}x_{ijk},\\] where \\(\\beta_{0ijk} = \\beta_0 + v_{0k} + u_{0jk} + e_{ijk}\\). Then, the general form of the 3-level random slopes mixed-effects model where there are random intercepts and random slopes at each level is given as follows. \\[y_{ijk} = \\beta_0 + \\beta_1x_{ijk} + v_{0k}+ v_{1k}x_{ijk} + u_{0jk} + u_{1jk}x_{ijk} + e_{0ijk}, \\] which can be rewritten as \\[ y_{ijk} = \\beta_{0ijk} + \\beta_{1jk}x_{ijk},\\] where \\(\\beta_{0ijk} = \\beta_0 + v_{0k} + u_{0jk} + e_{ijk}\\) and \\(\\beta_{1jk} = \\beta_1 + v_{1k} + u_{1jk}\\). Case 2: The second case for the 3-level mixed-effects model form is when the models at some levels are different, for example, if only the second level has a random slope, but all levels have a random intercept. The general form of this model is given as follows. \\[y_{ijk} = \\beta_0 + \\beta_1x_{ijk} + v_{0k} + u_{0jk} + u_{1jk}x_{ijk} + e_{0ijk}, \\] which can be rewritten as \\[ y_{ijk} = \\beta_{0ijk} + \\beta_{1jk}x_{ijk},\\] where \\(\\beta_{0ijk} = \\beta_0 + v_{0k} + u_{0jk} + e_{ijk}\\) and \\(\\beta_{1jk} = \\beta_1 + u_{1jk}\\). When using the lme() function in R, the random structure in the first case is specified as random = ~ () | level3/level2, compared to the second case where the random structure is specified as random = list(~ | ~ | ~ |). These differences are explored in this section with application to the exam dataset example, where pupils are nested within schools, and schools can be grouped by district, and schools within the same district share common characteristics. The indices of each level are then given as pupil \\(i\\) is nested in school \\(j\\) which is nested in district \\(k\\). #add district level to the exam data district &lt;- read.table(paste0(data_path, &#39;exam_district.csv&#39;), header = FALSE) colnames(district) &lt;- &quot;district&quot; exam[, 7] &lt;- district Case 1: The 3-level mixed-effects model with random intercepts only is given as follows. \\[GCSE_{ijk} =( \\beta_0 + v_{0k} + u_{0jk} ) + \\beta_1 LRTscore_{ijk} + e_{0ijk},\\] which is fitted in R using the lme() function in the following code. #fit the 3-level random intercepts only model exam_3lvlMRI &lt;- lme(GCSEscore ~ LRTscore, random = ~ 1|district/SchoolID, method = &#39;ML&#39;, data = exam) summary(exam_3lvlMRI) ## Linear mixed-effects model fit by maximum likelihood ## Data: exam ## AIC BIC logLik ## 9418.347 9449.89 -4704.174 ## ## Random effects: ## Formula: ~1 | district ## (Intercept) ## StdDev: 0.05137938 ## ## Formula: ~1 | SchoolID %in% district ## (Intercept) Residual ## StdDev: 0.276128 0.7530999 ## ## Fixed effects: GCSEscore ~ LRTscore ## Value Std.Error DF t-value p-value ## (Intercept) -0.0168807 0.03176277 3934 -0.53146 0.5951 ## LRTscore 0.5660127 0.01249837 3934 45.28692 0.0000 ## Correlation: ## (Intr) ## LRTscore 0.011 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -3.73839940 -0.63848886 0.02900113 0.67849607 3.28534013 ## ## Number of Observations: 4059 ## Number of Groups: ## district SchoolID %in% district ## 20 124 The 3-level mixed-effects model with random intercepts and random slopes at each level is given as follows. \\[GCSE_{ijk} = (\\beta_0 + v_{0k} + u_{0jk})+ (\\beta_1 + v_{1k} + u_{1jk})LRTscore_{ijk} + e_{0ijk},\\] which is fitted in R using the lme() function in the following code. #fit the 3-level random intercepts only model exam_3lvlMRS &lt;- lme(GCSEscore ~ LRTscore, random = ~ (1 + LRTscore) | district/SchoolID, data = exam, method = &#39;ML&#39;, control = lmeControl(opt = &quot;optim&quot;)) summary(exam_3lvlMRS) ## Linear mixed-effects model fit by maximum likelihood ## Data: exam ## AIC BIC logLik ## 9390.575 9447.353 -4686.287 ## ## Random effects: ## Formula: ~(1 + LRTscore) | district ## Structure: General positive-definite, Log-Cholesky parametrization ## StdDev Corr ## (Intercept) 0.03629237 (Intr) ## LRTscore 0.02284358 0.284 ## ## Formula: ~(1 + LRTscore) | SchoolID %in% district ## Structure: General positive-definite, Log-Cholesky parametrization ## StdDev Corr ## (Intercept) 0.2755233 (Intr) ## LRTscore 0.1141072 0.465 ## Residual 0.7449016 ## ## Fixed effects: GCSEscore ~ LRTscore ## Value Std.Error DF t-value p-value ## (Intercept) -0.0247791 0.03045501 3934 -0.813631 0.4159 ## LRTscore 0.5621427 0.01795309 3934 31.311754 0.0000 ## Correlation: ## (Intr) ## LRTscore 0.282 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -3.86834331 -0.62985398 0.03307898 0.67616436 3.46720448 ## ## Number of Observations: 4059 ## Number of Groups: ## district SchoolID %in% district ## 20 124 Case 2: The 3-level mixed-effects model with random intercepts for all levels and a random slope for level 2 is given as follows. \\[GCSE_{ijk} = (\\beta_0 + v_{0k} + u_{0jk}) + (\\beta_1 + u_{1jk}) LRTscore_{ijk} + e_{0ijk},\\] which is fitted in R using the lme() function in the following code. #fit the 3-level random intercepts only model exam_3lvlMRIS &lt;- lme(GCSEscore ~ LRTscore, random = list( ~ 1|district, ~ 1 + LRTscore|SchoolID), method = &#39;ML&#39;, data = exam, control = lmeControl(opt = &quot;optim&quot;)) summary(exam_3lvlMRIS) ## Linear mixed-effects model fit by maximum likelihood ## Data: exam ## AIC BIC logLik ## 9386.671 9430.832 -4686.336 ## ## Random effects: ## Formula: ~1 | district ## (Intercept) ## StdDev: 0.01850324 ## ## Formula: ~1 + LRTscore | SchoolID %in% district ## Structure: General positive-definite, Log-Cholesky parametrization ## StdDev Corr ## (Intercept) 0.2773087 (Intr) ## LRTscore 0.1170200 0.466 ## Residual 0.7448742 ## ## Fixed effects: GCSEscore ~ LRTscore ## Value Std.Error DF t-value p-value ## (Intercept) -0.0249776 0.02955805 3934 -0.84504 0.3981 ## LRTscore 0.5612958 0.01718500 3934 32.66196 0.0000 ## Correlation: ## (Intr) ## LRTscore 0.282 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -3.8691875 -0.6306811 0.0325292 0.6732546 3.4644060 ## ## Number of Observations: 4059 ## Number of Groups: ## district SchoolID %in% district ## 20 124 As with the other modelling approaches, the a likelihood ratio test can be used to check the significance, in this case of the 3rd level by comparing the 2-level random slopes model created above with the 3-level model with random intercepts at each level and a random slope at the school level. #compute lambda llmod1 &lt;- logLik(exam_2lvlMRS) llmod2 &lt;- logLik(exam_3lvlMRIS) lambda &lt;- -2*(llmod1-llmod2) #compute the corresponding p-value (df = 1 because of the additional term) pchisq(lambda[1], df = 1, lower.tail = FALSE) ## [1] 1 Which indicates that the addition of the district as a 3rd level is not significant given that the resulting \\(p\\)-value is not less than the 5% significance level. 4.10 Useful resources Statistical modelling: Introduction to Statistical Modeling Linear regression: Simple Linear Regression: Everything You Need to Know Polynomial regression: Polynomial Regression: An Introduction Non-linear regression: GeeksforGeeks Generalised linear regression: GeeksforGeeks Model predictions: Using Linear Regression for Predictive Modeling in R Model selection: Model Selection Stepwise regression: A Complete Guide to Stepwise Regression in R Stepwise regression: GeeksforGeeks Cross-validation:GeeksforGeeks Hierarchical regression: Hierarchical Linear Models "],["probability-theory-and-applications.html", "5 Probability Theory and Applications 5.1 Introduction to probability 5.2 Axioms of probability 5.3 Joint probability 5.4 Conditional probability 5.5 Contingency tables 5.6 Useful resources", " 5 Probability Theory and Applications This module provides an introduction to probability theory, focusing on the methods and applications rather than the code. Module 5 covers the foundations of both theoretical and experimental probability, the axioms of probability, and more detailed explanations of joint and conditional probability, with simple and easy-to-follow examples throughout. 5.1 Introduction to probability 5.1.1 Theoretical and experimental probability In the most simple sense, the probability of an event is how likely it is to happen, or the chance or an event occurring. This is known as theoretical probability. For example, the theoretical probability of a fair coin landing on heads when flipped is 0.5. This classical definition is given formally in the formula below. \\[\\text{theoretical probability} = \\frac{\\text{number of favourable (or desired) outcomes}}{\\text{total number of possible outcomes}}.\\] In contrast, experimental probability, or relative frequency, is the actual probability of an outcome, found through using an actual experiment. For example, you could flip a fair coin 10 times, landing on heads 4 times, leading to an experimental probability of landing on heads of 0.4, which is smaller than the theoretical probability of 0.5. This is given formally in the formula below. \\[\\text{experimental probability} = \\frac{\\text{number of times event actually occurs}}{\\text{total number of trials in experiment}}.\\] When the number of trials is very large, the experimental probability is typically seen to tend towards the theoretical probability. This is known as the relative frequency theory of probability or the Law of Large Numbers. For example, if a fair coin is flipped 10 times, landing on head 4 times, this experimental probability of 0.4 is different to the theoretical probability of 0.5. However, the more times the coin is flipped, the closer the experimental probability should get to 0.5. #create a fair &#39;coin&#39; coin &lt;- c(&quot;heads&quot;, &quot;tails&quot;) #set seed for random sampling to be repeatable set.seed(100) #simulate flipping the coin 10 times coin_sample_10 &lt;- sample(coin, size = 10, replace = TRUE) #experimental probability of flipping a fair coin 10 times sum(coin_sample_10 == &quot;heads&quot;)/10 ## [1] 0.4 #set seed for random sampling to be repeatable set.seed(100) #simulate flipping the coin 10000 times coin_sample_10000 &lt;- sample(coin, size = 10000, replace = TRUE) #experimental probability of flipping a fair coin 10000 times, it is much closer #to the theoretical probability sum(coin_sample_10000 == &quot;heads&quot;)/10000 ## [1] 0.5014 5.1.2 Keywords Outcome: A possible result from a trial or an experiment Event: One single outcome from a trial or an experiment Sample space: A set of all possible outcomes of a trial (e.g. for tossing a coin once, the sample space is \\(S={Heads, Tails}\\)) Mutually exclusive events: Two events that cannot occur at the same time. Let \\(A_1, A_2, \\cdots, A_k\\) mutually exclusive events, then \\(A_i \\cap A_j = \\varnothing\\) for all \\(1 \\leq i \\neq j \\leq k\\). Mutually exhaustive events: At least one of the events must occur. Let \\(A_1, A_2, \\cdots, A_k\\) be exhaustive events, then \\(A_1 \\cup A_2 \\cup \\cdots \\cup A_k = S\\) Independent events: Two events are said to be independent if the occurrence of one does not affect the occurrence of the other. Let \\(A\\) and \\(B\\) be two independent events, then \\(P(A\\cap B)= P(A) \\times P(B)\\). Sample point: One of the possible outcomes in a sample space. The following Venn diagrams provide a visual demonstration of how probabilities of events are written. Figure 5.1: Venn diagrams different event probabilities 5.2 Axioms of probability Before defining the axioms of probability, it is important to understand marginal probability. The marginal probability of an event \\(A\\), \\(P(A)\\), is the probability of only event \\(A\\) and does not take other events into consideration. For example, if event \\(A\\) is drawing an ace from a standard deck of cards, the marginal probability of \\(A\\) is given as: \\(P(A) = \\text{number of aces in the deck}/\\text{total number of cards in the deck} = 4/52 = 1/13\\). In this example, the probability of drawing an ace does not depend on any other cards being drawn, it only considers that event. Axiom 1: The probability of an event A is a non-negative real number, \\[0 \\leq P(A).\\] Axiom 2: The probability of the sample space S is equal to 1, \\[P(S)=1.\\] (The probability that at least one of all possible outcomes occurring is equal to 1). Axiom 3: If A and B are two mutually exclusive events, then the probability of their union is the sum of their individual probabilities, \\[ P(A \\cup B)=P(A)+P(B).\\] This is demonstrated visually with the below Venn diagram. Figure 5.2: Venn diagram for probability of events A or B occurring when A and B are mutually exclusive (Axiom 3) The following examples are used for demonstrative purposes to better explain the axioms of probability. Axiom 1: You cannot have the probability of landing on 1 when tossing a fair die be \\(-1/6\\), it must be non-negative. You can however have a probability of 0, for example, the probability of landing on 7 when tossing a fair 6 sided die is 0, given that there are no sides for the number 7. Axiom 2: The (theoretical) probability of landing on heads when flipping a fair coin is \\(1/2\\), and the probability of landing on tails is also \\(1/2\\). The sum of these two outcomes is equal to \\(1\\). Since you are certain that one of the two outcomes will occur (the coin must land on either heads or tails), the probability of heads or tails occurring is equal to 1. Axiom 3: When rolling a fair die, you cannot land on both sides for 1 and 6 at the same time. This means that these two events cannot happen at the same time and are therefore mutually exclusive. The probability of landing on a 1 or the probability of landing on a 6 in one roll is equal to the probabilities of each event occurring separately added (\\(1/6 + 1/6 = 2/6\\)). 5.3 Joint probability Joint probability refers to the probability of two (or more) events/outcomes occurring at the same time, where the notation \\(P(A \\cap B)\\) is used to denote the probability of events \\(A\\) and \\(B\\) occurring at the same time. This probability is shown in the below Venn diagram by the intersection of the two events. Figure 5.3: Venn diagram for probability of events A and B occurring at the same time 5.3.1 Joint probability table Joint probability tables are a useful tool for storing and organising the likelihoods of the different events and combinations when the variables are discrete, for example, a Bernoulli variable to indicate whether someone is vaccinated or not, or a categorical variable indicating a fever category (low, normal or high). The marginal probability of an event can be obtained from a joint probability table through summing the joint probabilities across the relevant rows or columns for the event of interest. For example, in the figure below, to compute the marginal probability of event \\(Y=1\\) occurring, you sum the columns under that event, leading to the marginal probability \\(P(Y=1)=a+b\\). Formally, if both \\(X\\) and \\(Y\\) are discrete, the marginal probability of \\(X\\) and \\(Y\\) are given as \\(f_X(x)=\\sum_y f(x,y)\\) and \\(f_Y(y)=\\sum_x f(x,y)\\) respectively, where \\(f(x,y)=P(X\\cap Y)\\) is known as the probability mass function. Figure 5.4: Example of a joint probability table Example: Suppose that an individual having a low or high fever indicates that the individual is infected with a disease A, where a high fever is more indicative that they are infected than a low fever. Also suppose that there is a vaccine for the disease that reduces the probability that an individual is infected. The likelihoods of that an individual is infected based on their vaccine and fever status is given in the below joint probability table. Low Normal High Vaccinated 0.11 0.03 0.19 Not Vaccinated 0.20 0.04 0.43 To work out the probability that an individual that is not vaccinated and has a low fever is infected, you can look at the cell that corresponds to the row for ‘Not Vaccinated’ and the column for ‘Low’ fever. This probability is 20%. Exercise: Following on from the example above, what is the probability that an individual that is vaccinated but has a high fever is infected with disease A? 5.4 Conditional probability Conditional probability refers to the probability of an event/outcome occurring given that another event/outcome has already occurred. It focuses on the probability of outcomes in situations where there is additional information on outcomes that have already happened. Formally, if \\(A\\) and \\(B\\) are events and \\(P(B)&gt;0\\), then the conditional probability of \\(A\\) given \\(B\\) \\((P(A|B))\\) is given as \\[P(A|B)=\\frac{P(A\\cap B)}{P(B)},\\] where \\(P(A\\cap B)\\) is the joint probability of \\(A\\) and \\(B\\) and \\(P(B)\\) is the marginal probability of \\(B\\). Example: Assume you have a bag of 10 buttons that contains 2 red buttons, 4 blue buttons and 4 purple buttons. To compute the conditional probability of drawing a blue button, given that the button drawn is not red, let event \\(A\\) be drawing a blue button from the bag, and event \\(B\\) be drawing a button from the bag that is not red. To work out this conditional probability, use the above formula as follows. \\[ \\begin{aligned} P(A|B)&amp;=\\frac{P(A\\cap B)}{P(B)}\\\\ &amp;= \\frac{\\text{number of blue buttons}/\\text{total number of buttons}}{\\text{number of buttons that are not red}/\\text{total number of buttons}}\\\\ &amp;=\\frac{6/10}{8/10}\\\\ &amp;=\\frac{3}{4}=0.75. \\end{aligned} \\] Therefore, the conditional probability of drawing a blue button, given that the button drawn is not red is 0.75. Exercise: Following on from the example above, what is the conditional probability of drawing a purple button given that the button drawn is not blue? 5.4.1 Multiplication rule of conditional probability The multiplication rule of conditional probability is found through rearranging the formula for conditional probability, resulting in the following formula. \\[ P(A\\cap B) = P(A)P(B|A) = P(B)P(A|B).\\] This multiplication rule can be extended for more than two events. For events \\(A_1, A_2, \\cdots, A_k\\), the multiplication rule of conditional probability is given as follows. \\[ P(A_1\\cap A_2 \\cap \\cdots \\cap A_k) = P(A_1)P(A_2|A_1)P(A_3|A_1\\cap A_2)\\cdots P(A_k|A_1\\cap A_2\\cap \\cdots \\cap A_{k-1}).\\] Example: Assume you have an island with 10 people, 6 women (red) and 4 men (blue), and want to select two people from the island (without replacement). Let \\(M_i\\) be the probability that the \\(i\\)-th person is male, and \\(W_i\\) be the probability that the \\(i\\)-th person is female, for \\(i=1,2\\). To compute the probability that both of the selected people are women, the above formula can be used as follows. \\[\\begin{aligned} P(W_1 \\cap W_2)&amp;=P(W_1)P(W_2|W_1)\\\\ &amp;= \\frac{\\text{number of women}}{\\text{total number of people}}\\times\\frac{\\text{number of women left}}{\\text{total number of people left}}\\\\ &amp;=\\frac{6}{10}\\times\\frac{5}{9}\\\\ &amp;=\\frac{1}{3}. \\end{aligned}\\] Therefore, the probability of selecting two women without replacement is \\(1/3\\). To compute the probability that one person selected is male and the other is female, the above formula can be used as follows. \\[\\begin{aligned} P(W_1 \\cap M_2) + P(Μ_1 \\cap W_2) =&amp;P(W_1)P(M_2|W_1)+P(M_1)P(W_2|M_1)\\\\ =&amp; \\frac{\\text{number of women}}{\\text{total number of people}}\\times\\frac{\\text{number of men}}{\\text{total number of people left}} + \\\\ &amp; \\frac{\\text{number of men}}{\\text{total number of people}}\\times\\frac{\\text{number of women}}{\\text{total number of people left}}\\\\ =&amp;\\frac{6}{10}\\times\\frac{4}{9}+\\frac{4}{10}\\times\\frac{6}{9}\\\\ =&amp;\\frac{24}{45}. \\end{aligned}\\] Exercise: Following on from the example above, what would the probability be of selecting 2 men from the island without replacement? 5.4.2 Law of total probability Let \\(B_1, B_2, \\cdots, B_k\\) be a set of mutually exclusive and exhaustive events, then an event \\(A\\) can be represented as follows. \\[A = A \\cap S = (A\\cap B_1)\\cup(A\\cap B_2)\\cup\\cdots\\cup(A\\cap B_k).\\] Given that \\((A\\cap B_1),(A\\cap B_2),\\cdots,(A\\cap B_k)\\) are then mutually exclusive events, Axiom 3 leads to the total probability formula as follows. \\[ \\begin{aligned} P(A)&amp;= P(A\\cap B_1)+ P(A\\cap B_2) + \\cdots + P(A\\cap B_k)\\\\ &amp;= P(B_1)P(A|B_1)+ P(B_2)P(A|B_2)+\\cdots+ P(B_k)P(A|B_k). \\end{aligned} \\] (#fig:total prob)Venn diagram for law of total probability Example: Assume that there are 3 regions in a country, regions A, B and C, with area proportions 50%, 20% and 30% respectively. Also assume in the year 20X5, respectively, 68%, 83% and 74% of the population will be vaccinated. This information is given in a table below. Region Area Proportion Percentage Vaccinated A 50% 68% B 20% 83% C 30% 74% What is the probability that someone is vaccinated in the year 20X5? \\[ \\begin{aligned} P(\\text{Vaccinated}) &amp;= P(A)P(\\text{Vaccinated}|A)+P(B)P(\\text{Vaccinated}|B)+P(C)P(\\text{Vaccinated}|C)\\\\ &amp;=0.50\\times 0.68 + 0.20\\times 0.83 + 0.30\\times 0.74 \\\\ &amp;= 0.728 \\end{aligned} \\] Therefore, in the country in the year 2X25, the probability that someone will be vaccinated is 72.8%. To work out the probability that someone is from region A given that they are vaccinated, Bayes’ theorem can be used. This will be covered in Module 6. 5.5 Contingency tables Contingency tables are similar to joint probability tables, but instead of showing the likelihoods of the different events and combinations, they display the bivariate frequencies of the discrete (categorical) data. In other words, instead of showing the probability of two events occurring at the same time, they show the frequency of the two events occurring at the same time. These tables are also known as frequency tables, two-way tables and cross tabs. Similarly to the joint probability tables, the marginal totals are found through summing either the rows or columns dependent of the variable of interest. For example, the marginal total of event \\(X=1\\) occurring in the figure below is equal to \\(a+c\\). Figure 5.5: Example of a contingency table These tables are useful for identifying which events and combinations occur most often, in addition to identifying how many events occurred in total. However, they can also be used to find the probabilities of the different events and combinations occurring. In general, to compute the probability, an adapted version of the experimental probability formula is used, given as follows. \\[P(\\text{Event}) = \\frac{\\text{Frequency of event}}{\\text{Overall total}}\\] This formula can be adapted to compute either the joint, marginal or conditional probabilities as follows. Joint probability: \\[P(\\text{row i, column j})=P(\\text{row i} \\cap \\text{column j}) = \\frac{\\text{joint total cell (i,j)}}{\\text{overall total}}\\] For example the joint probability of \\(X=0\\) and \\(Y=1\\) \\[P(X=0 \\cap Y=1) = \\frac{b}{a+b+c+d}\\] Marginal probability: \\[P(\\text{row i})= \\frac{\\text{marginal total for row i}}{\\text{overall total}}\\] or \\[P(\\text{column j})= \\frac{\\text{marginal total for column j}}{\\text{overall total}}\\] -For example, the marginal probability of \\(Y=0\\) would be \\[P(Y=1) = \\frac{a+b}{a+b+c+d}\\] Conditional probability: \\[P(A|B)= \\frac{P(A\\cap B)}{P(B)}=\\frac{\\text{joint probability of A and B}}{\\text{marginal probability of B}}\\] -For example, the conditional probability of \\(X=0\\) given \\(Y=1\\) would be \\[P(X=0|Y=1) = \\frac{P(X=0 \\cap Y=1)}{P(Y=1)} = \\frac{b}{a+b}\\] 5.6 Useful resources Theoretical vs experimental probability: Theoretical vs. Experimental Probability|Differences &amp; Examples Axioms of probability: Introduction to Probability, Statistics and Random Processes Joint probability: Joint Probability: Definition, Formula, and Example Conditional probability: Conditional Probability: Formula and Real-Life Examples Law of Total Probability: Introduction to Probability, Statistics and Random Processes "],["bayesian-statistical-inference.html", "6 Bayesian Statistical Inference 6.1 Introduction 6.2 Bayes’ theorem 6.3 The likelihood function 6.4 Prior distribution 6.5 Overview of posterior inference 6.6 Elicitation of priors 6.7 Sampling and evaluating the posterior density 6.8 Useful resources", " 6 Bayesian Statistical Inference This module covers the basics to more in-depth Bayesian statistical inference, including Bayes’ rule (also called Bayes’ theorem), likelihoods, the prior distribution and the posterior distribution. 6.1 Introduction In the previous modules, the focus has been on the frequentist (or classical) approach, wherein all the values of the unknown parameter \\(\\theta\\) in the parameter space \\(\\Theta\\) are treated as if they are equally important and whilst the true value of \\(\\theta\\) is unknown, it is considered to be constant. In this paradigm, the only information on \\(\\theta\\) available comes from the data \\(X_1, \\cdots, X_n\\). The alternative approach that will now be the focus is Bayesian inference where it is assumed that there is prior knowledge available through use of subject probability statements. In this approach, it is assumed that \\(\\theta\\) is a random variable and that there is some knowledge or information about \\(\\theta\\) (for example, some values of \\(\\theta\\) given in the parameter space \\(\\Theta\\) might be more likely to be the true value of \\(\\theta\\)) prior to observing the data \\(D_n = {X_1, \\cdots, X_n}\\). The information about \\(\\theta\\) that is contained in the data is then combined with the prior knowledge about \\(\\theta\\), where the combined information is then the current total information about \\(\\theta\\). The three main components to Bayesian inference are then as follows. The prior distribution, \\(p(\\theta)\\), expresses any existing beliefs about the parameter \\(\\theta\\) before any information on the observed data is available. The prior distribution indicates the probability that each value in the parameter space \\(\\Theta\\) is the true value of \\(\\theta\\). The joint probability density, \\(p(\\textbf{x}|\\theta) = p(D_n|\\theta)\\), is chosen which represents the conditional density of X conditioned on the parameter \\(\\theta\\). The normalising constant (or proportionality constant), also known as the evidence, is then given as \\[p(D_n)=\\int_{\\theta \\in \\Theta}p(\\theta)p(D_n|\\theta)d\\theta \\] for a given observed dataset \\(D_n = \\{X_1, \\cdots, X_n\\}\\). These components are then used to update the existing beliefs and calculate the posterior distribution \\(p(\\theta|D_n)\\), where the posterior distribution is the conditional distribution of \\(\\theta\\) given the observed data. It is important to distinguish between the probability of events and the probability distribution or density function for a random variable. In this manual, this difference is distinguished with notation where \\(P(.)\\) is used to denote the probability of an event and \\(p(.)\\) is used to denote the probability distribution or density function. 6.2 Bayes’ theorem Bayes’ theorem (also known as Bayes’ rule) is a mathematical formula that describes the likelihood of an event occurring based on prior knowledge of a previous outcome in similar circumstances. The most basic form of Bayes’ theorem is for events A and B. Let \\(A\\) be an event and let \\(B_1, B_2, \\cdots, B_k\\) be a set of mutually exclusive and exhaustive events (for clarification on mutually exclusive and exhaustive events, see Module 5). Then for \\(i= 1, \\cdots, k\\), Bayes’ theorem is given as follows. \\[P(B_i|A) = \\frac{P(B_i \\cap A)}{P(A)} = \\frac{P(B_i)P(A|B_i)}{P(A)}.\\] where \\(P(A) = \\sum_{j=1}^kP(B_j)P(A|B_j)\\) is the law of total probability (seen in Module 5). This theorem can also be thought of as updating the probability of \\(B_i\\) occurring using the information about another event (\\(A\\)) that has already occurred. Example: Following on from the last example given in Module 5, where it was assumed that there are 3 regions in a country, regions A, B and C, with area proportions 50%, 20% and 30% respectively. Also, in the year 20X5, respectively, 68%, 83% and 74% of the population will be vaccinated. This information is given in a table below. Region Area Proportion Percentage Vaccinated A 50% 68% B 20% 83% C 30% 74% Bayes’ theorem can be used to work out the probability that someone is from region A given that they are vaccinated. In Module 5, using the law of total probability, it was found that the probability of someone being vaccinated was \\(P(Vaccinated)=0.728\\), use this value in the formula for Bayes’ theorem. \\[P(A|Vaccinated) = \\frac{P(A)P(Vaccinated|A)}{P(Vaccinated)} = \\frac{0.5 \\times 0.68}{0.728} = 0.467\\] Therefore, the probability of someone who is vaccinated being from region A is 46.7%. Exercise: Following on from the example above, what is the probability that someone who is vaccinated is from region C? Note: For another example of Bayes’ theorem, see Spatial and Spatio-temporal Bayesian Models with R-INLA, page 57. 6.2.1 Bayes’ theorem for random variables When it is assumed that \\(\\theta\\) is a random variable, Bayes’ theorem can be adjusted to allow for the computation of the posterior distribution. Consider the model parameters \\(\\theta\\) and a given dataset \\(D_n = \\{X_1, \\cdots, X_n\\}\\), where \\(\\theta\\) is random and \\(D_n\\) is fixed. The posterior distribution can then be written as follows. \\[p(\\theta| D_n) = \\frac{p(\\theta)p(D_n|\\theta)}{p(D_n)}, \\] where the proportionality constant is given as \\[ p(D_n) = \\begin{cases} \\sum_{\\theta \\in \\Theta}p(\\theta)p(D_n|\\theta) &amp; \\text{ if } \\theta \\text{ is discrete,}\\\\ \\int_{\\theta \\in \\Theta}p(\\theta)p(D_n|\\theta)d\\theta &amp; \\text{ if } \\theta \\text{ is continuous}. \\end{cases} \\] In Bayes’ theorem, \\(p(D_n)\\) does not depend on \\(\\theta\\), only \\(X\\) given that it is a proportionality constant, and so is often written as \\[p(\\theta| D_n) \\propto p(\\theta)p(D_n|\\theta),\\] therefore, \\[\\text{posterior} \\propto \\text{prior}\\times \\text{likelihood},\\] where \\(\\propto\\) means proportional to. More information on likelihoods, prior and posterior probabilities is given in the next section. 6.3 The likelihood function The likelihood function is introduced in Module 4 with likelihood ratio testing for model selection. However, this section will go into more detail of what the likelihood function actually is and how it is related to the data. The likelihood function is a function of the parameters statistical model and given by the joint probability mass function (PMF) of discrete observed data or the joint probability density function (PDF) of continuous data. It is important to note that the likelihood function is different from a PMF or PDF, and is instead characterised by the joint distribution of the observed variables. The likelihood function of \\(\\theta\\) is defined as \\(L(\\theta) = L(\\theta|x) = p(\\textbf{x}|\\theta)\\), evaluated at \\(x\\) and considered a function of \\(\\theta\\). It is also an important component to both frequentist (for example the LRT for model selection) and Bayesian approaches. The likelihood function is calculated by finding the joint distribution of the observed variables, found through finding the product of the PMF/PDF as follows. \\[L(\\theta) = \\prod_{i=1}^n p(x_i;\\theta)= p(x_1;\\theta) \\times p(x_2;\\theta) \\times \\cdots \\times p(x_n;\\theta),\\] where \\(p(x;\\theta)\\) is the PMF if each \\(X_i\\) has discrete distribution and \\(p(x;\\theta)\\) is the PDF if each \\(X_i\\) has continuous distribution. Given that the observations \\(x_1,...,x_n\\) are known since they are observed, the likelihood function can be described as a function of the unknown \\(\\theta\\). It is common to need to maximise the likelihood function, for example, to find the maximum likelihood estimate (the estimated value of the parameter of interest that is most likely to be the true value), however, it can be challenging to maximise a product. Therefore, the log-likelihood is often used, since it is typically easier to maximise a sum than a product. The log-likelihood is given as follows. \\[\\log L(\\theta) = \\ell (\\theta) = \\sum_{i=1}^n \\log p(x_i;\\theta). \\] The log-likelihood of a model in R can be found through using the logLik() function with the model of interest as an argument. This is demonstrated in Module 4 for use in the likelihood ratio test. 6.3.1 Different types and structures The likelihood function varies dependent on the data type and structure, as it depends on the distribution of the data itself. To choose which likelihood function to use, it is important to look at the data itself and check for the data type and assumptions, using statistical tests or plots such as box plots and histograms for model assumption checking. For example, starting by looking at whether the data is discrete or continuous. If the data is discrete, look at the properties of the data. Are there many outcomes or only two outcomes? Are there repeated trials or only one trial conducted? If the data is continuous, it is important to look at whether the data is strictly positive or whether there are negative values. The main data types are given below, with examples for possible distributions given for each data type to help identify which likelihood function to use and when. 6.3.1.1 Count data For count data, the most common distributions are binomial, Poisson, negative-binomial and geometric. Given that count data is discrete, the probability mass function of each of the distributions is used in the construction of the likelihood function. An important note for count data is that the counts are always non-negative, \\(X=\\{0,1,2,\\cdots\\}\\). The binomial distribution is used in the case where there are only two possible outcomes and the data is collected over a series of repeated trials. In this distribution, the two outcomes are labelled as either a success or a failure, where the focus is on the number of successes in \\(n\\) trials. An example of when this distribution is used is when the data contains information on a series 100 births, there are two possible outcomes, male or female. The associated probability mass function is given as \\[X \\sim binomial(n, p), \\text{ } p(x;n,p) = {}^nC_x p^x(1-p)^{n-x}.\\] For more information on the binomial distribution, see Investopedia. The Poisson distribution is used to identify how likely a given count of times an event is to occur within/over a specified period of time. The corresponding probability mass function is given as follows. \\[ X \\sim Poisson(\\theta),\\text { } p(x;\\theta) = \\frac{\\theta^x \\exp(-\\theta)}{x!}.\\] For more information on the Poisson distribution, see Investopedia. An important assumption of the Poisson distribution is that the mean is equal to the variance. If this assumption cannot be met, often the negative-binomial distribution is used as an alternative to the Poisson distribution. It has an additional (dispersion) parameter, which allows for more flexibility than the Poisson distribution. The focus of the negative-binomial distribution is on the number of failures before the \\(r\\)th success, where the associated probability mass function is given as follows. \\[X \\sim negbin(r,p), \\text{ } p(x;r,p) = \\frac{\\Gamma(x+r)}{x!\\Gamma(r)}p^r(1-p)^{x}\\] For more information on the negative-binomial distribution, see Wolfram. The geometric distribution is a special case of the negative-binomial distribution where the focus is on the number of failures before the first success, with the corresponding probability mass function given as follows. \\[ X \\sim geometric(p),\\text { } p(x;p) = p(1-p)^{x-1}.\\] For more information on the geometric distribution, see Britannica. 6.3.1.2 Binary data The most common distribution for binary data is the Bernoulli distribution. It is similar to the binomial distribution in that there are only two possible outcomes, however, instead of repeated trials, only one trial is conducted. For a Bernoulli trial, outcomes are either labelled as \\(k=0\\) indicating a failure or \\(k=1\\) indicating a success. The probability mass function of the Bernoulli distribution is given as follows. For more information on the Bernoulli distribution, see Wolfram. 6.3.1.3 Proportional data For proportions, the beta-binomial distributions is the most commonly used distribution type. In this discrete distribution, the probability \\(p\\) for a binomial distribution is chosen from a beta distribution, leading to the number of successes being a beta-binomial random variable. The corresponding probability mass function is given as follows. \\[X \\sim beta-binomial(n, \\alpha, \\beta), \\text{ } {}^nC_x \\frac{B(x+\\alpha, n-x+\\beta)}{B(\\alpha, \\beta)},\\] where \\(\\alpha\\) and \\(\\beta\\) are shape parameters, \\(n\\) is the number of trials and \\(B\\) is the Beta function given as \\[ \\begin{aligned} B(\\alpha, \\beta) &amp;= \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)} \\\\ &amp;= \\frac{(\\alpha-1)!(\\beta-1)!}{(\\alpha+\\beta-1)!}, \\end{aligned} \\] For more information on the beta-binomial distribution see Buffalo. 6.3.1.4 Continuous data Unlike with the discrete distributions where the probability of an exact outcome can be computed, for a continuous distribution, the focus is on computing the probability for a range of outcomes. The most common distributions for continuous data are the normal, log-normal, gamma and log-logistic distributions. The normal distribution relies on symmetry around the mean, with its ‘bell curve’ shape when plotted well known. This distribution is used when there is a high frequency of data points near the mean with few data points further away from the mean. It is important to note that for the normal distribution, the data can be negative, so is not suitable for data that is strictly non-negative. To test the relevant assumptions associated with the normal distribution, plots such as histograms can be used. This is covered in Module 4. The probability density function for the normal distribution is given as follows. \\[X \\sim normal(\\mu, \\sigma^2), \\text{ } p(x;\\mu, \\sigma)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right).\\] For more information on the normal distribution see Investopedia. As mentioned in Module 4, if the assumptions are not met, the data can be transformed, for example using the logarithm function. If the logarithm of the random variable is normally distributed, then the log-normal distribution may be used. In this case, if X is log-normally distributed then \\(Y=\\log(X)\\). The corresponding probability density function is given as \\[X \\sim lognormal(\\mu, \\sigma^2), \\text{ } p(x;\\mu, \\sigma)=\\frac{1}{x\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{(\\log(x)-\\mu)^2}{2\\sigma^2}\\right).\\] For more information on the log-normal distribution see Investopedia. Information on alternative continuous distributions is given here Knime. 6.4 Prior distribution As mentioned in the introduction, the prior distribution \\(p(\\theta)\\), is a probability distribution that expresses beliefs about the parameter \\(\\theta\\) that exist before, or prior to, conducting the experiment. There are different types of prior distributions, and the choice of which type is important. There are a few main things to consider when choosing the prior distribution. Firstly, as with choosing the likelihood function, the type of distribution should be considered as the prior distribution chosen needs to be representative of the parameters. There is often an instinctive choice for the prior distribution given the type of distribution. Some examples are given below. If the parameter of the model for the underlying system is continuous and symmetric, then the prior distribution chosen should be able to vary between either \\(-\\infty\\) and \\(\\infty\\) or \\(0\\) and \\(\\infty\\). If the parameter of the model for the underlying system is continuous and positive, then the prior distribution chosen should be able to vary only between \\(0\\) and \\(\\infty\\). If the parameter of the model for the underlying system is a proportion, then the prior distribution chosen should be able to vary between \\(0\\) and \\(1\\). Secondly, the hyperparameters should be considered as they can make the prior distribution either more or less informative, where hyperparameters are the parameters of the prior distribution (this terminology is used to help distinguish between the parameters of the model for the underlying system and the parameters for the prior distribution). Lastly, you should always check the sensitivity and robustness of the posterior distribution to different choices of priors. 6.4.1 Conjugate priors In some cases, the posterior distribution can be solved through using a closed-form solution, meaning that there is not a need for more complex approximation methods. In most of these cases, the posterior density belongs to the same (parametric) family as the prior distribution. If this is the case, then the prior distribution, \\(p(\\theta)\\), is called a conjugate prior of \\(\\theta\\) under the likelihood \\(p(D_n|\\theta)\\). Some examples of conjugate prior distributions are given below. Likelihood Conjugate Prior Binomial Beta Poisson Gamma Normal Normal Exponential Gamma It is important to note that the conjugate models have limited flexibility, for example, conjugacy can be broken when a GLM is specified, or not all of the likelihoods available have an associated conjugate prior. As a result of this, these models are not frequently used in practice. To demonstrate what a conjugate prior means, the Poisson-gamma model will be used. The Poisson distribution can only have non-negative integer values. Since the parameter of a Poisson distribution can take any positive real number, the prior distribution chosen should be able to vary between \\(0\\) and \\(\\infty\\). In this case, since there is no upper bound, although a uniform distribution can be chosen such that it is non-negative, an upper bound must be set, for example, \\(X \\sim Uniform(0, 1000)\\). Therefore, the uniform distribution would not be an appropriate choice for prior distribution. Instead, a gamma distribution is a good choice, since the ‘tail’ of the distribution goes to infinity and the ‘peak’ of the distribution is close to zero, which are similar characteristics to the Poisson distribution. This is demonstrated in the plots below. For more information see StatLect. 6.4.2 Flat, improper and non-informative priors One of the ways of selecting the prior is to use subjectivism. This method follows the belief that the prior should reflect the subjective opinion of \\(\\theta\\) (subjective beliefs about \\(\\theta\\) before the data is collected). However, this approach is not only not possible in many situations, it is also not appropriate in many cases, especially since it is often a goal to keep the inference objective. This is typically when a non-informative prior is used instead. In situations where there isn’t a preference for any given value in \\(\\Theta\\), meaning that the values in \\(\\Theta\\) are equally likely to be the true value of \\(\\theta\\), there is a lack of information about \\(\\theta\\). A prior distribution that reflects this is called a non-informative prior. One type of non-informative prior is a flat prior, where \\(p(\\theta) \\propto k\\), where \\(k\\) is a constant, for example, \\(p(\\theta)=1\\). Given the relationship between the prior, likelihood and the posterior distribution, when the prior is constant, the posterior becomes some fraction of the likelihood. This means that the posterior is then only affected by the likelihood function. A probability density function should integrate to 1, \\(\\int_{-\\infty}^\\infty p(\\theta)d\\theta=1\\) and a probability mass function should sum to 1 \\(\\sum_{-\\infty}^{\\infty} p(\\theta)=1\\). However, when the prior distribution is constant, this basic property of the PDF or PMF is violated. For example, if the data is continuous and \\(p(\\theta)=k\\) for all \\(-\\infty &lt; \\theta &lt; \\infty\\), then the integral \\[\\int_{-\\infty}^\\infty p(\\theta)d\\theta = k \\int_{-\\infty}^\\infty d\\theta\\] does not exist, no matter how small \\(k\\) is since \\(k&gt;0\\). When this is the case, it is called an improper prior, and the distribution can only be assumed if the resulting posterior distribution is proper, \\(\\int_{-\\infty}^\\infty p(\\theta|D_n)d\\theta=&lt; \\infty\\). If \\(p(\\theta)=k\\) for values of \\(\\theta\\) where the likelihood function has appreciable value (a value that is not insignificant) and \\(p(\\theta)=0\\) otherwise, then the prior distribution is called a locally uniform prior. One of the most commonly used non-informative priors is that of Jeffreys’ prior, given by the square root of the Fisher information matrix as follows. \\[p(\\theta) \\propto \\sqrt{I(\\theta))},\\] where \\[I(\\theta) = -E\\left[\\frac{\\partial^2}{\\partial \\theta^2}\\log(p(\\theta|D_n))\\right].\\] For example, the Jeffreys’ prior for \\(\\theta\\) when a binomial distribution is assumed is as follows. \\[p(\\theta)\\propto {\\theta(1-\\theta)}^{-1/2}.\\] For more information on non-informative priors and prior selection, see Box and Tiao, Section 1.3. 6.4.3 Informative prior In the instance that there is prior information available, typically from existing results from prior experiments focused on the same area or topic, an informative prior should be used to incorporate this information into the model. For example, when looking at the success rates of a new drug in testing, if previous experiments focused on the success rate of a similar drug have occurred, then this existing information can be used to construct an informative prior distribution. For a more in depth example, an example of this kind is demonstrated in the book Spatial and Spatio-temporal Bayesian Models with R-INLA on page 69. 6.5 Overview of posterior inference As previously mentioned, in Bayesian statistics, the likelihood and prior distribution are combined to get the posterior distribution. Unlike with classical statistics, where simply the mean and standard deviation can be used to make inference on the parameter of interest, in order to make inference from the posterior distribution, samples need to be generated from the posterior distribution. 6.5.1 Markov Chain Monte Carlo approach To generate the samples required, Markov Chain Monte Carlo methods (discussed in more detail in Module 8) can be utilised, where the type of MCMC approach used depends on what the posterior distribution itself looks like. In the closed form case where the posterior distribution looks like a recognisable probability distribution, such as the normal or Poisson distributions, the MCMC approach known as Gibbs sampler can be used. However, a closed form solution is not always available. If this is the case, then other Metropolis-Hastings algorithms are required, such as Random Walk and Independence Sampler. Once the samples have been generated from the posterior distribution, it needs to be checked whether the samples are drawn from the target (or stationary) distribution, which is the distribution that is desired when the likelihood and the prior distribution are multiplied together. To check this, when MCMC approaches are used, a trace plot can be created. Trace plots are useful for assessing whether a chain is well-mixed or not. If the chain is well-mixed, after the burn-in period, the trace plot won’t have any flat sections, which would indicate that the chain remains stationary for too long, and the plot won’t have too many consecutive steps in the same direction. Overall, you do not want the trace plot to show any obvious correlation. To demonstrate what the plot should look like, the function plot.ts() can be used to plot a time series plot with randomly sampled data. Below, 1000 numbers are randomly sampled from a normal distribution with mean 10 and standard deviation 10. The corresponding time series plot shows oscillation around the central value (mean) of 10, with good mixing. #set seed for reproducibility set.seed(123) #randomly sample numbers post1 &lt;- rnorm(1000, 10, 10) #plot the sampled numbers plot.ts(post1) To contrast this well-mixed plot, the data sampled below comes from the same normal distribution but only has 100 values, leading to a plot that is quite sparse. It is a lot harder to identify a central value and as a result, a larger sample size is required. #set seed for reproducibility set.seed(123) #randomly sample numbers post2 &lt;- rnorm(100, 10, 10) #plot the sampled numbers plot.ts(post2) In contrast with the samples taken from a normal distribution, the below (1000) values are sampled from a gamma distribution. Whilst the sample size is large, there is poor convergence, given that the central value changes after the first 500 observations. If this chain was obtained from an MCMC approach, the chain would be poorly mixed. #set seed for reproducibility set.seed(123) #randomly sample numbers post3 &lt;- c(rgamma(500, 10, 2), rgamma(500, 10, 1)) #plot the sampled numbers plot.ts(post3) Another diagnostic measure of the MCMC approach is the use of R-hat values. Ideally, you want all of the chains to be sampling from the same underlying distribution which can be identified by the R-hat values being close to 1, where a value close to 1 indicates convergence. However, if the R-hat values are notably larger than 1.1, there is indication that convergence has not been achieved and there may be an issue. The R-hat value can be found using the Rhat() function from the rstan package. This is demonstrated in the example below where the resulting value is close to 1. #install rstan package install.packages(&quot;rstan&quot;) #load the rstan package library(rstan) #set seed for reproducibility set.seed(123) #sample numbers post4 &lt;- runif(100, 1, 1.2) #compute R-hat statistic Rhat(post4) ## [1] 0.9997899 R-hat values can also be plotted with the mcmc_rhat() function (or as a histogram with the mcmc_rhat_hist() function) from the bayesplot package. These plots are colour-coded by default, clearly visualising the proportion of values within different ranges (\\(\\leq1.05\\), \\(1.05&lt;\\hat{r}\\leq 1.1\\) and \\(&gt;1.1\\)). #install bayesplot package install.packages(&quot;bayesplot&quot;) #load the bayesplot package library(bayesplot) #sample R-hat values rhat &lt;- c(runif(100, 1, 1.2)) #plot the values mcmc_rhat(rhat) mcmc_rhat_hist(rhat) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 6.5.2 Integrated Nested Laplace Approximation approach Integrated Nested Laplace Approximation (INLA) provides a faster, more efficient approach to using MCMC for posterior inference through using approximation. It approximates the posterior distribution with the posterior marginal distribution through using an evolution approach that is based on Laplace approximation. It does not involve sampling and is deterministic, accurate and fast. The corresponding R package, INLA can be used for modelling. For more information on the INLA package, see R-INLA Project. To perform modelling with INLA in R as an alternative approach to the methods discussed in Module 4, the function inla() can be used with the following arguments. formula: a formula object that specifies the linear predictor. data: a data frame with the data. If you wish to predict the response variable for some observations, you need to specify the response variable of these observations as NA. family: a string or vector of strings that indicate the likelihood family such as Gaussian, Poisson or binomial. The default family for this argument is Gaussian. A list of alternatives can be seen by running the code names(inla.models()$likelihood) and details for individual families can be seen with the code inla.doc(\"familyname\"). control.compute: a list with the specification of several computing variables such as dic, which is a Boolean variable indicating whether the Deviance information criterion (DIC, explored further in Module 9) of the model should be computed. control.predictor: a list with the specification of several predictor variables such as a link which is the link function of the model, and compute which is a Boolean variable that indicates whether the marginal densities for the linear predictor should be computed. It is important to note that the formula should be specified first, outside the function, and specified in the form formula &lt;- response ∼ x1 + x2 + ... xM. #install the INLA package from the code given on the website install.packages(&quot;INLA&quot;,repos=c(getOption(&quot;repos&quot;),INLA=&quot;https://inla.r-inla-download.org/R/stable&quot;), dep=TRUE) #template for using inla() function formula &lt;- response ~ x1 + x2 + ... xM #the nominal form of the model res &lt;- inla(formula, data = data.frame(data), #your data frame family = &quot;poisson&quot;, #the probability distribution of the response control.predictor = list(compute = TRUE), control.compute = list(dic = TRUE) + ) summary(res) #produces summaries of the posterior parameter estimates To demonstrate the posterior estimation, (standardised) population data from Cameroon is used. Information on how to standardise the dataset first is given in Module 8, however, for now the focus is on the modelling. Firstly, the dataset needs to be imported into the R environment. #input the data Data_CMR &lt;- read.csv(paste0(data_path, &quot;Data_CMR_std.csv&quot;)) #load the variable names file var_names &lt;- read.csv(paste0(data_path, &quot;var_names.csv&quot;)) y ~ x2 + x16 + x20 + x24 + x31 + x36 + x40 As an example, if the model \\(\\log(\\lambda) = intercept + \\beta_1*x_2 + \\beta_2*x_{16} + \\beta_3*x_{20} + \\beta_4*x_{24} + \\beta_5*x_{36} + \\beta_6x_{40}\\) is the chosen model, the inla() function can be used to fit this model as follows. #nominal form of the model density &lt;- (Data_CMR$Total_Pop/Data_CMR$Total_Building_Count) form_pop1a &lt;- density ~ x2 + x16 + x20 + x24 + x31 + x36 + x40 mod1 &lt;- inla(form_pop1a, data = data.frame(Data_CMR), family = &quot;gamma&quot;, #the probability distribution of the #response variable control.predictor = list(compute = TRUE), control.compute = list(dic = TRUE, waic = TRUE, cpo = TRUE, config = TRUE) #necessary to to draw #samples from the posterior ) #summary of the INLA model summary(mod1) ## Time used: ## Pre = 0.909, Running = 1.35, Post = 0.185, Total = 2.45 ## Fixed effects: ## mean sd 0.025quant 0.5quant 0.975quant mode kld ## (Intercept) 1.479 0.020 1.440 1.479 1.518 1.479 0 ## x2 -0.197 0.024 -0.243 -0.197 -0.150 -0.197 0 ## x16 -0.204 0.020 -0.244 -0.204 -0.165 -0.204 0 ## x20 0.299 0.026 0.248 0.299 0.351 0.299 0 ## x24 0.119 0.020 0.081 0.119 0.158 0.119 0 ## x31 -0.003 0.023 -0.048 -0.003 0.041 -0.003 0 ## x36 -0.359 0.024 -0.405 -0.359 -0.312 -0.359 0 ## x40 -0.067 0.027 -0.119 -0.067 -0.015 -0.067 0 ## ## Model hyperparameters: ## mean sd 0.025quant 0.5quant 0.975quant mode ## Precision-parameter for the Gamma observations 1.58 0.051 1.48 1.58 1.68 1.58 ## ## Deviance Information Criterion (DIC) ...............: 7275.40 ## Deviance Information Criterion (DIC, saturated) ....: 1340.67 ## Effective number of parameters .....................: -209.98 ## ## Watanabe-Akaike information criterion (WAIC) ...: 8268.99 ## Effective number of parameters .................: 397.68 ## ## Marginal log-Likelihood: -3908.32 ## CPO, PIT is computed ## Posterior summaries for the linear predictor and the fitted values are computed ## (Posterior marginals needs also &#39;control.compute=list(return.marginals.predictor=TRUE)&#39;) Once the chosen model has been fitted, the desired information can be extracted. Firstly, to obtain the fixed effects estimates, the argument summary.fixed can be extracted from the model as follows. #obtain fixed effects estimates round(mod1$summary.fixed, 4) ## mean sd 0.025quant 0.5quant 0.975quant mode kld ## (Intercept) 1.4788 0.0200 1.4396 1.4788 1.5180 1.4788 0 ## x2 -0.1968 0.0237 -0.2433 -0.1968 -0.1503 -0.1968 0 ## x16 -0.2042 0.0202 -0.2437 -0.2042 -0.1646 -0.2042 0 ## x20 0.2994 0.0261 0.2481 0.2994 0.3507 0.2994 0 ## x24 0.1192 0.0197 0.0805 0.1192 0.1579 0.1192 0 ## x31 -0.0031 0.0227 -0.0476 -0.0031 0.0414 -0.0031 0 ## x36 -0.3588 0.0236 -0.4051 -0.3588 -0.3124 -0.3588 0 ## x40 -0.0669 0.0267 -0.1192 -0.0669 -0.0146 -0.0669 0 To obtain the predicted values, the argument summary.fitted.values can be extracted from the model. There are different predicted values available however within this argument, so for completeness, it is important to extract the predicted means with the mean argument, as well as the upper and lower bounds of the 95% credible intervals with the 0.025quant and 0.975quant arguments respectively. It is important to exponentiate the results given that the logarithm transformed population was used in the model. #obtain the predicted values pred1 &lt;- mod1$summary.fitted.values$mean #predicted means pred1L &lt;- mod1$summary.fitted.values$`0.025quant` #lower bounds of 95% #credible intervals pred1U &lt;- mod1$summary.fitted.values$`0.975quant` #upper bounds of 95% #credible intervals Arguably the most important part of this estimation is the posterior estimates, which can be obtained through extracting summary.linear.predictor$mean from the model, and exponentiating the results given that the logarithm transformed population was used in the model. #obtain posterior estimates post_est &lt;- exp(mod1$summary.linear.predictor$mean) The predicted values can then be compared to the posterior estimates, where it can be seen that they are almost identical. #comparing predicted values to posterior estimates head(cbind(pred1, post_est)) #almost identical ## pred1 post_est ## [1,] 1.781561 1.765761 ## [2,] 3.192312 3.176338 ## [3,] 7.618386 7.603349 ## [4,] 6.947705 6.915962 ## [5,] 4.095951 4.093035 ## [6,] 3.868976 3.866658 6.6 Elicitation of priors There is a lot of flexibility when choosing a prior, but there are some things that should be kept in mind that should not be done. Firstly, the prior should not be chosen based on the observed data. Instead, the prior should be chosen to reflect the uncertainty of the parameters prior to observing the data. It is classed as “data snooping” or “p-hacking” when the prior is chosen based on the observed data in order to elicit a desired result. Additionally, if a prior assigns 0 probability to the values of the parameter, this prior should not be chosen. It is important to consider the range of possible values of the parameter, as if the prior density is 0 over the range of values of the parameter, no amount of data can turn that probability into a positive posterior probability. Therefore, always check that the prior density is non-zero over the range of parameter values. Lastly, do not focus solely on finding the “perfect” prior. The prior is an assumption of the model and as with many aspects of statistics, assumptions are rarely perfectly satisfied. The focus instead should be to find a prior which results in a suitable model for which the assumptions are reasonably satisfied. It should also be kept in mind that if you feel that a singular prior is not suitable enough, more than one model can be considered, where each model has a different prior, and the average of the chosen models used for computing the results. 6.6.1 Priors INLA In R-INLA, the function inla.models()$prior returns a list with the names of each of the available priors. If this code is used in the names() function as names(inla.models()$prior), then the names of the priors available will be returned. If you wish to see documentation regarding a specific prior, then the function inla.doc(\"priorname\") can be used. By default, the intercept of the model is assigned a Gaussian prior with mean and precision equal to 0. The rest of the fixed effects are assigned Gaussian priors with mean equal to 0 and precision equal to 0.001. These values can be seen with the code inla.set.control.fixed.default()[c(\"mean.intercept\", \"prec.intercept\", \"mean\", \"prec\")]. The values of these priors can be changed in the control.fixed argument of the function inla() by assigning a list with the mean and precision of the Gaussian distributions. Specifically, the list contains mean.intercept and prec.intercept which represent the prior mean and precision for the intercept respectively, as well as mean and prec which represent the prior mean and precision for all fixed effects except the intercept respectively. The below code is a template of how to change these values in the inla() function. #template for inla with prior fixed prior.fixed &lt;- list(mean.intercept = &lt;&gt;, prec.intercept = &lt;&gt;, mean = &lt;&gt;, prec = &lt;&gt;) res &lt;- inla(formula, data = d, control.fixed = prior.fixed) Alternatively, to specify a prior on the hyperparameters of the latent effects, a list that defines the prior can be passed through the hyper parameter within the f() function, where the list contains the following arguments. prior: the name of the given prior distribution param: the values of the parameters for the given prior distribution initial: (optional) the initial value(s) of the hyperparameters. If no initial argument is given, the initial value(s) will be set to their default value(s). fixed: (optional) a logical argument defining whether the initial value(s) are kept fixed or allowed to change. If no fixed argument is given, it will be set to FALSE by default. In the example below, a gamma prior is defined with parameter values of 0.01 and 0.01. Here, the arguments of initial and fixed are also included, but are set to their default values. #example of gamma prior prec.prior1 &lt;- list(prec = list(prior = &quot;loggamma&quot;, param = c(0.01, 0.01)), initial = 4, fixed = FALSE) formula1 &lt;- y ~ 1 + f(set_type, model = &quot;iid&quot;, hyper = prec.prior1) loggamma is just one example of the different priors that can be implemented in INLA. For a comprehensive list of the different priors, run the code names(inla.models()$prior). If you wish to have more detailed information on a specific prior from this list, the code inla.doc(\"prior\") can be used, where \"prior\" is the prior you wish to have more information on, for example, use inla.doc(\"loggamma\") for more information on the gamma prior. 6.6.1.1 Penalised Complexity priors Penalised Complexity priors or PC priors were designed to suit additive models that are defined by different components, and get their name from the fact that the priors penalise any departure from the base model. To follow parsimony (preferring the simplest model that still fits the data well), the base model is always preferred for PC priors, as long as there is no evidence provided against this base model. As a result of this, they improve the predictive performance and can reduce over-fitting. For more information on PC priors, with a more in-depth explanation, see Bayesian inference with INLA. In the example code seen below, a penalised complexity prior is defined through specifying prior = \"pc.prec\", with parameters 1 and 0.01. To change the prior, the values of the parameters in the param argument simply need to be changed. #example of PC prior prec.prior2 &lt;- list(prec = list(prior = &quot;pc.prec&quot;, param = c(1, 0.01))) formula2 &lt;- y ~ f(set_type, model = &quot;iid&quot;, hyper = prec.prior2) To demonstrate this methodology, an example from Geospatial Health Data is used, where data from 12 hospitals on surgical mortality rates is modelled in order to assess the performance of each of the hospitals. Here, the focus is on the code, however, to see more information on the modelling process itself, see Chapter 4 in the book itself. In this dataset, n refers to the total number of operations undertook within a one-year period for each hospital, r refers to the number of deaths within a 30-day period of surgery for each hospital and hospital refers to each of the hospitals in the data. #create surgery dataset Surg &lt;- data.frame(n = c(47, 148, 119, 810, 211, 196, 148, 215, 207, 97, 256, 360), r = c(0, 18, 8, 46, 8, 13, 9, 31, 14, 8, 29, 24), hospital = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;, &quot;F&quot;, &quot;G&quot;, &quot;H&quot;, &quot;I&quot;, &quot;J&quot;, &quot;K&quot;, &quot;L&quot;)) Below, a Penalised Complexity prior is set by selecting prior = \"pc.prec\" and specifying that the probability of the standard deviation being greater than 1 is equal to 0.01 by setting the parameters to param = c(1, 0.01). #set PC prior for surgery dataset prior.prec &lt;- list(prec = list(prior = &quot;pc.prec&quot;, param = c(1, 0.01))) Once the prior is set, it can be used in the model formula, input into the hyper argument, before calling the inla() function and specifying the given formula. In the inla() function, to compute the posterior marginals of the parameters, control.predictor = list(compute = TRUE) is included as an argument. Similarly, for model comparison purposes, to compute the DIC, the argument control.compute = list(dic = TRUE) can be added. Finally, to compute the marginals, the logical statement return.marginals.predictor = TRUE is included in the list for the control.compute argument as control.compute = list(dic = TRUE, return.marginals.predictor = TRUE). #create INLA model for PC prior formula &lt;- r ~ f(hospital, model = &quot;iid&quot;, hyper = prior.prec) surg.mod &lt;- inla(formula, data = Surg, family = &quot;binomial&quot;, Ntrials = n, control.predictor = list(compute = TRUE), control.compute = list(dic = TRUE, return.marginals.predictor = TRUE, config = TRUE) ) As with standard modelling in R, the summary() function can be used to obtain the results from an INLA model as follows. #summary for INLA model summary(surg.mod) ## Time used: ## Pre = 0.386, Running = 0.311, Post = 0.163, Total = 0.859 ## Fixed effects: ## mean sd 0.025quant 0.5quant 0.975quant mode kld ## (Intercept) -2.547 0.141 -2.841 -2.542 -2.28 -2.543 0 ## ## Random effects: ## Name Model ## hospital IID model ## ## Model hyperparameters: ## mean sd 0.025quant 0.5quant 0.975quant mode ## Precision for hospital 11.41 11.85 2.36 8.26 38.41 5.34 ## ## Deviance Information Criterion (DIC) ...............: 74.47 ## Deviance Information Criterion (DIC, saturated) ....: 24.70 ## Effective number of parameters .....................: 8.08 ## ## Marginal log-Likelihood: -41.16 ## is computed ## Posterior summaries for the linear predictor and the fitted values are computed ## (Posterior marginals needs also &#39;control.compute=list(return.marginals.predictor=TRUE)&#39;) 6.7 Sampling and evaluating the posterior density Once the model has been fitted, the predicted values can be obtained and compared to the observed values with a scatter plot. To obtain the predicted values, the argument summary.fitted.values can be extracted from the model. #obtain predicted values results &lt;- surg.mod$summary.fitted.values The observed values can be added to this results variable as follows. #install tidyverse package install.packages(&quot;tidyverse&quot;) #load the tidyverse package library(tidyverse) #add observed values results &lt;- results %&gt;% mutate(observed = Surg$r/Surg$n) %&gt;% #proportion rename(predicted = mean) %&gt;% #predicted as_tibble() Then the scatter plot comparing the observed and predicted values can be created. For a basic scatter plot with a smooth curve, the function scatter.smooth() from the stats package can be utilised, inputting the observed and predicted values as arguments. #scatter plot with smooth curve scatter.smooth(results$observed, results$predicted) Alternatively, the geom_point() function within the ggplot() function from the ggplot2 package can be utilised as follows (see Module 2 for more help with scatter plots). #scatter plot using ggplot ggplot(data = results, aes(x = observed, y = predicted))+ geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;red&quot;)+ #geom_smooth is used #to draw a line theme_bw()+ labs(title = &quot;Scatter Plot&quot;, x = &quot;Observed&quot;, y = &quot;Predicted&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; To plot histograms and density plots using the posterior results, the first step is to sample from the posterior using the inla.posterior.sample() function. #sample from posterior in model samples &lt;- inla.posterior.sample(n = 100, result = surg.mod, num.threads = &quot;1:1&quot;) To work with the data, the variable containing the sampled data needs to be transposed. #transpose samples and select the first 12 columns samples &lt;- t(sapply(samples, function(x) x$latent)) The first 12 columns of the transposed sampled data correspond to the predicted values for each of the hospitals, and can therefore be subset for a new variable. #first 12 columns are predicted values predicted_values &lt;- samples[, 1:12] The predicted values need to be transformed to be in the correct scale. This is done with the inverse logit function for back transformation, and to make it simpler, a function is first created then applied to the predicted values. #inverse logit function for back transformation inverse_logit &lt;- function(aa) { out = exp(aa)/(1 + exp(aa)) return(out) } #apply function predicted_values &lt;- inverse_logit(predicted_values) Given that the values for each hospital are of interest, to make the results easier to interpret, the columns of the predicted values can be named to correspond with each of the hospital labels. #add hospitals labels to the posteriors hospitals &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;, &quot;F&quot;, &quot;G&quot;, &quot;H&quot;, &quot;I&quot;, &quot;J&quot;, &quot;K&quot;, &quot;L&quot;) #label the columns colnames(predicted_values) &lt;- c(hospitals) Finally, the predicted values variable needs to be converted from a matrix to a data frame so that the values can be plotted as usual. #convert the matrix to a data frame predicted_values_df &lt;- as.data.frame(predicted_values) To visualise the posterior values for each hospital, histograms and density plots are utilised. To demonstrate these methods, only hospital A is plotted at first. For the histogram, ggplot() is used, specifying the data frame for the predicted values and specifying which hospital (A) using aes(x=A), with the additional function geom_hisogram() as follows. #histogram of posterior values for hospital A ggplot(data = predicted_values_df , aes(x = A)) + geom_histogram(bins = 20, color = &quot;#00b4d8&quot;, alpha = 1, fill = &quot;#1d3557&quot;) + theme_bw() + labs(title = &quot;Posterior Predicted values for A&quot;, x = &quot;Posteriors&quot;, y = &quot;Frequency&quot;) The same approach is used for the density plot, but using the additional function geom_density() as follows. #density plot of posterior values for hospital A ggplot(data = predicted_values_df , aes(x = A)) + geom_density(alpha = 1, fill = &quot;#1d3557&quot;) + theme_bw() + labs(title = &quot;Posterior predicted Values for A&quot;, x = &quot;Posteriors&quot;, y = &quot;Density&quot;) Exercise: Create a histogram of the posterior values for hospital D. Exercise: Create a density plot of the posterior values for hospital G. Given that there are 12 hospitals, it can be beneficial for comparison purposes to plot the histograms or density plots for each hospitals side-by-side. To do this within ggplot(), a new variable needs to be specified which contains the the predicted values with the corresponding hospital label in a long table. #long table for predicted values and hospital labels predicted_long &lt;- predicted_values_df %&gt;% pivot_longer(cols = everything(), names_to = &quot;Hospitals&quot;, values_to = &quot;Posteriors&quot;) Then, the function facet_wrap() can be used within the ggplot() function to plot either the histogram or the density plot for each hospital at the same time on the same panel. #histogram for posterior values for each hospital ggplot(data = predicted_long, aes(x = Posteriors)) + geom_histogram(bins = 20, color = &quot;#00b4d8&quot;, alpha = 1, fill = &quot;#1d3557&quot;) + theme_bw() + labs(title = &quot;Posterior Values&quot;, x = &quot;Posteriors&quot;, y = &quot;Frequency&quot;) + facet_wrap( ~ Hospitals, scales = &quot;free&quot;) #density plot for posterior values for each hospital ggplot(data = predicted_long , aes(x = Posteriors)) + geom_density(alpha = 1, fill = &quot;#1d3557&quot;) + theme_bw() + labs(title = &quot;Posterior Values&quot;, x = &quot;Posteriors&quot;, y = &quot;Density&quot;) + facet_wrap( ~ Hospitals, scales = &quot;free&quot;) 6.8 Useful resources Overall: Spatial and Spatio-temporal Bayesian Models with R-INLA Binomial distribution: Binomial Distribution: Definition, Formula, Analysis and Example Poisson distribution: Poisson Distribution: Formula and Meaning in Finance Negative-binomial distribution: Negative Binomial Distribution Geometric distribution: Geometric Distribution Bernoulli distribution: Binomial Distribution Beta-binomial distribution: Probability Playground: The Beta-Binomial Distribution Normal distribution: Normal Distribution: What It Is, Uses, and Formula Log-normal distribution: Log-Normal Distribution: Definition, Uses, and How To Calculate Alternative continuous distributions: What are continuous probability distributions &amp; their 8 common types? Conjugate prior: Conjugate prior Non-informative priors and prior selection: Bayesian Inference in Statistical Analysis INLA: R-INLA Project Penalised Complexity priors: Bayesian inference with INLA Penalised Complexity priors example: Geospatial Health Data: Modeling and Visualisation with R-INLA and Shiny "],["introduction-to-small-area-population-estimation-and-modelling-sapem.html", "7 Introduction to Small Area Population Estimation and Modelling (SAPEM) 7.1 Small area population estimation 7.2 Direct estimation 7.3 Indirect estimation 7.4 Useful resources", " 7 Introduction to Small Area Population Estimation and Modelling (SAPEM) This module focuses on small area population estimation, with information on both direct and indirect estimation methods, where the latter includes both top-down and bottom-up methodology. For this module, much of the relevant code is provided by the Book of Methods. 7.1 Small area population estimation Small area estimates (SAE) of a population use statistical techniques and models to provide a reliable population number at smaller (granular) area units, where the areas or domains often correspond to geographic domains (e.g. school districts and health service areas) or to socio-demographic groups (e.g. age by gender groups and groups in poverty). This is often done using multiple data sources, for example, sample surveys, and auxiliary data which have wider coverage with geospatial covariates, administrative data and census data. Small area population estimation is both quick and cost effective. SAE is needed, often as the focus of data collection systems is on the “big picture”, meaning that resulting estimates are reliable only to the highly aggregated levels such as national or regional levels, and not reliable for the smaller areas such as provinces or school districts. Additionally, granular or disaggregated data collection typically requires more resources, for example, more money, people and equipment, and often takes much longer. In many cases, limited resources are available, with time and financial constraints, so the granular data is not available, which is where SAE comes in useful. It should also be noted that there are challenges with traditional surveys, where census data is often very outdated (illustrated later in the module in Section 3. Indirect estimation). There is a demand for SAE given the importance of granular data being used in policy-making and decision-making, such as economic policies, business planning and allocation of government funds. There are two main methods to small area population estimation, direct estimation methods and indirect estimation methods, where the latter includes spatial statistical population models, for example, a bottom-up model. For implementation of SAE in R, the following packages can be utilised. car: provides functions that are useful for assessing regression models, calculations around those models and graphics. sae: provides functions to obtain model-based estimates for small areas, for both (basic) direct and indirect estimation methods, among other useful functions. InformationValue: provides functions that assess the performance of classification models among other useful functions. rsq: provides functions that compute the generalised R-squared and partial R-squared statistics, and partial correlation coefficients for GLMs. One of the problems with using satellite images for population size estimation is that satellite imagery does not always identify where there are buildings, for instance, when there is tree canopy covering a settlement. In this case, modelling is useful for better predicting the population size. A 2-step model solution is used as follows. Step 1: The relationships between the geospatial covariates and building intensity are exploited with a Bayesian hierarchical modelling framework. Estimates of building intensity across the entire area, including the areas under tree canopy cover, are obtained using the best fitting model. Step 2: The estimates of the building intensity obtained in Step 1 are then used to calculate population density. The relationship between population density and a set of geospatial covariates are explored within a Bayesian hierarchical modelling framework. As discussed in previous modules, the form of linear and generalised linear models are as given below. The form of a generalised additive model (GAM) is also given for reference. Linear model (LM): \\(Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\\), where \\(\\beta_0\\) is the intercept, \\(\\beta_1\\) is the slope and \\(\\epsilon_i\\) is the error term. Generalised linear model (GLM): \\(g(\\mu_i) = \\beta_0 + \\beta_1X_i\\), where \\(\\beta_0\\) is the intercept and \\(\\beta_1\\) is the slope. Generalised additive model (GAM): \\(g(\\mu_i) = \\alpha + \\sum_{j=1}^K f_j(x_j)\\) where \\(g(\\mu_i)\\) is the mean function (for example, log, logit or identity), \\(\\alpha\\) is the intercept and \\(f_j(x_j)\\) is the smooth function for continuous covariates. Similarly, the following models are with a non-spatial random effect, \\(v_i\\), that does not describe any form of spatial dependence between the observations in the linear, generalised linear and generalised additive mixed models. Linear mixed model (LMM): \\(Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i + v_i\\) Generalised linear model (GLMM): \\(g(\\mu_i) = \\beta_0 + \\beta_1X_i + v_i\\) Generalised additive model (GAMM): \\(g(\\mu_i) = \\alpha + \\sum_{j=1}^K f_j(x_j) + v_i\\) However, these models are non-spatial, as they do not have a spatial random effect. The following models are spatial statistical models, given that they have a spatially varying random effect, \\(v_i\\), which varies in space as a result of spatial geography (where individuals live). This relationship is described using the statistical models that are usually distanced based or use neighbourhood matrices. Linear mixed model (LMM): \\(Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i + v_i\\) Generalised linear model (GLMM): \\(g(\\mu_i) = \\beta_0 + \\beta_1X_i + v_i\\) Generalised additive model (GAMM): \\(g(\\mu_i) = \\alpha + \\sum_{j=1}^K f_j(x_j) + v_i\\) It is important to check which geospatial covariates are significant, since as with non-spatial models and covariates discussed in Module 4, you only want to include significant geospatial covariates in the spatial statistical models. This can be done in R by following these steps: Specify an appropriate GLM for the response with all the covariates included to obtain the model f1. In the example below, given that the data is count data, a Poisson distribution is used. For more information on why, see Module 4 Section 4: Generalised linear regression. #specify appropriate GLM f1 &lt;- glm(pop ~., data = data_pop, family = poisson) #count data for Poisson #summary for the model summary(f1) Run a stepwise selection with the stepAIC() function with the MASS package. See Module 4 Section 7: Stepwise regression for more information on the stepAIC() function and stepwise regression. #use stepAIC() for both ways stepwise regression step_pop &lt;- stepAIC(f1, scale = 0, direction = c(&quot;both&quot;), trace = 1, keep = NULL, steps = 1000, use.start = FALSE, k = 2) step_pop Rerun the selected model and call it something else, for example, f2. Check for multicollinearity through computing the Variance Inflation Factors (VIF) using the vif() function from the car package. #use vif() to check for multicollinearity vif_f2 &lt;- vif(f2) Retain only significant covariates with a VIF value of less than 5 (note: 5 is not always the chosen level of significance). #select covariates with vif value of &lt; 5 vif_f2[which(vif_f2 &lt; 5)] 7.2 Direct estimation Firstly, the domain of a survey specifies to which level precise and accurate estimates can be obtained. Then, suppose that the target population of the survey can be divided into \\(M\\) domains (\\(D_1, \\cdots, D_m, \\cdots,D_M\\)). To estimate the population size of a small area (for example, the total number of people in poverty), direct (survey) estimation can be used. The characteristic of interest, \\(y_j\\) must be specified (for example, \\(y_j=1\\) if the sampled unit is classified as in poverty and \\(y_j=0\\) otherwise), with each sampling unit having an associated sampling weight, \\(w_j\\). The resulting direct survey estimate \\(\\hat{Y}\\) can then be computed as the weighted sum of each characteristic of interest, \\(y_j\\), for each sampled unit \\(j\\) as follows. \\[ \\hat{Y} = \\sum_{j \\in S} w_j y_j,\\] where \\(j\\in S\\) corresponds to each sampled unit in the survey. If a domain is divided up into \\(N\\) sub-domains (\\({SD}_1, \\cdots, {SD}_n, \\cdots, {SD}_N\\)), the above formula can be adapted to compute the direct survey estimate for only the sampled units in sub-domain \\(n\\) by further restricting the summation to only include the sampled units within sub-domain \\({SD}_n\\) as follows. \\[ \\hat{Y}_n = \\sum_{j \\in S, j \\in {SD}_n} w_j y_j.\\] Whilst most direct survey estimators are unbiased, it is important to note that the sampling error of \\(\\hat{Y}_n\\) can be high if there is not a suitable number of observations used in the calculation, i.e. if the sub-domain \\({SD}_n\\) is notably smaller than any of the survey domains \\(D_m\\). A basic example of direct estimation can be found when trying to compute the average household size of a region. If you know the total population size and the number of households within a given region, then to estimate the average household size, you would divide the total population by the total number of households. This direct estimation is demonstrated in the code below. #define total population total_pop &lt;- 149375 #define total household size total_hh &lt;- 29875 #compute average household size ave_hh_size &lt;- total_pop/total_hh ave_hh_size ## [1] 5 For more information, see Asian Development Bank. For an example of implementation in R, see page 48. Alternatively, see the help files for the relevant packages. For example, the direct() function in the sae package can be used to compute the direct estimators of domain means, and the help file (?direct) provides the following example code to demonstrate the use of the function. 7.3 Indirect estimation Maps are very important for population modelling. For example, if in a given country, the aim is to vaccinate as close to 100% of under 5s as possible against polio, maps can aid in the visualisation of how many under 5s there are and where they are in order to ensure the correct amount of vaccine is available for each area. Additionally, detailed maps of each region are required in order to plan vaccinator logistics and routes. It is also important to consider the population at risk. To do this, supporting data on how many people there are and where they are, who they are and how do they move around is required. In terms of population data, censuses are one of the best sources available for population levels and trends, for example, to visualise age pyramids. Census data also provides population demographic and socio-economic characteristics, in addition to information on housing characteristics and socio-economic inequalities, including poverty levels. Population data can be very valuable for many different uses. It can help with appropriate resource allocation and evidence-based policy making and planning, act as a support tool for public health planning and also aid in monitoring development goals. However, whilst valuable, there are many challenges faced with relying solely on census data for population data. Whilst census data is very valuable, it is expensive and only collected once a decade, meaning that it can be very outdated with no way of monitoring the inter-censal years. This increases the need for more timely and detailed data. Registry and administration data can help fill these gaps, but they can be incomplete and unreliable in low-income settings, adding to the challenge of tracking progress towards development goals. Additionally, in some countries, even the most basic census, boundary and mapping data are lacking, where there have not been reliable censuses for many years, making it difficult to define a baseline and measure changes sub-nationally and regularly from this baseline. This means that additional resources are required, where new geospatial data sources can help compliment existing traditional sources to fill in the gaps and keep a sub-national focus. (#fig:africa census)Map of Africa illustrating scarcity of up-to-date census data (data correct as of December 2024) These data sources can include geo-located household surveys, satellite, GIS and mobile phone data. All of these data sources are available more regularly than census data in inter-censal periods and are useful in low- and middle-income settings. However, whilst these sources are valuable in providing regular measurements and sub-national detail, they contain biases and gaps, therefore, it is important to integrate them with each other and more traditional data sources to draw on the strengths of each source and overcome some of the biases present. In this context, population modelling is the process of combining aggregated/outdated census or survey data with a set of ancillary data to provide disaggregated population data. There are two main ways of doing this, top-down, where the admin-unit based census/official estimated counts are disaggregated, and bottom-up, where the census data is outdated/unreliable and so the satellite and mobile data is integrated with survey data. The bottom-up approach is independent of census data, and is more time-consuming and labour intensive, but can be the best approach when the census data is outdated and reliable. The top-down approach is the approach that forms most of WorldPop’s outputs at present. 7.3.1 Approaches for creating gridded population datasets The data available changes at different stages of the census cycle, so the current stage of the cycle needs to be considered. Top-down Bottom-up Data sources Census data. Administrative boundaries. Survey data. Satellite imagery. Other ancillary data. Directly following the census enumeration Assumes complete spatial coverage. Assumes that the population totals for the administrative areas (e.g. EAs, wards, districts) can be spatially disaggregated. If the spatial coverage is incomplete, the population enumeration areas can be used to predict the population in the incomplete areas. During the inter-censal period The projected population totals can be spatially disaggregated, however projections can become increasingly uncertain with the more time elapsed since the census was completed. Activities such as listings from household surveys and census cartography that include complete (geolocated) listings of households or collect population count information can be used. Advantages It can be very straight-forward to implement. It can estimate the population distributions quickly. Can capture the population distributions in more detail. When census data is unreliable, outdated or poor quality, can give more accurate estimates. Can estimate the population distributions in areas where there is no census data available. Disadvantages In areas where the population dynamics are complex or change rapidly, the accuracy can decrease. The accuracy depends on the quality of the census data - if the census data is outdated, unreliable or poor quality, the results can be inaccurate. Takes more time, with more expertise needed, to produce the results. Requires more data than top-down for implementation. 7.3.1.1 Top-down As mentioned above, this is the approach most commonly taken in WorldPop, focused on the disaggregation of admin-unit based census/official estimate counts. The approach combines the census population counts with the available geospatial covariates, disaggregating the population with a dasymetric approach, and producing gridded population estimates. The population data that is input for the top-down approach includes area-level population counts (e.g. wards and districts) with complete spatial coverage (generally from census/projections). (#fig:top-down process)Illustration of the top-down approach process from Wardrop et al. (2018) PNAS There are two main techniques for spatial disaggregation mapping, detailed as follows. Simple areal weighting: evenly redistributing aggregated population counts across all grid cells within each spatial unit. Pycnophulactic redistribution: producing a smooth surface with disaggregated population values not changing abruptly across spatial unit boundaries. Mask simple areal weighting: using a mask to define where, within each target unit, the input population counts must be evenly redistributed. Dasymetric disaggregation: redistributing input aggregated population counts across all grid cells within each spatial unit according to pre-defined weights (distributing population counts within selected boundaries through using covariate information such as land cover). Mask dasymetric disaggregation: using a mask to define where, within each target unit, the input population counts must be daysmetrically distributed. (#fig:disaggregation image)Visual example of top-down dasymetric approach There are many benefits to producing gridded estimates, including aggregation flexibility and a fine-grained understanding of population variation. Also, a consistent grid enables easy comparison between areas and with other data themes/topics, for example, health data (location of health facilities, catchment areas, health zones, areas subject to floods). There are many covariates available that depict contextual factors for top-down gridded population estimate. Some examples of these covariates are as follows. Covariate type Examples Building patterns Building counts Building density Building mean area Building total area Residential patterns Residential proportion Residential count Residential mean area Residential total area Socio-economic patterns Distance to marketplaces Distance to place of education Distance to local/main roads Distance to health providers Environmental patterns Slope Elevation Distance to bodies of water Distance to protected areas Distance to vegetation Distance to cultivated areas The covariates used should relate to the spatial distribution of the population and available for the entire area. They should also include the geographical information on the location. Typically, covariates with continuous values are better than categorical values. (#fig:mask dasymetric process)Illustration of the mask dasymetric disaggregation process 7.3.1.1.1 Random Forest model The Random Forest algorithm is a commonly-used machine learning algorithm used construct individual regression trees in order to predict values of a response variable, for example, population density, for both categorical variables (classification) and continuous variables (regression). Prediction is done through using the relationship between the response and the geospatial covariates. It is important to note that Random Forest models are not good at extrapolating values. Each of the trees constructed is trained using a random sub-sample of approximately 70% of the initial dataset. The final output is then the average prediction of all trees. To use the Random Forest model in R, the package popRF developed for Random Forest-Informed Population Disaggregation can be used. It is published openly and descriptions available on the CRAN website. Previously, the dasymetric mapping method was very complex in terms of computation, with for example, different plug-ins or scripts from the weighting scheme to the disaggregation of the population counts. However, the popRF() function performs the entire modelling workflow automatically, making the process much simpler. To use the popRF() function, the following arguments are required. pop: a tabular file containing the IDs of the sub-national areas and the corresponding total population. The population counts file is a .csv file that contains two columns. One that contains the unique area IDs (the same as in the mastergrid) and another column that contains the corresponding population values. cov: at least one ancillary raster covariate dataset must be provided. If multiple covariates are used, then they need to be in a list format. mastergrid: the raster file representing the sub-national areas and corresponding tabular population count for those areas. Mastergrid is also known as zonal data, and is the template gridded raster that contains the unique area IDs as their value. Once the level of analysis is decided, it is recommended to create the mastergrid first. watermask: a binary raster file indicating the pixels with water (1) and no water (0). Surface water does not need to be predicted and so a water mask raster is a required input. If only building footprints are provided, the watermask can only contain 0s (i.e. non water) as the model will only predict to settled pixels. px_area: a raster file containing the area of each pixel in square meters. To install the package directly from github, the function install_github() from the devtools package can be used as follows. install.packages(&quot;devtools&quot;) devtools::install_github(&quot;wpgp/popRF&quot;) library(&quot;popRF&quot;) Once the package is installed and loaded into R, the following are the main steps required to use the popRF() function. Set the directory location of the inputs. Create a list from the names and locations of the required covariates. State the names and locations of the mandatory inputs. Write the command line that executes the Random Forest application. To assess the results from using the popRF() function, first it must be checked that the aggregated admin totals match the values in the population input file used. Also, it must be checked that the explained variance is sufficiently high (&gt;80%), and remember that smaller-scale inputs improve the results a lot. Also, you should look over the results to see if there are any ‘weird’ shapes or patterns, and finally, check the results against a satellite image base layer. For more information and implementation in R, see Chapters 17 and 18 of the Book of Methods. 7.3.1.1.2 Weighting layer The weighting layer is used to divide up the population totals for each municipality into their enumeration areas (EA). This layer is created from predictive variables (geospatial covariates) related to population density, and associated with both the built and natural environment. The weights represent the proportions of the population from each municipality (for \\(j=1,...,J\\)) that lives within EA \\(i\\) (for \\(i=1,...,I_j\\)), and can be calculated using the model predictions from the random forest model as follows. \\[ \\text{weight}_i = \\frac{\\exp(\\text{predicted}_i)}{\\sum_{i=1}^{I_j}\\exp(\\text{predicted}_i)}\\] where \\(\\text{predicted}_i\\) is defined as the model prediction for EA \\(i\\) and \\(I_j\\) is defined as the total number of EAs in municipality \\(j\\). Geospatial covariates are used as proxies of variation in population density and are spatially harmonised so that all variables have the same spatial resolution and grid cell alignment for the geospatial covariate stack. For the geospatial covariate stack, the subset of geospatial covariates that are most strongly correlated with population density should be selected. It is commonly found that the covariates derived from building footprints or residential information are strong predictors for population density. If there are any covariates which are unexpectedly high predictors for population density, those covariates can be removed to test whether they are influential/necessary. 7.3.1.2 Bottom-up As mentioned earlier, the bottom-up methods are typically used when there is no recent (or appropriate) census data available in order to use the top-down approach. For the bottom-up methods, a sample of locations are chosen for which there is geolocated household survey data available. This data is then used in conjunction with geospatial covariates to fit a statistical model which estimates the population size in areas which were not sampled. In order to make the best use of the available data, for this approach, customised statistical models are applied, which also allow for probabilistic estimates of uncertainty. The bottom-up modelling process is then as follows. Step Breakdown Data collation, preparation and data analyses Data cataloguing Data cleaning Variable description Variable preparation Shapefile cleaning Data visualisation Exploratory analyses Results presentation/discussion Exploration of settlement data/ creation of geospatial covariates Raster visualisation Raster cleaning Various raster analyses Covariate checks Correlation analysis Raster stacking Other activities Development and exploration of various statistical model structures Data integration Random intercept model development (STAN and INLA) Observation-level prediction Bayesian models with covariates (STAN and INLA) Observation-level prediction Model selection, model validation, population prediction/ uncertainty quantification Model checks and testing Model selection Cross-validation Population prediction Uncertainty quantification Gridded population, estimates, model documentation, report and paper Population gridding production Model outline documentation Report writing Manuscript preparation for scientific publication (#fig:bottom-up process)Illustration of the bottom-up approach process from Wardrop et al. (2018) PNAS For more in-depth explanations of the bottom-up methods and implementation in R, see Chapters 5, 6 and 7 of the Book of Methods. A summary of the key types of input data required as explained in the Book of Methods is given below. There are a few key types of input data for bottom-up methods, these are as follows. Population data Should include enumerated population counts for enumerated areas only (at cluster or geolocated household level). Ideally includes a polygon shapefile with the boundary of each enumeration area and the total population within each area. Additional (optional) information collected during census and surveys such as point locations of buildings and/or households within enumeration areas can be useful for providing higher resolution information. Obtained from partial census results, microcensus surveys designed for population modelling and pre-survey listing data from routine household surveys Settlement map Used for identifying where residential structures are located/exist via building locations (points), building footprints (polygons) and/or gridded maps identifying pixels that contain buildings (raster). May classify areas into different settlement types. Additional (optional) information for each building such as building area, height and use can be beneficial for population modelling. Obtained from satellite imagery, pre-census cartography, building points and footprints and gridded derivatives of building footprints. Geospatial covariates Spatial datasets with national coverage focused on variables which are likely to have some correlation to the population density. Common geospatial covariates include road networks, night-time lights, locations of public facilities (e.g. hospitals or schools). To ensure that the accuracy of the population estimates is as high as possible, the covariates used in the estimation should be highly correlated to the population density (good quality covariates) and has comprehensive national coverage. Administrative boundaries May include regions, states (provinces), divisions and sub-divisions. Administrative units are often nested within each other. If there are population totals for each administrative unit, they can be used to summarise model results. 7.4 Useful resources Small area estimation: Introduction to Small Area Estimation Techniques popRF: WorldPop tutorial Top-down: Book of Methods Top-down: WorldPop tutorial Bottom-up: Book of Methods "],["bayesian-hierarchical-population-modelling-in-r.html", "8 Bayesian Hierarchical Population Modelling in R 8.1 Data sources 8.2 Data cleaning and covariates extraction 8.3 Exploratory analysis 8.4 Model set-up 8.5 STAN (MCMC) approach 8.6 INLA and INLA-SPDE approach", " 8 Bayesian Hierarchical Population Modelling in R This module expands upon the Bayesian hierarchical population modelling covered in earlier modules, focusing instead on the methods with spatial data. It introduces the key different data sources, methods for covariate extraction, exploratory analysis, and model set up. The remainder of the module briefly covers the STAN (MCMC) approach, and goes into detail for the INLA-SPDE approach to Bayesian hierarchical modelling. 8.1 Data sources 8.1.1 Population data There are three main forms of population (or demographic) data: micro-census, partial census and pre-survey listings. A brief description of each is given as follows. Micro-census: A targeted household survey that covers a fraction of the population. It is a complete enumeration of a micro-census cluster. However, there are very scarce data contexts with no data to re-purpose. Partial census: It covers only a part of the population or country. Useful when there are inaccessible areas, locations in conflict or territories controlled by anti-government groups, among other uses. pre-survey listings: Identifying and listing all household or dwellings in the survey area. It is done through defining the survey frame and re-purposing household surveys (e.g. health, livestock, …). There are 4 main steps for processing data as follows. Identify the spatial unit of analysis Identify variables common in the shapefile and the survey dataset (unique ids) Summarise variables of interest in the survey data Join the summarised survey data with the shapefile using unique ids Each of the datasets must be summarised by summing the household size to get the total population in each enumeration area (EA). This can be done using the following code as a template. #Data survey 1 Data_survey1 &lt;- Data_survey1 %&gt;% group_by(EA_ID) %&gt;% summarise(Total_Pop = sum(HH_Size)) %&gt;% mutate(Survey = &quot;survey1&quot;) #Data survey 2 Data_survey2 &lt;- Data_survey2 %&gt;% group_by(EA_ID) %&gt;% summarise(Total_Pop = sum(HH_Size)) %&gt;% mutate(Survey = &quot;survey2&quot;) #Data survey 3 Data_survey3 &lt;- Data_survey3 %&gt;% group_by(EA_ID) %&gt;% summarise(Total_Pop = sum(HH_Size)) %&gt;% mutate(Survey = &quot;survey3&quot;) #rbind all data as one data frame combined_data &lt;- rbind(Data_survey1, Data_survey2, Data_survey3) This data processing is demonstrated in the following code using the functions group_by(), sumarise() and mutate() with the 5 Cameroon survey datasets: Data_CMIS was collected in 2022 Data_EESI was collected in 2021 Data_ECAM1 was collected in the first Quarter of 2021 Data_ECAM2 was collected in the second Quarter of 2021 Data_ECAM3 was collected in the third Quarter of 2021 #load the sf and tidyverse packages library(sf) library(tidyverse) #load datasets Data_ECAM1 &lt;- read.csv(paste0(data_path, &quot;Data_ECAM1.csv&quot;)) Data_ECAM2 &lt;- read.csv(paste0(data_path, &quot;Data_ECAM2.csv&quot;)) Data_ECAM3 &lt;- read.csv(paste0(data_path, &quot;Data_ECAM3.csv&quot;)) Data_EESI &lt;- read.csv(paste0(data_path, &quot;Data_EESI.csv&quot;)) Data_CMIS &lt;- read.csv(paste0(data_path, &quot;Data_CMIS.csv&quot;)) #Data_CMIS Data_CMIS &lt;- Data_CMIS %&gt;% group_by(EA_ID) %&gt;% summarise(Total_Pop = sum(HH_Size)) %&gt;% mutate(Survey = &quot;CMIS&quot;) #Data_EESI Data_EESI &lt;- Data_EESI %&gt;% group_by(EA_ID) %&gt;% summarise(Total_Pop = sum(HH_Size))%&gt;% mutate(Survey = &quot;EESI&quot;) #Data_ECAM1 Data_ECAM1 &lt;- Data_ECAM1 %&gt;% group_by(EA_ID) %&gt;% summarise(Total_Pop = sum(HH_Size))%&gt;% mutate(Survey = &quot;ECAM1&quot;) #Data_ECAM2 Data_ECAM2 &lt;- Data_ECAM2 %&gt;% group_by(EA_ID) %&gt;% summarise(Total_Pop = sum(HH_Size))%&gt;% mutate(Survey = &quot;ECAM2&quot;) #Data_ECAM3 Data_ECAM3 &lt;- Data_ECAM3 %&gt;% group_by(EA_ID) %&gt;% summarise(Total_Pop = sum(HH_Size))%&gt;% mutate(Survey = &quot;ECAM3&quot;) #rbind all data as one data frame combined_data &lt;- rbind(Data_CMIS, Data_EESI, Data_ECAM1, Data_ECAM2, Data_ECAM3) In different surveys, as with this example of the five surveys, some enumeration areas are surveyed in a different survey. In this instance, the most recent survey needs to be maintained. For example if a particular EA was surveyed in both CMIS (2022) and EESI (2021), CMIS is maintained since it is the most recent. This will be done across all surveys to ensure that there are not duplicated EAs in the dataset. In order to do this, the data must be ordered using the function mutate() before removing the duplicated data in order of priority with the functions arrange() and filter(). #order data combined_data &lt;- combined_data %&gt;% mutate(Data_Priority = case_when(Survey == &quot;ECAM1&quot; ~ 1, Survey == &quot;ECAM2&quot; ~ 2, Survey == &quot;ECAM3&quot; ~ 3, Survey == &quot;EESI&quot; ~ 4, Survey == &quot;CMIS&quot; ~ 5)) #remove duplicated EA in order of priority combined_data &lt;- combined_data %&gt;% arrange(EA_ID, -Data_Priority) %&gt;% filter(!duplicated(EA_ID)) Once the duplicates are removed, the shapefile for the enumeration area can be joined with the combined survey dataset based on on the enumeration area ID which is a unique ID to both datasets. #load EA_shapefile EA_Shapefile &lt;- st_read(paste0(data_path, &quot;EA_Shapefile.shp&quot;)) ## Reading layer `EA_shapefile&#39; from data source ## `F:\\Study\\WorldPop Training Materials\\GitBook\\wp_training_manual\\data\\CMR\\EA_shapefile.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 2290 features and 1 field ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 8.498025 ymin: 1.653369 xmax: 16.1904 ymax: 12.96129 ## Geodetic CRS: WGS 84 #join the shapefile and data based on EA_ID (a unique id to both datasets) Population_data &lt;- inner_join(EA_Shapefile, combined_data, by = &quot;EA_ID&quot;) Finally, the unwanted variables can be removed from the resulting population data with the final result outputted as a shapefile. #remove unwanted variables Population_data &lt;- Population_data %&gt;% dplyr::select(-Data_Priority) #write data as a shapefile write_sf(Population_data, paste0(output_path, &quot;EA_population.shp&quot;)) 8.1.2 Settlement data There are four main sources of settlement data, detailed as follows. Building footprints: Typically given as a polygon shapefile of buildings. Sources of building footprints are: Maxar &amp; Ecopia building footprint (with processing partner: Ecopia AI), a frequently used settlement data source for gridded population estimations. Google - Open Buildings, available from Google. Microsoft - Building Footprints, available on GitHub. World Settlement Footprint (with processing partners: A collaboration between the European Space Agency, German Aerospace Center and Google Earth Engine). This dataset is given as a raster based on Sentinel 2. OpenStreetMap Built-up area: Typically a raster file at 100m resolution. A source of this is: GHS-Built-Up Surface Global Human Settlement Layer (with processing partner: European Commission). Gridded building intensity: This is provided by Planet NSO building datasets: This is data on building information collected during a census or survey, the building count and/or the building classification. There are multiple uses of settlement data. For example, as a proxy to extrapolate population totals. In this instance, the building count or settled area is used as the input and can be used to model the population in surveyed areas, to estimate the population total in areas not surveyed and to produce high resolution estimates. Additionally, settlement data can be used as covariates to detect density variation. For this use, of interest are the different summary metrics (area, perimeter, proximity, angle, shape, …), the different aggregation metrics at the grid cell (minimum, mean, maximum, standard deviation, …) and the different window sizes for contextual information. 8.1.3 Other sources Settlement data is not the only source of geospatial data. Other sources include (but are not limited to): Night-time light Distance to hospitals Distance to schools Water bodies Vegetation cover Land uses Conflict data However, not all geospatial data sources are good to use as geospatial covariates in a model. When selecting a geospatial covariate, it needs to be considered whether or not the covariate has a significant effect on population. Additionally, the availability of the covariate data (open-source/closed) should be considered, along with the spatial and temporal resolution of the covariate. 8.2 Data cleaning and covariates extraction Prior to working with the covariates and modelling the data, the data itself must be extracted. To demonstrate these methods, the Cameroon population data will be used as an example. The extraction process essentially overlays polygons onto the covariate raster data, and extracts the covariate information from each raster that is within the polygon. To begin with, the relevant data must be read into R. In this case, the population dataset must also be projected using the function st_transform() to be the same spatial reference as the mastergrid. #install raster processing packages install.packages(&quot;terra&quot;) install.packages(&quot;tictoc&quot;) install.packages(&quot;exactextractr&quot;) #load raster processing packages library(terra) #raster analysis library(tictoc) library(exactextractr) #covariate extraction library(raster) #read combined population dataset and mastergrid pop_data_shp &lt;- st_read(paste0(data_path, &quot;Population/EA_population.shp&quot;)) ## Reading layer `EA_population&#39; from data source ## `F:\\Study\\WorldPop Training Materials\\GitBook\\wp_training_manual\\data\\CMR\\Population\\EA_population.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 1592 features and 3 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 8.498025 ymin: 1.653369 xmax: 16.1904 ymax: 12.96129 ## Geodetic CRS: WGS 84 mgrid &lt;- rast(paste0(mgrid_path, &quot;CMR_mastergrid_WP_ND.tif&quot;)) #project pop_data to same spatial reference as mgrid pop_data_shp &lt;- st_transform(pop_data_shp, crs = st_crs(mgrid)) 8.2.1 Extracting continuous rasters Starting with continuous rasters, first all of the continuous raster covariates must be imported. When there are multiple covariates, instead of importing each manually, the function list.files() can be used to list all of the relevant covariates at once. In this function, to extract the correct information, as arguments the drive path to the covariates must be specified under path, the file type specified under pattern (in this case, .tif$), whether you want all files (in this case all.files = TRUE) and whether you want full names (in this case full.names = FALSE). #import all continuous raster covariates raster_list &lt;-list.files(path = raster_path1, pattern = &quot;.tif$&quot;, all.files = TRUE, full.names = FALSE) #let see the rasters in the list raster_list ## [1] &quot;acled_battles_2021_CMR_ed_masked.tif&quot; ## [2] &quot;acled_conflict_data_2021_CMR_ed_masked.tif&quot; ## [3] &quot;acled_explosions_2021_CMR_ed_masked.tif&quot; ## [4] &quot;acled_protests_2021_CMR_ed_masked.tif&quot; ## [5] &quot;acled_riots_2021_CMR_ed_masked.tif&quot; ## [6] &quot;acled_strategic_developments_2021_CMR_ed_masked.tif&quot; ## [7] &quot;acled_violence_against_civilians_2021_CMR_ed_masked.tif&quot; ## [8] &quot;CMR_AccessToCities2015.tif&quot; ## [9] &quot;CMR_buildings_cv_area.tif&quot; ## [10] &quot;CMR_buildings_cv_length.tif&quot; ## [11] &quot;CMR_buildings_density.tif&quot; ## [12] &quot;CMR_buildings_mean_area.tif&quot; ## [13] &quot;CMR_buildings_mean_length.tif&quot; ## [14] &quot;CMR_buildings_total_length.tif&quot; ## [15] &quot;CMR_dst_coastline_100m_2000_2020.tif&quot; ## [16] &quot;CMR_dst_in_water_100m_2000_2012.tif&quot; ## [17] &quot;CMR_esaccilc_dst011_100m_2015.tif&quot; ## [18] &quot;CMR_esaccilc_dst040_100m_2015.tif&quot; ## [19] &quot;CMR_esaccilc_dst130_100m_2015.tif&quot; ## [20] &quot;CMR_esaccilc_dst140_100m_2015.tif&quot; ## [21] &quot;CMR_esaccilc_dst150_100m_2015.tif&quot; ## [22] &quot;CMR_esaccilc_dst160_100m_2015.tif&quot; ## [23] &quot;CMR_esaccilc_dst190_100m_2015.tif&quot; ## [24] &quot;CMR_esaccilc_dst200_100m_2015.tif&quot; ## [25] &quot;CMR_FrictionVehicle2020.tif&quot; ## [26] &quot;CMR_FrictionWalk2020.tif&quot; ## [27] &quot;CMR_grid_100m_wclim_precip_2020.tif&quot; ## [28] &quot;CMR_grid_100m_wclim_temp_2020.tif&quot; ## [29] &quot;CMR_osm_dst_education.tif&quot; ## [30] &quot;CMR_osm_dst_health.tif&quot; ## [31] &quot;CMR_osm_dst_localroads.tif&quot; ## [32] &quot;CMR_osm_dst_majorroads.tif&quot; ## [33] &quot;CMR_osm_dst_mktplaces.tif&quot; ## [34] &quot;CMR_osm_dst_PofW.tif&quot; ## [35] &quot;CMR_osm_dst_Proad_intersections.tif&quot; ## [36] &quot;CMR_osm_dst_railways.tif&quot; ## [37] &quot;CMR_osm_dst_waterbodies.tif&quot; ## [38] &quot;CMR_osm_dst_waterways.tif&quot; ## [39] &quot;CMR_srtm_topo_100m.tif&quot; ## [40] &quot;CMR_viirs_100m_2020.tif&quot; ## [41] &quot;CMR_wdpa_dst_cat1_100m_2017.tif&quot; Once all the covariates are imported, they can be stacked using the rast() function as follows. This means that the data can be processed more efficiently through extracting all rasters at once rather than needing to extract each raster individually. #stack all covariates cont_raster_covariates &lt;- rast(paste0(raster_path1, c(raster_list))) For visualisation, using the plot() function as discussed in Module 2, the raster can be plotted. #plot first raster plot1 &lt;- plot(cont_raster_covariates[[1]]) plot2 &lt;- plot(cont_raster_covariates[[2]]) The function exact_extract() from the exactextractr package can be used as follows to extract the covariate information from the stack of rasters, through including the raster stack and population shapefile as arguments in addition to specifying the function used to extract the rasters with the argument fun =. In this case, to use the mean the argument fun = 'mean' is given, which extracts the mean value of the covariate information for each polygon region. The functions tic() and toc() from the tictoc package can be used to compute the elapsed time required for running the function, it is not necessary to time the process but can be beneficial to know how long a section of code takes to run, for example, to know how long to expect it to take in future for time management and planning purposes. To do this, place the tic() function at the start of the code you wish to time and place the toc() function at the end. Without additional parameters, the amount of time elapsed to run the code between the two functions will be outputted at the end. #Extract rasters using their mean values tic() cont_raster_extract &lt;- exact_extract(cont_raster_covariates, pop_data_shp, fun = &#39;mean&#39;) toc() Functions discussed in earlier modules can then be used to extract further information from the raster data. The function names() can be used to check and extract the names of the rasters, with the function colnames() being used to change the names to something more uniform such as x1, x2, … in that order. Finally, the variable names can be combined with the cbind() function and exported as a .csv file with the function write.csv(). Each of these operations are demonstrated below. #check names of rasters names(cont_raster_extract) #extract variable names var_names &lt;- names(cont_raster_extract) #change names to x1, x2, x3... in that order colnames(cont_raster_extract) &lt;- c(paste0(&#39;x&#39;, 1:41)) #extract names of raster var_names2&lt;- names(cont_raster_extract) #cbind names var_names &lt;- cbind(var_names, var_names2) %&gt;% as_tibble() #export names write.csv(var_names, paste0(output_path, &quot;var_names.csv&quot;)) 8.2.2 Extracting categorical rasters Similar methods to those used for continuous rasters can be used to extract categorical rasters. The first step is to use the list.files() function to import the categorical raster covariates into R, then stacking all of the covariates with the rast() function. #import all categorical raster covariates raster_list &lt;-list.files(path = raster_path2, pattern = &quot;.tif$&quot;, all.files = TRUE, full.names = FALSE) raster_list ## [1] &quot;CMR_Department.tif&quot; &quot;CMR_Regions.tif&quot; ## [3] &quot;CMR_Settlement_Classification.tif&quot; #stack all covariates cat_raster_covariates &lt;- rast(paste0(raster_path2, c(raster_list))) The following step is to extract the rasters using the exact_extract() function. However, instead of using the mean function, the modal value is now used to extract the rasters with the argument fun = 'mode', extracting the modal value of the categorical covariate information (the category with highest frequency of occurrence) from the polygon regions. Finally, the variables can be renamed with the function rename() with arguments for the names of the individual variables using the format new_name = old_name. #rename variables cat_raster_extract &lt;- cat_raster_extract %&gt;% rename(Regions = mode.CMR_Regions, Department = mode.CMR_Department, Settlement_Type = mode.CMR_Settlement_Classification) 8.2.3 Extracting building count Another of the covariates given for this dataset is the building count, indicated by the file name CMR_buildings_count.tif. To load this file, the function rast() can be used with the mastergrid and shapefile as arguments. #load b_count r1 &lt;- rast(paste0(mgrid_path,&quot;CMR_buildings_count.tif&quot;)) Once the building count has been loaded, the function extract() from the terra package can be used to extract the values with arguments for the building count and population shapefile as arguments with the function argument specified as fun = sum and removing NA values as na.rm = TRUE. #extract values B_Count &lt;-terra::extract(r1, pop_data_shp, fun = sum, na.rm = TRUE) Finally, the variables can be reamed with the select() function from the dplyr package, in addition to the rename() function. #rename variable B_Count &lt;- B_Count %&gt;% dplyr::select(CMR_buildings_count)%&gt;% rename(Total_Building_Count = CMR_buildings_count) 8.2.4 Exracting building total area Similar methods can be used as for extracting building count to extract the building total area, starting with using the function rast() to load the building total area data. #building Area r2 &lt;- rast(paste0(mgrid_path,&quot;CMR_buildings_total_area.tif&quot;)) Then the extract() function from the terra package can be used to extract the values. As with for the building count, the building area and shapefile is given as arguments, in addition to the function for summing the values fun = sum and setting na.rm = TRUE. #extract values B_Area &lt;-terra::extract(r2, pop_data_shp, fun = sum, na.rm = TRUE) The last step is to then rename the variables with the select() and rename() functions as before. #rename variable B_Area &lt;- B_Area %&gt;% dplyr::select(CMR_buildings_total_area)%&gt;% rename(Total_Building_Area = CMR_buildings_total_area) Once you have your variables of interest, the function cbind() can be used to combine the variables together into a complete population dataset. #column bind the variables together to create a dataset Pop_Data_Complete &lt;- cbind(pop_data_shp, cat_raster_extract, B_Count, B_Area,cont_raster_extract) 8.2.5 Adding admin area names to the data To add admin area names to the dataset, first the relevant shapefiles need to be identified and loaded into R with the function st_read(). In this case, following the shapefile_path, the relevant shapefiles are for the region and department. #load the shapefiles for region and department regions &lt;- st_read(paste0(shapefile_path, &quot;Region_SHP.shp&quot;)) ## Reading layer `Region_SHP&#39; from data source ## `F:\\Study\\WorldPop Training Materials\\GitBook\\wp_training_manual\\data\\CMR\\Shapefiles\\Region_SHP.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 10 features and 2 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 945997.2 ymin: 182845.8 xmax: 1802307 ymax: 1458913 ## Projected CRS: WGS 84 / World Mercator dept &lt;- st_read(paste0(shapefile_path, &quot;Departement_SHP.shp&quot;)) ## Reading layer `Departement_SHP&#39; from data source ## `F:\\Study\\WorldPop Training Materials\\GitBook\\wp_training_manual\\data\\CMR\\Shapefiles\\Departement_SHP.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 58 features and 3 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 945997.2 ymin: 182845.8 xmax: 1802307 ymax: 1458913 ## Projected CRS: WGS 84 / World Mercator From there, the region names and IDs can be selected using the select() function, in conjunction with the rename() function to better identify the region. #select region names and ids regions &lt;- regions %&gt;% as_tibble() %&gt;% dplyr::select(id, libelle) %&gt;% rename(region_libelle = libelle) To join the regions to the complete data, the function inner_join() introduced in an earlier module can be used, including the regions as an argument with specification of how to join the data with the by = argument. #join regions to complete data Pop_Data_Complete &lt;- Pop_Data_Complete %&gt;% inner_join(regions, by = c(&quot;Regions&quot;=&quot;id&quot;)) Additionally, the department names can be selected and joined to the data using the same methods shown above. #select department names dept &lt;- dept %&gt;% as_tibble() %&gt;% dplyr::select(id, libelle) %&gt;% rename(dept_libelle = libelle) #join department to the complete data Pop_Data_Complete &lt;- Pop_Data_Complete %&gt;% inner_join(dept, by = c(&quot;Department&quot;=&quot;id&quot;)) The select() function can then be used to sort the data into a proper order before exporting the resulting shapefile as a geopackage file with the write_sf() function. #sort data in proper order Pop_Data_Complete &lt;- Pop_Data_Complete %&gt;% dplyr::select(EA_ID, Department, dept_libelle, Regions, region_libelle, Total_Pop, Settlement_Type, Total_Building_Count, Total_Building_Area, starts_with(&quot;x&quot;)) #export shapefile as a geopackage file write_sf(Pop_Data_Complete, paste0(output_path,&quot;Pop_Data_Complete.gpkg&quot;)) Alternatively, the shapefile can be converted to a data frame with the as.data.frame() function, before removing the geometry with the select() function to export as a .csv file with the write.csv() function. #convert shapefile to data frame Pop_Data_Complete1 &lt;- as.data.frame(Pop_Data_Complete) #remove Geometry Pop_Data_Complete1 &lt;- Pop_Data_Complete1 %&gt;% dplyr::select(-geometry) #export data frame as csv write.csv(Pop_Data_Complete1, paste0(output_path, &quot;Pop_Data_Complete.csv&quot;)) 8.3 Exploratory analysis To start with, the dataset resulting from the code in the previous section Pop_Data_Complete.csv will be used, in addition to the variable names file created earlier. #input the data Data_CMR &lt;- read.csv(paste0(data_path,&quot;Pop_Data_Complete.csv&quot;)) #load the variable names file var_names &lt;- read.csv(paste0(data_path,&quot;var_names.csv&quot;)) As introduced in Module 2, the describe() function from the psych package can be used to give a summary of the data, similarly, the str() function can be used to display the structure of the data. #load the psych package library(psych) #describe the data using both str() and describe() str(Data_CMR); psych::describe(Data_CMR) 8.3.1 Basic visualisation To visualise the data, methods discussed in Modules 2 and 3 can be used. To examine the distribution of the data, a histogram can be plotted using the ggplot() function within the ggplot2 package. #examine data distribution using histograms p1&lt;- Data_CMR %&gt;% ggplot(aes(x = Total_Pop/Total_Building_Count)) + geom_histogram(bins = 20, color = &quot;white&quot;, fill = &quot;#004C92&quot;, position = &quot;dodge&quot;)+ scale_fill_manual(values = &quot;#004C92&quot;) + labs(title = &quot;Histogram of population density&quot;, x = &quot;Population density&quot;, y = &quot;Frequency&quot;)+ theme_classic() p1 ## Warning: Removed 3 rows containing non-finite outside the scale range (`stat_bin()`). Alternatively, a box plot can be used to investigate any outliers in continuous variables. This time, the function boxplot() from base R, also as seen in Module 2, to display the information from just one continuous covariate at a time and also information from multiple continuous covariates at a time. #investigate outliers in continuous variables using box plots boxplot(Data_CMR[,&quot;Total_Pop&quot;]) #visualise the box plots of multiple continuous covariates at a time boxplot(Data_CMR[,paste0(&quot;x&quot;, 1:7)], col = rainbow(7)) For categorical variables, bar plots can be used to examine the distribution of the data. For example, in this case, the distribution of settlement types can be visualised with a bar plot, where it can be seen that settlement type 1 has the highest frequency of occurrence, and settlement type 2 the lowest. #examine distribution of categorical variable using bar charts barplot(table(Data_CMR$Settlement_Type), col = &quot;#004C92&quot;) The mutate() function can also be used to transform data before plotting, in this instance, log-transform the population density to plot a histogram of the log-transformed population density. #transformed data p2&lt;- Data_CMR %&gt;% dplyr::mutate(logDensity = log(Total_Pop/Total_Building_Count)) %&gt;% ggplot(aes(x = logDensity)) + geom_histogram(color = &quot;white&quot;, fill = &quot;#E69F00&quot;, position = &quot;dodge&quot;)+ labs(title = &quot;Histogram of log-transformed \\n population density&quot;, x = &quot;Log(Population density)&quot;, y = &quot;Frequency&quot;)+ theme_classic() p2 ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 3 rows containing non-finite outside the scale range (`stat_bin()`). The correlation among multiple continuous covariates can be examined through using the function corrplot() from the package corrplot. This is not something which has been previously discussed, but can be used as in the example below with the Cameroon data. To use this plot function, the covariates you wish to explore the correlation between must be specified, along with arguments for the visualisation method used (in this case, method=square) and the type of matrix (in this case, type=upper for an upper triangular matrix). For more information on the possible arguments and how to use them, look at the help file ?corrplot::corrplot. #load the corrplot package library(corrplot) #examine correlation among multiple continuous covariates using corrplot corrplot( cor(Data_CMR[,paste0(&quot;x&quot;, 1:8)]), method = &#39;square&#39;, type = &#39;upper&#39;, tl.col = &#39;black&#39;, tl.cex = 1, col = colorRampPalette(c(&#39;purple&#39;, &#39;dark green&#39;))(200) ) Lastly, the function ggarrange from the package ggpubr can be used as discussed in Module 2 to keep the plots tidy and display them together. #load the ggpubr package library(ggpubr) #used within ggplot to keep plots tidier #arrange the plots side-by-side ggarrange(p1, p2, ncol = 2, nrow = 1) 8.3.2 Geospatial visualisation To visualise the data on a map similar to as in Module 3, the following packages should be installed and loaded. #install the relevant packages install.packages(&quot;tmap&quot;) install.packages(&quot;terra&quot;) install.packages(&quot;leaflet&quot;) install.packages(&quot;mapview&quot;) install.packages(&quot;units&quot;) install.packages(&quot;tidyverse&quot;) install.packages(&quot;RColorBrewer&quot;) install.packages(&quot;exactextractr&quot;) #load the relevant packages library(tmap) library(terra) library(leaflet) library(mapview) library(units) library(tidyverse) library(RColorBrewer) library(exactextractr) # tmap_options(check.and.fix = TRUE) #fix potential issues during the processing The country’s (Cameroon) national boundary is can be extracted from the World dataset from the tmap package which produces a world map. Following that, using the filter() function, Cameroon can be subset from the world map and visualised. #extract the boundary for the country data(&quot;World&quot;) class(World) ## [1] &quot;sf&quot; &quot;data.frame&quot; names(World) ## [1] &quot;iso_a3&quot; &quot;name&quot; &quot;sovereignt&quot; &quot;continent&quot; &quot;area&quot; &quot;pop_est&quot; ## [7] &quot;pop_est_dens&quot; &quot;economy&quot; &quot;income_grp&quot; &quot;gdp_cap_est&quot; &quot;life_exp&quot; &quot;well_being&quot; ## [13] &quot;footprint&quot; &quot;inequality&quot; &quot;HPI&quot; &quot;geometry&quot; tm_shape(World) + tm_polygons(col = &quot;name&quot;, legend.show = FALSE) ## Warning: Number of levels of the variable &quot;name&quot; is 177, which is larger than max.categories (which is ## 30), so levels are combined. Set tmap_options(max.categories = 177) in the layer function to show all ## levels. #subset Cameroon cmr_national &lt;- World %&gt;% filter(name == &quot;Cameroon&quot;) #visualise the Cameroon boundary map tm_shape(cmr_national) + tm_polygons(col = &quot;gray&quot;, legend.show = FALSE) + tm_compass(position = c(&quot;right&quot;, &quot;top&quot;), text.size = 1.5) + tm_scale_bar(position = c(&quot;left&quot;, &quot;top&quot;), text.size = 1.5, size = 1) ## Warning: The argument size of tm_scale_bar is deprecated. It has been renamed to text.size To visualise data on the map of Cameroon, starting with points data, the corresponding datasets first need to be imported into R. #EA shapefile ea_shp &lt;- st_read(paste0(data_path,&quot;Pop_Data_Complete.gpkg&quot;)) ## Reading layer `Pop_Data_Complete&#39; from data source ## `F:\\Study\\WorldPop Training Materials\\GitBook\\wp_training_manual\\data\\CMR\\Pop_Data_Complete.gpkg&#39; ## using driver `GPKG&#39; ## Simple feature collection with 1592 features and 50 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 8.498025 ymin: 1.653369 xmax: 16.1904 ymax: 12.96129 ## Geodetic CRS: WGS 84 #admin 2 (departments) shapefile department &lt;- st_read(paste0(data_path,&quot;/Shapefiles/Departement_SHP.shp&quot;)) ## Reading layer `Departement_SHP&#39; from data source ## `F:\\Study\\WorldPop Training Materials\\GitBook\\wp_training_manual\\data\\CMR\\Shapefiles\\Departement_SHP.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 58 features and 3 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 945997.2 ymin: 182845.8 xmax: 1802307 ymax: 1458913 ## Projected CRS: WGS 84 / World Mercator Once you have the data in the R environment, the points (centroids of the polygons) can be extracted from the files. Below, the function as() is used to convert the geometry obtained from the shapefiles with the function st_geometry() to a spatial object. #extract the points shp &lt;- as(st_geometry(ea_shp), &quot;Spatial&quot;) #convert to spatial object shp0 &lt;- as(st_geometry(cmr_national), &quot;Spatial&quot;) #convert cmr_national to spatial object shp2 &lt;- as(st_geometry(department), &quot;Spatial&quot;) #convert department to spatial object The longitude and latitude can then be extracted and added to the main Cameroon demographic data as variables. This will enable the points to be plotted in the correct locations on the map of Cameroon. #add the lon-lat to the demographic data Data_CMR$lon &lt;- coordinates(shp)[,1] #extract and add the longitude to the data Data_CMR$lat &lt;- coordinates(shp)[,2] #extract and add the latitude to the data head(Data_CMR, 6) ## X EA_ID Department dept_libelle Regions region_libelle Total_Pop Settlement_Type ## 1 1 ABONG - MBANG_6 30 Haut Nyong 3 Est 890 1 ## 2 2 AFANLOUM_702 3 Mefou et Afamba 2 Centre 815 4 ## 3 3 AKOEMAN_3 49 Nyong et Soo 2 Centre 764 4 ## 4 4 AKOM II_702 25 Ocean 10 Sud 746 4 ## 5 5 AKONOLINGA_18 50 Nyong et Mfoumou 2 Centre 1109 4 ## 6 6 AKONOLINGA_700 50 Nyong et Mfoumou 2 Centre 1357 4 ## Total_Building_Count Total_Building_Area x1 x2 x3 x4 x5 x6 ## 1 286 32018.38 240337.7 86203.41 413765.8 139978.61 86203.41 184933.17 ## 2 395 38397.15 338154.9 36501.53 317968.1 36501.53 36501.53 74746.61 ## 3 367 33433.51 437903.8 54406.30 278816.8 72425.01 54406.30 72425.01 ## 4 269 24597.57 572474.0 65059.10 207275.2 167467.12 65059.10 171884.52 ## 5 286 39113.68 346930.5 47410.98 334817.6 80121.03 47410.98 80121.03 ## 6 402 30872.22 344902.1 55245.77 333230.8 76245.29 55245.77 78916.31 ## x7 x8 x9 x10 x11 x12 x13 x14 x15 x16 ## 1 184933.17 49.00933 0.8580208 0.5127055 1874.8511 124.96131 43.36115 663.00330 369.54599 66.4226456 ## 2 74746.61 94.78092 0.3557110 0.2034208 294.0987 102.34338 39.95544 96.79045 264.67972 28.3830357 ## 3 72425.01 88.32083 0.3629119 0.2126397 328.2499 90.47405 36.73468 104.04270 177.91858 22.1087456 ## 4 171884.52 399.27695 0.3817135 0.2085105 403.9308 100.72127 39.06967 128.10530 63.06371 29.8604965 ## 5 80121.03 65.29633 0.5584586 0.3535644 1485.4635 132.72063 45.55459 590.01727 262.23849 0.5587888 ## 6 78916.31 60.77288 0.3845364 0.2158287 318.3337 79.70691 34.87637 92.70645 264.28845 4.6560779 ## x17 x18 x19 x20 x21 x22 x23 x24 x25 ## 1 0.2506842 0.2168530 0.8513528 53.339569 571.1937 1.780275 -0.05533693 118.72293 0.001490207 ## 2 0.3522204 -0.2481708 11.8486433 11.467803 493.3647 30.267328 26.14026642 77.73151 0.023819378 ## 3 2.0923870 -1.9790570 33.6847458 55.099686 515.8448 38.471977 22.51198578 201.92075 0.024310285 ## 4 7.1534176 -7.0361733 65.0350876 65.647385 483.9221 62.489433 34.99201965 320.03601 0.045389920 ## 5 0.6619807 -0.3192018 0.7926053 1.597502 531.0815 4.531395 0.59316117 115.86395 0.014609964 ## 6 0.4313623 -0.3258749 8.7227964 5.226479 525.6237 9.087230 7.94695139 107.66605 0.018198114 ## x26 x27 x28 x29 x30 x31 x32 x33 x34 ## 1 0.01200000 4.149579e-05 297.4118 2296.646 288.1877 14.69919 225.3033 390.2323 476.4956 ## 2 0.02870464 4.402846e-05 297.6783 35368.785 47777.6641 11518.32129 8068.7568 48683.5234 26756.2227 ## 3 0.02861884 5.605924e-05 297.4679 21204.406 28928.2246 1605.29187 38670.0625 40177.6914 31366.0879 ## 4 0.04607061 1.064296e-04 298.1458 34093.812 40311.1211 9626.27930 6508.7632 35146.3125 43208.0781 ## 5 0.03405531 4.095252e-05 297.7811 1161.185 1691.4470 630.90527 37276.1211 1713.5939 2773.5483 ## 6 0.02470175 4.092990e-05 297.7823 7917.528 8837.5303 996.53833 29232.8477 9127.7480 9087.2295 ## x35 x36 x37 x38 x39 x40 x41 lon lat ## 1 2.410922 99782.04 435.05762 722.6469 694.9709 0.8488668 499.7907 13.17765 3.985047 ## 2 8.193155 34135.90 6064.46387 20564.3906 732.0835 0.1883050 387.9030 12.10438 4.195674 ## 3 14.047594 51499.27 10335.01562 5314.2729 684.1978 0.1645098 315.3396 11.57054 3.214603 ## 4 13.598531 102436.87 23514.07422 8531.0508 603.2908 0.2087497 201.3288 10.45882 2.733207 ## 5 1.513200 77467.59 87.84882 354.3566 647.6537 0.3821937 392.5313 12.23236 3.776276 ## 6 3.314217 73533.62 3984.64355 4101.4194 691.9840 0.2072315 393.2006 12.22670 3.853698 Once the data is set up, the longitude and latitude coordinates can be combined and the resulting points plotted using the basic plot() function. However, this on its own is not very useful. For the results to be easily interpreted and better visualised, the national boarder can be plotted first with the coordinate points be plotted over the top with the points() function. #display point data coords &lt;- cbind(Data_CMR$lon, Data_CMR$lat) plot(coords, col = &quot;red&quot;, xlab = &quot;longitude&quot;, ylab = &quot;latitude&quot;) #overlay points on the national boundary plot(shp0) points(coords, col = &quot;red&quot;) 8.3.3 Checking for NAs As it has been discussed in other modules, it is important to check for NA values within your dataset as missing values can greatly affect both computation and results. To check whether there are any NA values in a chosen dataset, the function is.na() can be used, inputting the selected data as an argument. You can include the entire dataset in this function, or a subset of the chosen dataset. If a single covariate is included as an argument, the function outputs a logical vector the same length as its argument x, indicating whether the corresponding value is missing or not. If the entire dataset or a selection of covariates is used as the argument in the function, a logical matrix is returned with the same dimensions as the data frame included as the argument. The logical vector (or matrix) returned, is often not of use on its own, particularly if it is very long. What is typically more of interest is the total number of NA values in the data, found using the sum() function with the results from the is.na() function. This is useful to know as it can be used to decide whether to drop or impute the missing values. The following code demonstrates how to perform these tasks. #check for NA values NAvalues_x27 &lt;- is.na(Data_CMR$x27) sum(NAvalues_x27) ## [1] 52 Another figure which is often of interest is which index (or indices) of the variable(s) correspond to the missing values. This can be done through using the which() function, including the results from the is.na() function as the argument. #find which values are NA which(is.na(NAvalues_x27)) ## integer(0) As mentioned, often, any NA values are simply just removed. This is demonstrated later on in the module in the generalised linear regression section, where the NA values are removed from the covariate information prior to modelling using the function drop_na() from the tidyr package. In this instance, modelling in R cannot be done when there are missing values, so it is important that the missing data is removed (if they are not removed beforehand, R can remove the observations with missing information during the modelling process). However, removing all observations with NA values is not always the best option. In some situations, removing all observations with NA values will not leave enough information to accurately model the data from. When this occurs, the missing data can first be imputed using the remaining results. There are multiple approaches to impute the missing data, but one of the most commonly used approaches is to use the mean (or median) value of the remaining observations. 8.3.4 Checking for minimum and maximum values Familiarising yourself with the dataset is crucial to be able to properly understand the data and interpret any results. One of the ways in which you can familiarise yourself with the data is to check what the minimum and maximum values are in the relevant data, for example, minimum and maximum values of elevation. This can help you to not only look out for any outlier or extreme values with predictions for example, but it can also help you to understand the scale of each data type that you are working with. The methods as to how you can check for the minimum and maximum values are discussed in Module 1, by using the min() and max() functions respectively. #minimum total population count min(Data_CMR$Total_Pop) ## [1] 19 #maximum total population count max(Data_CMR$Total_Pop) ## [1] 26689 8.3.5 Checking the distribution of categorical variables As with the other aspects of exploratory analysis, it is important to check the distribution of the data prior to modelling to be familiar with the data itself. The distribution of categorical variables was explored briefly above by demonstrating that a bar plot can be used to show the distribution of settlement types with a bar plot, however, the distribution of categorical variables can be explored using a variety of methods, including simple frequency tables using the function table(). The code below demonstrates this by producing a table of the frequencies of the different settlement types. #produce a table for frequency checking table(Data_CMR$Settlement_Type) ## ## 1 2 3 4 ## 768 167 194 463 It is also important to look at how many different categories there are within the variable. This can be done using the unique() function, which returns a vector with each of the unique categories in order of appearance in the variable. An example of this can be seen below, where the unique regions are returned, followed by the number of unique categories, found using the length() function #find the unique regions unique(Data_CMR$Regions) ## [1] 3 2 10 7 8 9 1 5 6 4 #find out how many different regions there are length(unique(Data_CMR$Regions)) ## [1] 10 8.4 Model set-up This section repeats many of the methods discussed in Module 4 such as simple linear regression, multiple regression and GLM stepwise regression with application to the Cameroon dataset, before exploring more complex methods such as a Bayesian approach and introducing spatial covariates. The first step is to identify the response variable (and the independent variables if necessary) of interest. In the case of the Cameroon dataset, the response variable is the population density. The distribution of the response variable can be checked through plotting a histogram. In this instance, the log-transformed population density is normally distributed which can be identified through the bell-curve shape of the histogram. #check the distribution Data_CMR &lt;- Data_CMR %&gt;% mutate(Density = Total_Pop/Total_Building_Count) hist(Data_CMR$Density, xlab = &quot;Population density&quot;, main =&quot;Histogram of population density&quot;, breaks = 20) hist(log(Data_CMR$Density), xlab = &quot;log-transformed population density&quot;, main =&quot;Histogram of log-transformed population density&quot;, breaks = 20) Given that the log-transformed population density is normally distributed, a variable can be created to work with this density and model the data. #create the log transformed Density variable Data_CMR &lt;- Data_CMR %&gt;% mutate(LDensity = log(Density), y = LDensity) #this will be the response for the density models 8.4.1 Simple linear regression Since the log-transformed population density is normally distributed, the lm() function can be used to fit a simple linear regression model. In this instance, the simplest model, the intercept-only, model is fitted first, with outputs for the model summary, coefficients, residuals and predicted values returned also, all using the functions introduced in Module 2. #intercept only fit1 &lt;- lm(y ~ 1, data = Data_CMR) #gives all the model parameters estimates summary(fit1) ## ## Call: ## lm(formula = y ~ 1, data = Data_CMR) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.9476 -0.4811 -0.0339 0.4731 5.7481 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.12919 0.02019 55.94 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8046 on 1588 degrees of freedom ## (3 observations deleted due to missingness) #gives only the estimates of the fixed effects parameters coefficients(fit1) ## (Intercept) ## 1.129186 #gives the residuals (observed - predicted) head(resid(fit1)) ## 1 2 3 4 5 6 ## 0.00604343 -0.40488387 -0.39598028 -0.10917200 0.22603595 0.08739335 #gives the predicted values head(fitted(fit1)) ## 1 2 3 4 5 6 ## 1.129186 1.129186 1.129186 1.129186 1.129186 1.129186 8.4.2 Multiple regression Before adding covariates into the models, since the geospatial covariates are obtained based on different scales of measurements, it is important to transform the data first to standardise the covariates. This makes the interpretation of the model estimates much easier. Here, the standardisation is done through using the Z-score with a function created below to do this standardisation. #standardisation of model covariates stdise &lt;- function(x) { stds &lt;- (x - mean(x, na.rm = TRUE))/sd(x, na.rm = TRUE) return(stds) } This function created can then be applied to covariates from a given dataset using the apply() function for efficiency to standardise the selected covariates. #apply the function to standardise the covariates cov_vars &lt;- paste0(&quot;x&quot;, 1:41) Data_CMR_std &lt;- Data_CMR #standardise the covariates only Data_CMR_std[, cov_vars] &lt;- apply(Data_CMR[, cov_vars], 2, stdise) #obtain subset of the data with only the covariates and the response variable mod_covs &lt;- Data_CMR_std[,c(&quot;y&quot;, cov_vars)] #summary of covariates before and after standardisation head(Data_CMR[,cov_vars[1:5]]) ## x1 x2 x3 x4 x5 ## 1 240337.7 86203.41 413765.8 139978.61 86203.41 ## 2 338154.9 36501.53 317968.1 36501.53 36501.53 ## 3 437903.8 54406.30 278816.8 72425.01 54406.30 ## 4 572474.0 65059.10 207275.2 167467.12 65059.10 ## 5 346930.5 47410.98 334817.6 80121.03 47410.98 ## 6 344902.1 55245.77 333230.8 76245.29 55245.77 head(Data_CMR_std[,cov_vars[1:5]]) ## x1 x2 x3 x4 x5 ## 1 -0.62880443 1.10518418 1.5292650 0.2777396 0.89020837 ## 2 -0.13168532 -0.01029845 0.7477583 -0.8029266 -0.09342136 ## 3 0.37525087 0.39154679 0.4283667 -0.4277586 0.26092475 ## 4 1.05915270 0.63063252 -0.1552609 0.5648168 0.47174994 ## 5 -0.08708668 0.23454744 0.8852148 -0.3473849 0.12248313 ## 6 -0.09739513 0.41038730 0.8722700 -0.3878613 0.27753826 Using the same methods as discussed in Module 2 for simple linear and multiple regression, examples are given below with the application of the methods to the Cameroon dataset. #example of Simple linear regression (only one covariate) fit2 &lt;- lm(y ~ x3, data = Data_CMR_std) summary(fit2) ## ## Call: ## lm(formula = y ~ x3, data = Data_CMR_std) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.5403 -0.4465 -0.0012 0.4426 5.5186 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.1293 0.0192 58.81 &lt;2e-16 *** ## x3 -0.2486 0.0192 -12.95 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7655 on 1587 degrees of freedom ## (3 observations deleted due to missingness) ## Multiple R-squared: 0.09555, Adjusted R-squared: 0.09498 ## F-statistic: 167.7 on 1 and 1587 DF, p-value: &lt; 2.2e-16 #example Multiple regression (more than one covariate) fit3 &lt;- lm(y ~ x1 + x2 + x3 + x9, data = Data_CMR_std) summary(fit3) ## ## Call: ## lm(formula = y ~ x1 + x2 + x3 + x9, data = Data_CMR_std) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.4001 -0.3742 0.0056 0.3612 6.3559 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.12877 0.01779 63.454 &lt; 2e-16 *** ## x1 0.08682 0.02808 3.092 0.00202 ** ## x2 -0.08814 0.01989 -4.432 9.98e-06 *** ## x3 -0.08441 0.02677 -3.153 0.00165 ** ## x9 0.24758 0.01972 12.557 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7091 on 1584 degrees of freedom ## (3 observations deleted due to missingness) ## Multiple R-squared: 0.2253, Adjusted R-squared: 0.2234 ## F-statistic: 115.2 on 4 and 1584 DF, p-value: &lt; 2.2e-16 #multiple regression with all available covariates in the data fit4 &lt;- lm(y ~ ., data = mod_covs) #fit4 &lt;- lm(y ~ ., data = Data_CMR_std) #this will include everything else #summary(fit4) 8.4.3 Generalised linear regression As discussed in Module 4, the main difference between the lm() and glm() functions is that glm() can be used for both normal and non-normal data and therefore, the distribution of the response must be specified when glm() is used. To use the glm() function with stepwise regression to select the best fitting model with covariates, the MASS package must be loaded. Additionally, the car package can be loaded to compute the Variance Inflation Factor (VIF). #install MASS and car packages install.packages(&quot;MASS&quot;) install.packages(&quot;car&quot;) #load MASS and car packages library(MASS) library(car) The first step is to ensure that all of the NA values are removed, this can be done using the drop_na() function in the tidyr package. #remove all NA values mod_covs &lt;- mod_covs %&gt;% tidyr::drop_na() Then the glm() function can be used to model the data, in this case all of the standardised covariates are used with the continuous data belonging to the Gaussian family. As discussed in Module 4, stepwise regression can be completed through using the stepAIC() function. Below, the stepwise regression is applied to the full GLM model with the direction of the model selection going both ways. #covariates selection using GLM stepwise regression fdens &lt;- glm(y~., data = mod_covs, family = gaussian) #continuous data summary(fdens) step_dens&lt;- stepAIC(fdens, scale = 0, direction = c(&quot;both&quot;), trace = 1, keep = NULL, steps = 1000, use.start = FALSE, k = 2) The resulting best fitting model can then be copied and pasted from the output in order to refit the model and select only the statistically significant variables. #refit the model with selected covariates fdens2 &lt;- glm(formula = y ~ x1 + x2 + x3 + x7 + x9 + x10 + x11 + x12 + x13 + x14 + x15 + x16 + x19 + x20 + x21 + x24 + x28 + x31 + x32 + x36 + x38 + x40 + x41, family = gaussian, data = mod_covs) summary(fdens2) ## ## Call: ## glm(formula = y ~ x1 + x2 + x3 + x7 + x9 + x10 + x11 + x12 + ## x13 + x14 + x15 + x16 + x19 + x20 + x21 + x24 + x28 + x31 + ## x32 + x36 + x38 + x40 + x41, family = gaussian, data = mod_covs) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.12178 0.01604 69.935 &lt; 2e-16 *** ## x1 0.94340 0.14009 6.734 2.34e-11 *** ## x2 -0.14578 0.02708 -5.384 8.45e-08 *** ## x3 -0.21609 0.04018 -5.378 8.72e-08 *** ## x7 0.08323 0.03197 2.604 0.009317 ** ## x9 -0.23291 0.14283 -1.631 0.103161 ## x10 0.26976 0.15722 1.716 0.086398 . ## x11 -0.35442 0.09667 -3.666 0.000255 *** ## x12 -0.43686 0.10530 -4.149 3.53e-05 *** ## x13 0.46231 0.12020 3.846 0.000125 *** ## x14 0.61476 0.11219 5.480 4.98e-08 *** ## x15 0.96766 0.14963 6.467 1.35e-10 *** ## x16 -0.03775 0.02051 -1.841 0.065866 . ## x19 -0.05983 0.03313 -1.806 0.071132 . ## x20 0.06331 0.03173 1.995 0.046168 * ## x21 0.35461 0.05045 7.029 3.15e-12 *** ## x24 -0.08049 0.03392 -2.373 0.017773 * ## x28 0.09290 0.03862 2.405 0.016278 * ## x31 -0.05528 0.01974 -2.800 0.005170 ** ## x32 0.06524 0.02198 2.968 0.003046 ** ## x36 -0.08218 0.02789 -2.947 0.003261 ** ## x38 -0.03024 0.02010 -1.504 0.132741 ## x40 -0.12636 0.03092 -4.087 4.61e-05 *** ## x41 0.17710 0.06440 2.750 0.006031 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 0.3922937) ## ## Null deviance: 990.78 on 1537 degrees of freedom ## Residual deviance: 593.93 on 1514 degrees of freedom ## AIC: 2951.3 ## ## Number of Fisher Scoring iterations: 2 The Variance Inflation Factors (VIF) of the covariates can then be computed with the vif() function, where in this case, a VIF of less than 5 is acceptable. #check the VIF values for the refitted model vif_dens = vif(fdens2) #extract the covariates with VIF values of less than 5 vif_dens[which(vif_dens &lt; 5)] ## x2 x7 x16 x19 x20 x24 x31 x32 x36 x38 x40 ## 2.839034 3.966596 1.664360 4.373456 3.961965 4.188336 1.546725 1.914355 2.978645 1.607727 3.836262 The GLM can be refitted again, this time with only the covariates which have a VIF value of less than 5. #refit the model with covariates that have a small VIF fdens3 &lt;- glm(formula = y ~ x2 + x7 + x16 + x19 + x20 + x24 + x31 + x32 + x36 + x38 + x40, family = gaussian, data = mod_covs) summary(fdens3) ## ## Call: ## glm(formula = y ~ x2 + x7 + x16 + x19 + x20 + x24 + x31 + x32 + ## x36 + x38 + x40, family = gaussian, data = mod_covs) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.121989 0.017528 64.012 &lt; 2e-16 *** ## x2 -0.123374 0.022846 -5.400 7.71e-08 *** ## x7 0.017466 0.020369 0.857 0.391311 ## x16 -0.103893 0.020735 -5.010 6.06e-07 *** ## x19 0.001239 0.031963 0.039 0.969089 ## x20 0.102426 0.032859 3.117 0.001860 ** ## x24 0.056417 0.020008 2.820 0.004868 ** ## x31 -0.079566 0.020014 -3.975 7.35e-05 *** ## x32 -0.001893 0.020948 -0.090 0.928024 ## x36 -0.216090 0.022026 -9.811 &lt; 2e-16 *** ## x38 -0.004909 0.020655 -0.238 0.812185 ## x40 0.089437 0.024187 3.698 0.000225 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 0.4715244) ## ## Null deviance: 990.78 on 1537 degrees of freedom ## Residual deviance: 719.55 on 1526 degrees of freedom ## AIC: 3222.4 ## ## Number of Fisher Scoring iterations: 2 The summary output of the model identifies which of the covariates are significant and which are not. Since the best model will include only covariates which are statistically significant (to the 5% level), the model should be refit with only these covariates. #do the final refit with the covariates that are significant in the summary fdens4 &lt;- glm(formula = y ~ x2 + x16 + x20 + x24 + x31 + x36 + x40, family = gaussian, data = mod_covs) summary(fdens4) ## ## Call: ## glm(formula = y ~ x2 + x16 + x20 + x24 + x31 + x36 + x40, family = gaussian, ## data = mod_covs) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.12154 0.01750 64.079 &lt; 2e-16 *** ## x2 -0.11604 0.02002 -5.797 8.20e-09 *** ## x16 -0.10718 0.01982 -5.409 7.36e-08 *** ## x20 0.10585 0.01882 5.624 2.22e-08 *** ## x24 0.05268 0.01926 2.735 0.00631 ** ## x31 -0.08059 0.01904 -4.233 2.44e-05 *** ## x36 -0.21498 0.02169 -9.911 &lt; 2e-16 *** ## x40 0.09536 0.02302 4.142 3.62e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 0.4705534) ## ## Null deviance: 990.78 on 1537 degrees of freedom ## Residual deviance: 719.95 on 1530 degrees of freedom ## AIC: 3215.2 ## ## Number of Fisher Scoring iterations: 2 Finally, return the VIF for the last covariates and double check that they are all below 5, then the final best covariates can be selected. #rerun VIF for the last covariates vif_dens4 = vif(fdens4) vif_dens4[which(vif_dens4 &lt; 5)] ## x2 x16 x20 x24 x31 x36 x40 ## 1.293601 1.295430 1.162679 1.126268 1.199379 1.502057 1.772468 #identify the best covariates best_covs &lt;- c(&quot;x2&quot;, &quot;x16&quot;, &quot;x20&quot;, &quot;x24&quot;, &quot;x31&quot;, &quot;x36&quot;, &quot;x40&quot;) 8.4.4 Fitting fixed effects models in R-INLA As seen in previous modules (see Module 4), the mathematical model specification for the fixed effects model is as follows. \\[\\eta_i = \\log(\\lambda_i) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\cdots + \\beta_Mx_M,\\] which can also be written as \\[ \\eta_i = \\beta_0 + \\sum_{m=1}^M \\beta_mx_{im},\\] where in this case, the covariates \\(x_m\\) are geospatial covariates, \\(\\lambda\\) is the mean parameter and \\(\\eta\\) is the linear predictor. In INLA, the model specification for the above model is given in the form model &lt;- response ~ x_1 + x_2 +... x_M. For demonstration of simple models in INLA, see Module 6. 8.4.5 Fitting Bayesian hierarchical mixed effects models in R-INLA Similarly to with the fixed effects models, the mathematical model specification for the mixed effects model can be given as \\[\\log(\\lambda_i) = \\beta_0 + \\sum_{m=1}^M \\beta_mx_{im} + EA_i,\\] which in INLA is given as model &lt;- response ~ x_1 + x_2 +... x_M + f(EA, model = \"iid\"). The way in which the inla() function is used for the mixed effects models is the same as for the fixed effects models with the exception of the formula specification. Applying this methodology to the Cameroon data, using settlement type as a random variable can be seen in the code below. First the variable for settlement type is converted to a factor, then the formula is specified and finally the inla() function is used. In this case, the family is set to \"gaussian\". #load the INLA package library(INLA) #convert settlement type to a factor Data_CMR_std$set_typ &lt;- factor(Data_CMR_std$Settlement_Type) #specify the formula formula1 &lt;- y ~ 1 + x2 + x16 + x20 + x24 + x31 + x36 + x40 + f(set_typ, model = &quot;iid&quot;) #create the INLA model res1 &lt;- inla(formula1, data = Data_CMR_std, family = &quot;gaussian&quot;, control.predictor = list(compute = TRUE), control.compute = list(dic = TRUE) ) summary(res1) ## Time used: ## Pre = 0.49, Running = 0.588, Post = 0.204, Total = 1.28 ## Fixed effects: ## mean sd 0.025quant 0.5quant 0.975quant mode kld ## (Intercept) 1.048 0.084 0.876 1.049 1.213 1.049 0.001 ## x2 -0.127 0.019 -0.165 -0.127 -0.089 -0.127 0.000 ## x16 -0.089 0.020 -0.127 -0.089 -0.051 -0.089 0.000 ## x20 0.100 0.018 0.064 0.100 0.137 0.100 0.000 ## x24 0.050 0.018 0.015 0.050 0.086 0.050 0.000 ## x31 -0.053 0.021 -0.093 -0.053 -0.013 -0.053 0.000 ## x36 -0.196 0.021 -0.237 -0.196 -0.155 -0.196 0.000 ## x40 0.018 0.025 -0.030 0.018 0.066 0.018 0.000 ## ## Random effects: ## Name Model ## set_typ IID model ## ## Model hyperparameters: ## mean sd 0.025quant 0.5quant 0.975quant mode ## Precision for the Gaussian observations 2.19 0.078 2.04 2.19 2.35 2.19 ## Precision for set_typ 62.04 46.200 11.69 49.97 183.02 30.43 ## ## Deviance Information Criterion (DIC) ...............: 3279.39 ## Deviance Information Criterion (DIC, saturated) ....: 1603.11 ## Effective number of parameters .....................: 11.73 ## ## Marginal log-Likelihood: -1705.96 ## is computed ## Posterior summaries for the linear predictor and the fitted values are computed ## (Posterior marginals needs also &#39;control.compute=list(return.marginals.predictor=TRUE)&#39;) Once you have the model output, the fixed and random effects can be extracted separately, using the $ with the arguments summary.fixed for the fixed effects and sumary.random for the random effects. Additionally, the hyperparameters (for the EA) can be obtained through extracting the summary.hyperpar argument. #extract the fixed effects only res1$summary.fixed ## mean sd 0.025quant 0.5quant 0.975quant mode kld ## (Intercept) 1.04809495 0.08373841 0.87621515 1.04947658 1.21304635 1.04948155 6.778342e-04 ## x2 -0.12680156 0.01926844 -0.16458900 -0.12680171 -0.08901328 -0.12680171 2.464670e-12 ## x16 -0.08881522 0.01953135 -0.12712372 -0.08881346 -0.05051674 -0.08881345 4.366662e-12 ## x20 0.10033415 0.01848780 0.06407699 0.10033424 0.13659081 0.10033424 2.479269e-12 ## x24 0.05048589 0.01795358 0.01527733 0.05048565 0.08569582 0.05048565 2.461501e-12 ## x31 -0.05300326 0.02063559 -0.09345719 -0.05300846 -0.01251972 -0.05300849 1.985144e-11 ## x36 -0.19622302 0.02092853 -0.23726956 -0.19622196 -0.15518254 -0.19622195 3.112967e-12 ## x40 0.01779198 0.02456362 -0.03034851 0.01778042 0.06599825 0.01778038 5.689345e-11 #extract the random effects only res1$summary.random ## $set_typ ## ID mean sd 0.025quant 0.5quant 0.975quant mode kld ## 1 1 0.24113074 0.08899076 0.07321712 0.23792481 0.42730761 0.23797400 0.0004331911 ## 2 2 -0.05221826 0.09080821 -0.23524821 -0.05161731 0.12771649 -0.05161399 0.0003582377 ## 3 3 -0.14661778 0.09051006 -0.33364043 -0.14422077 0.02718301 -0.14427934 0.0003739361 ## 4 4 -0.04229985 0.08824968 -0.21991420 -0.04194413 0.13353841 -0.04193628 0.0004488043 #round the values to 4 decimal places round(res1$summary.random$set_typ[,c(2,4,6)], 4) ## mean 0.025quant 0.975quant ## 1 0.2411 0.0732 0.4273 ## 2 -0.0522 -0.2352 0.1277 ## 3 -0.1466 -0.3336 0.0272 ## 4 -0.0423 -0.2199 0.1335 #extract the hyperparameters res1$summary.hyperpar ## mean sd 0.025quant 0.5quant 0.975quant mode ## Precision for the Gaussian observations 2.189257 0.07789163 2.039378 2.188058 2.346022 2.186062 ## Precision for set_typ 62.040814 46.20041287 11.688763 49.966517 183.018971 30.428360 Similarly to as in Module 6 with the fixed effects models in INLA, the predicted values for the Bayesian hierarchical mixed effects models can be obtained through extracting the summary.fitted.values argument, adding further specification for if you want to obtain the predicted means (mean), or the upper (0.975quant) and lower (0.025quant) bounds of the 95% credible intervals. 8.5 STAN (MCMC) approach Markov Chain Monte Carlo (MCMC) methods are a class of algorithms that are used to sample from the joint distribution, comprised of two main parts. Firstly, Monte Carlo, which is a mathematical method for drawing samples from a distribution to obtain parameter estimates. The second part is a Markov Chain which is a sequence of correlated samples in which a future value only depends on the present given the past. The basic premise is to perform Monte Carlo simulations and estimation on Markov Chains of the posterior distribution. Alternatively, to simulate a Markov chain with the stationary distribution given by the target (posterior) distribution, and after a number of iterations, sampling from this Markov chain repeatedly, samples from the joint distribution can be obtained. For more information on Markov chains, see Markov Chains Setosa where they are explained visually. Whilst there are a number of different algorithms proposed, the Metropolis-Hastings algorithm can be used with Bayesian inference. 8.5.1 Metropolis-Hastings algorithm To obtain new values for the different parameters, the Metropolis-Hastings algorithm uses a proposal distribution. The basic steps are as follows. Arbitrarily select a starting point \\(\\boldsymbol{z}^{(0)}\\). Let \\(q\\) be the proposal distribution and \\(\\boldsymbol{y}\\) be a candidate point. Generate \\(\\boldsymbol{y}\\) from \\(q(\\boldsymbol{y}|\\boldsymbol{z}^{(t)})\\). Calculate the acceptance probability \\[\\alpha(\\boldsymbol{z}^{(t)}, \\boldsymbol{y}) = min \\left\\{1, \\frac{p(\\boldsymbol{y})q(\\boldsymbol{z}^{(t)}|\\boldsymbol{y})}{p(\\boldsymbol{z}^{(t)})q(\\boldsymbol{y}|\\boldsymbol{z}^{(t)})}\\right\\}\\] Accept \\(\\boldsymbol{z}^{(t+1)} =\\boldsymbol{y}\\) with probability \\(\\alpha(\\boldsymbol{z}^{(t)}, \\boldsymbol{y})\\) otherwise, set \\(\\boldsymbol{z}^{(t+1)} = \\boldsymbol{z}^{(t)}\\) It is important to note the involvement of \\(p\\), the target density, in that it is only present through the ratio \\(\\frac{p(y)}{p(\\boldsymbol{z}^{(t)})}\\). Therefore, the normalising constant does not need to be known to implement the algorithm since it cancels in the ratio. \\(\\alpha (\\boldsymbol{z},\\boldsymbol{y}) = min \\left\\{1, \\frac{p(\\boldsymbol{y})}{p(\\boldsymbol{z})} \\right\\}\\) There are three special cases of the Metropolis-Hastings algorithm detailed below. Metropolis algorithm: \\(q(\\boldsymbol{y}|\\boldsymbol{z})=q(\\boldsymbol{z}|\\boldsymbol{y})\\) - This is where the proposal density is symmetric. It has a special case called the Random-Walk Metropolis algorithm where \\(q(\\boldsymbol{y}|\\boldsymbol{z})=q(|\\boldsymbol{y}-\\boldsymbol{z}|)\\) leading to \\(\\alpha (\\boldsymbol{z},\\boldsymbol{y}) = min \\left\\{1, \\frac{p(\\boldsymbol{y})}{p(\\boldsymbol{z})} \\right\\}\\). Independence sampler: \\(q(\\boldsymbol{y}|\\boldsymbol{z})=g(\\boldsymbol{y})\\) - Draw proposals from a fixed density \\(g\\), which can be chosen as \\(g=p\\) to get independent samples, leading to \\(\\alpha(\\boldsymbol{z},\\boldsymbol{y})=1)\\) and no chain will be rejected as a candidate. It is important to note that in general however, \\(p\\) is unknown and that in order for the chain to converge to the stationary distribution, the tails of \\(g\\) must be heavier than the tails of \\(p\\). Gibbs sampler: \\(\\alpha(\\boldsymbol{z},\\boldsymbol{y})=1\\) - This is when the acceptance probability is equal to 1. For more information on the Metropolis-Hastings algorithm and its use in stan, see here. 8.6 INLA and INLA-SPDE approach To fit geostatistical models using INLA-SPDE, the following steps should be followed. Load the data Specify the coordinates Build the mesh Construct the SPDE Build projection matrix A Define the data location indices Stack the data Specify and fit the appropriate model Run posterior checks and model fit checks Carry out model predictions 8.6.1 Mesh construction The SPDE approach approximates the continuous Gaussian field as a discrete Gaussian Markov random field by means of a finite basis function defined on a triangulated mesh of the region of study. In other words, it discretises the continuous Gaussian surface using a triangulated mesh. A triangulated mesh can be constructed to perform this approximation with the function inla.mesh.2d() from the R-INLA package which constructs a 2D mesh over a specified boundary. This mesh is important for spatial modelling as it converts the study area into smaller elements (triangles) that can be used in faster computational burdens. The arguments of this function include: loc: the data location coordinates that are used as initial mesh vertices. boundary: an object describing the boundary of the domain. offset: the distance that specifies the size of the inner and outer extensions around the data locations. It controls the padding around the boundary to ensure that edge effects do not influence the analysis. cutoff: the minimum allowed distance between points. -It helps in reducing the number of mesh points by ignoring points that are too close to each other, which can improve computational efficiency. In other words, it is used to avoid building many small triangles around clustered locations. max.edge: the values denoting the maximum allowed triangle edge lengths in the region and in the extension. It controls the granularity of the mesh. Smaller values lead to finer meshes with more triangles providing more resolution so more detail of the spatial variation can be captured. It should not be too small to control the computation time to run the INLA model. To select appropriate values of offset, cutoff and max.edge, as they depend on the specific range and distribution of the coordinates of the study dataset, it is typically a trail-and-error process. There are some steps which can be taken to aid in fine-tuning the parameters as follows. Start with the recommended values. offset = c(max.edge/2, max.edge) cutoff = max.edge/10 max.edge = c(max(diff(range(data$lon))), diff(range(data$lat)))/15 Check the mesh generation time. If the process does not generate a mesh within 1 minute, terminate it and increase the value of max.edge by decreasing the denominator from 15 (for example, use 10). In complex cases where the default values are not suitable, try alternative starting values. offset = c(0.01, 0.05) cutoff = 0.01 max.edge = c(0.05, 0.1) Use incremental adjustments. If the mesh still does not generate within 1 minute, increase the values of all three parameters by a factor of 1 (for example, offset = offset*10, cutoff cutoff*10 and max.edge = max.edge*10). Assess the fit. Once a mesh is generated, check the number of vertices with mesh$n. Visualise the mesh by plotting the non-convex hull boundary and the coordinate points. Make sure that the mesh triangles inside the non-convex hull boundary (study area) are smaller than those outside it. This will also reduce computational time. Fine tuning. Adjust the size of max.edge to improve the fit of the mesh triangles within the boundary. Smaller max.edge values will result in finer mesh triangles inside the study area. To construct the mesh, either the coordinates of the data locations or the boundary of the data locations can be used. Below, to construct the mesh using the coordinates of the locations, the function inla.mesh.2d() is called setting loc equal to the matrix with the coordinates of the recording stations (coords). The offset is then specified as offset = c(50, 100) to have an inner extension of size 50, and an outer extension of size 100 around the locations. Then set cutoff = 1 to avoid building many small triangles where we have some very close points. Finally, set max.edge = c(30, 60) to use small triangles within the region, and larger triangles in the extension. #method 1: using coordinates of the locations summary(dist(coords)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.000 1.679 2.784 3.541 5.527 11.214 mesh &lt;- inla.mesh.2d( loc = coords, #coordinates of the spatial domain offset = c(0.1, 1), #adjusts the distance between the outer and inner meshes cutoff = 0.03, max.edge = c(0.05, 1)) #adjusts the mesh nodes sizes between the inner and #outer meshes #plot the mesh plot(mesh) #add the points for coordinates points(coords, col = &quot;red&quot;) #extract the number of vertices (controls the minimum size of triangles allowed) mesh$n ## [1] 48991 Alternatively, the boundary of the regions can be used to construct the mesh. To do this method, first the boundary needs to be created with the function inla.nonconvex.hull() where the following arguments are available to be included. The points argument is necessary to include, but the other arguments are not necessary to include in the function if the default values are desired. points: the 2d point coordinates. convex: the desired extension radius with default value -0.05. This parameter controls how much the hull contracts around the points (the “tightness” of the hull). A smaller (more negative) value corresponds to a tighter hull which hugs the data points closer. This is beneficial when the goal is to avoid including empty spaces within the hull. concave: the desired minimum concave curvature radius with default value equal to the convex value ( = -0.05 if no convex value given). This parameter influences how much the hull expands to account for concavities (indentations in the boundary). A smaller (more negative) value corresponds with a hull that expands more to include concavities. This is beneficial when the dataset has significant indentations that need to be included within the boundary. resolution: the internal computation resolution with default vector c(100,100). This argument indicates the grid resolution that will be used for the hull calculation. If the default values are not used for the above arguments, then the process of selecting the appropriate concave and convex values is typically trial-and-error since they depend on the specific range and distribution of the coordinates present in the chosen dataset. However, there are some steps that can be followed to aid in the selection process. Start with the default values. Plot the hull and coordinates, typically with the following code. #plot the hull and coordinates plot(non_convex_boundary1$loc) points(data$long, data$lati, col = &quot;red&quot;) Assess the fit. - Ensure that all the coordinates are included within the hull. - Check that there is minimal empty space between the points and the hull boundary. Repeat steps 2 and 3, adjusting the convex and concave values each time to improve the fit, until the optimal hull is achieved that fits the data closely and does not have excessive empty space. #method 2: using boundary of the regions bnd &lt;- inla.nonconvex.hull(coords, -0.03, -0.05, resolution = c(100, 100)) meshb &lt;- inla.mesh.2d(boundary = bnd, max.edge = c(0.1, 1), #adjusts the mesh nodes sizes between #the inner and outer meshes offset = c(0.05, 1), #adjusts the distance between the outer and inner meshes cutoff = 0.003) #controls the minimum size of triangles allowed #plot the mesh plot(meshb) #add the points for coordinates points(coords, col = &quot;red&quot;) #extract the number of vertices mesh$n ## [1] 48991 8.6.2 Building the SPDE The function inla.spde2.matern() can be used to build the SPDE model on the mesh passing mesh and alpha as arguments. The alpha argument is related to the smoothness parameter of the process, namely \\(\\alpha = v + d/2\\). In this example, the smoothness parameter is set to be equal to 1 and in the spatial case, \\(d=2\\) so \\(\\alpha = 1+ 2/2 = 2\\). The constraint argument is then set as const = TRUE to impose an integrate-to-zero constraint. #build the SPDE spde &lt;- inla.spde2.matern(mesh = mesh, alpha = 2, constr = TRUE) 8.6.3 Projection matrix A projection matrix A needs to be constructed to project the GRF from the observations to the triangulations vertices (for the prediction). The matrix A has the number of rows equal to the number of observations, and the number of columns equal to the number of vertices of the triangulation. Row of A corresponding to an observation at location possibly has three non-zero values at the columns that correspond to the vertices of the triangle that contains the location. If is within the triangle, these values are equal to the barycentric coordinates. That is, they are proportional to the areas of each of the three sub-triangles defined by the location \\(s_i\\) and the triangle’s vertices, and sum to 1. If \\(s_i\\) is equal to a vertex of the triangle, row \\(i\\) has just one non-zero value equal to 1 at the column that corresponds to the vertex. Intuitively, the value \\(Z(s)\\) at a location that lies within one triangle is the projection of the plane formed by the triangle vertices weights at location s. An example of a projection matrix is given below. This projection matrix projects \\(n\\) observations to G triangulation vertices. The first row of the matrix corresponds to an observation with location that coincides with vertex number 3. The second and last rows correspond to observations with locations lying within triangles. R-INLA provides the inla.spde.make.A() function to easily construct a projection matrix A. We create the projection matrix of our example by using inla.spde.make.A() passing the triangulated mesh mesh and the coordinates coo. #construct projection matrix A &lt;- inla.spde.make.A(mesh = mesh, loc = coords) We can type dim(A) to see if A has the number of rows equal to the number of observations (number of rows in the data), and the number of columns equal to the number of mesh nodes (mesh$n). 8.6.4 Data location indexing The next step is to generate the index set of the SPDE model. This is done through using the function inla.spde.make.index() where the name of the effect(s) and the number of vertices in the SPDE model (found using spde$n.spde) are specified. #generate the index set indexs &lt;- inla.spde.make.index(&quot;s&quot;, spde$n.spde) 8.6.5 Projection data The next step is to construct a matrix with the coordinates of the locations where the prediction for certain values (for example, rainfall) take place. To do this, firstly a grid is constructed using the function expand.grid(), combining the vectors x and y, then converting to a matrix using the as.matrix() function. In this example, the result is called coop, with 50×50 locations. Once the matrix is created, the function point.in.polygon() is used to find the indices of the points which are contained within the polygons. The matrix can then be updated to only keep the points which are in the polygons. #import the shapefile data ea_don_shp &lt;- st_read(paste0(data_path, &quot;/Shapefiles/CMR_Boundary.shp&quot;)) ## Reading layer `CMR_Boundary&#39; from data source ## `F:\\Study\\WorldPop Training Materials\\GitBook\\wp_training_manual\\data\\CMR\\Shapefiles\\CMR_Boundary.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 1 feature and 4 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 945997.2 ymin: 182845.8 xmax: 1802307 ymax: 1458913 ## Projected CRS: WGS 84 / World Mercator #plot the data plot(ea_don_shp$geometry) #load the sp package library(sp) #convert sf object to spatial object sp_object &lt;- as(ea_don_shp, &quot;Spatial&quot;) #create the bounding box bb &lt;- bbox(sp_object) bb ## min max ## x 945997.2 1802307 ## y 182845.8 1458913 #set the coordinates x &lt;- seq(bb[1, &quot;min&quot;] - 1, bb[1, &quot;max&quot;] + 1, length.out = 50) y &lt;- seq(bb[2, &quot;min&quot;] - 1, bb[2, &quot;max&quot;] + 1, length.out = 50) coop &lt;- as.matrix(expand.grid(x, y)) #Data stacking To organise the data, effects and projection matrices, the function inla.stack() is used with the following arguments. tag: a string for identifying the data. data: a list of data vectors. A: a list of projection matrices. effects: a list with fixed and random effects. A stack with data for estimation (stk.e) is created and is tagged with the string \"est\". In this stack, the fixed effects are the intercept (b0) and the random effect is the spatial Gaussian Random Field (s). Therefore, in effects a list with a data.frame with the fixed effects, and a list s containing the indices of the SPDE object (indexs) is passed. A is set to a list where the second element is A, the projection matrix for the random effects, and the first element is 1 to indicate the fixed effects are mapped one-to-one to the response. In data, the response vector is specified. Additionally, a stack with data for prediction (stk.p) is created, this time it is tagged with the string pred. In this stack, the response vector is set to NA and the data is specified at the prediction locations. Finally, a full stack (stk.full) is created through using the function inla.stack() to put the stacks for estimation and prediction together. The following code demonstrates this process. #create a variable for the density Data_CMR_std$Density &lt;- Data_CMR_std$Total_Pop/Data_CMR_std$Total_Building_Count #select covariates based on earlier model selection covs &lt;- Data_CMR_std %&gt;% dplyr::select(x2, x16, x20, x24, x31, x36, x40) #stack the covariates stk.e &lt;- inla.stack(data = list(y = Data_CMR_std$Density), #response variable A = list(A,1), #A matrix; the 1 is included to make the #list(covariates) effects = list(c(list(Intercept = 1), #the Intercept indexs), #the spatial index list(covs) #the covariates ), tag = &#39;est&#39;) #quick name to call upon easily #create a projection matrix for the prediction Ap &lt;- inla.spde.make.A(mesh = mesh, loc = coop) dim(Ap) ## [1] 2500 48991 #stack for prediction (stk.p) stk.p &lt;- inla.stack(tag = &quot;pred&quot;, data = list(y = NA), A = list(1, Ap), effects = list(data.frame(b0 = rep(1, nrow(coop))), s = indexs) ) #stk.full has both stk.e and stk.p stk.full &lt;- inla.stack(stk.e, stk.p) #Model specification and fitting To specify and fit the model with spatially varying random effects in R, the code introduced in the earlier section for model set-up can be used. For convenience, this code is repeated here. #specify the formula formula &lt;- y ~ 0 + b0 + f(s, model = spde) #create the INLA model res &lt;- inla(formula, data = inla.stack.data(stk.full), control.predictor = list( compute = TRUE, A = inla.stack.A(stk.full))) #generate the index set index &lt;- inla.stack.index(stk.full, tag = &quot;pred&quot;)$data #obtain predicted values for the mean and quantiles for 95% credible intervals pred_mean &lt;- res$summary.fitted.values[index, &quot;mean&quot;] pred_ll &lt;- res$summary.fitted.values[index, &quot;0.025quant&quot;] pred_ul &lt;- res$summary.fitted.values[index, &quot;0.975quant&quot;] #Project to new areas Vectors res$summary.random$s$mean and res$summary.random$s$sd contain the posterior mean and the posterior standard deviation, respectively, of the spatial field at the mesh nodes. These values can be projected at different locations by computing a projection matrix for the new locations, followed by multiplying the projection matrix by the spatial field values. For example, to compute the posterior mean of the spatial field at locations in the matrix newloc, the following code can be used. #set the new locations newloc &lt;- cbind(c(219, 678, 818), c(20, 20, 160)) #create the projection matrix Aproj &lt;- inla.spde.make.A(mesh, loc = newloc) #obtain the posterior mean Aproj %*% res$summary.random$s$mean ## 3 x 1 Matrix of class &quot;dgeMatrix&quot; ## [,1] ## [1,] 0 ## [2,] 0 ## [3,] 0 Additionally, the functions inla.mesh.projector() and inla.mesh.project() can be used to project the spatial field values at different locations. To do so, firstly, the inla.mesh.projector() function is needed to compute a projection matrix for the new locations, specifying either the locations in the argument loc or computing the locations on a grid by specifying the arguments xlim, ylim and dims. For example, to compute a projection matrix for 300x300 locations on a grid that covers the mesh region, the following code can be used. #obtain the range rang &lt;- apply(mesh$loc[, c(1, 2)], 2, range) #project the spatial field values proj &lt;- inla.mesh.projector(mesh, xlim = rang[, 1], ylim = rang[, 2], dims = c(300, 300)) Following this, the function inla.mesh.project() is used to project the posterior mean and the posterior standard deviation of the spatial field calculated at the mesh nodes to the grid locations. #project the posterior mean mean_s &lt;- inla.mesh.project(proj, res$summary.random$s$mean) #project the posterior standard deviation sd_s &lt;- inla.mesh.project(proj, res$summary.random$s$sd) The projected values can be plotted suing the ggplot2 package in R. To do so, first create a data frame with the coordinates of the grid locations and the spatial field values. the coordinates of the grid locations can be obtained by combining proj$x and proj$y, done through using the function expand.grid(). The values of the posterior mean of the spatial field are in the matrix mean_s and the values of the posterior standard deviation of the spatial field are in the matrix st_s. #load the viridis and cowplot packages library(viridis) library(cowplot) #prepare the data df &lt;- expand.grid(x = proj$x, y = proj$y) df$mean_s &lt;- as.vector(mean_s) df$sd_s &lt;- as.vector(sd_s) #plot the projected values - mean gmean &lt;- ggplot(df, aes(x = x, y = y, fill = mean_s)) + geom_raster() + scale_fill_viridis(na.value = &quot;transparent&quot;) + coord_fixed(ratio = 1) + theme_bw() #plot the predicted values - standard deviation gsd &lt;- ggplot(df, aes(x = x, y = y, fill = sd_s)) + geom_raster() + scale_fill_viridis(na.value = &quot;transparent&quot;) + coord_fixed(ratio = 1) + theme_bw() plot_grid(gmean, gsd) Useful resources Open Buildings: Google Building Footprints: Microsoft Gridded building intensity: Planet Modelling in R: R for Data Science R-INLA package: Geospatial Health Data: Modeling and Visualization with R-INLA and Shiny Mixed-effects models with INLA: Bayesian inference with INLA Markov Chain Monte Carlo: Setosa Metropolis-Hastings algorithm: Bayes Rules Book "],["model-fit-checks-and-cross-validation.html", "9 Model Fit Checks and Cross-Validation 9.1 Model assumption checking 9.2 Model selection 9.3 Cross-validation", " 9 Model Fit Checks and Cross-Validation This module expands on the methods discussed in earlier modules, detailing the model assumption checking methods, and different techniques for model selection. Additionally, this module covers cross-validation methods in more detail than in Module 4. 9.1 Model assumption checking For this section, the data and model used follow on from Module 8, where the Cameroon data is explored and modelled. The resulting best model has covariates: \\(x2, x16, x20, x24, x31, x36, x40\\), which are used in the model fdens4. For more clarification, check the model set-up section of Module 8. Once the chosen model is fitted, as mentioned in Module 4, it is important to check the validity of the model assumptions. One method of doing this is to obtain the model residuals and plot a histogram of the residuals to test for normality. In the histogram below, there is evidence that the normality assumption is upheld with an approximate bell-curve shape to the histogram that is roughly symmetric. #get the model residuals model_residuals = fdens4$residuals #plot the histogram of the residuals to test for the normality of the residuals hist(model_residuals, col = &quot;#004C92&quot;, breaks = 20, main = &quot;Histogram of model residuals&quot;, xlab = &quot;Model residuals&quot;) Alternatively, a quantile-quantile, or Q-Q, plot can be used to test for the normality of the residuals with the functions qqnorm() and qqline(). #plot the Q-Q plot of the residuals to test for the normality of the residuals qqnorm(model_residuals) #plot the Q-Q line qqline(model_residuals) The homoscedasticity assumption, or the constant variance assumption, can be checked by plotting the residuals with the basic plot() function, adding a horizontal line to the plot with the abline() function to aid in the interpretation of results. #check for constant variance assumption (homoscedasticity) plot(model_residuals) abline(h = 0, col = 2, lwd = 3) It is important that this assumption is met, as if not, the resulting standard errors may be biased leading to unreliable inferences from the coefficients. If the assumption of homoscedasticity is not met (the case of heteroscedasticity, the variance is not constant), robust standard errors can be computed, or the dependent variable can be transformed using weighted least squares (WLS) regression. For information on these methods, see Andrew Heiss and R-bloggers. To further demonstrate that this model is the preferred model, the generalised linear model with the selected significant covariates can be fitted against a nested (reduced) model. In the below code, a nested model is compared to the best model using the AIC where it can be seen that the AIC value for the model with the significant covariates found from the above workings is lower than for the nested model. #fit the glm fit1 &lt;- glm(formula = y ~ x2 + x16 + x20 + x24, family = gaussian, data = Data_CMR_std) summary(fit1) ## ## Call: ## glm(formula = y ~ x2 + x16 + x20 + x24, family = gaussian, data = Data_CMR_std) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4.55781 0.03114 -146.374 &lt; 2e-16 *** ## x2 -0.34434 0.03250 -10.595 &lt; 2e-16 *** ## x16 -0.30925 0.03218 -9.611 &lt; 2e-16 *** ## x20 0.11053 0.03291 3.359 0.000801 *** ## x24 0.21823 0.03163 6.899 7.53e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 1.540658) ## ## Null deviance: 2903.7 on 1588 degrees of freedom ## Residual deviance: 2440.4 on 1584 degrees of freedom ## (3 observations deleted due to missingness) ## AIC: 5203.2 ## ## Number of Fisher Scoring iterations: 2 #best model fit2 &lt;- glm(formula = y ~ x2 + x16 + x20 + x24 + x31 + x36 + x40, family = gaussian, data = Data_CMR_std) summary(fit2) ## ## Call: ## glm(formula = y ~ x2 + x16 + x20 + x24 + x31 + x36 + x40, family = gaussian, ## data = Data_CMR_std) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4.55818 0.02961 -153.932 &lt; 2e-16 *** ## x2 -0.25674 0.03343 -7.680 2.78e-14 *** ## x16 -0.14195 0.03342 -4.247 2.29e-05 *** ## x20 0.16773 0.03194 5.251 1.71e-07 *** ## x24 0.10987 0.03130 3.510 0.000461 *** ## x31 -0.02331 0.03233 -0.721 0.470987 ## x36 -0.32647 0.03599 -9.070 &lt; 2e-16 *** ## x40 0.18504 0.03901 4.743 2.29e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 1.393316) ## ## Null deviance: 2903.7 on 1588 degrees of freedom ## Residual deviance: 2202.8 on 1581 degrees of freedom ## (3 observations deleted due to missingness) ## AIC: 5046.4 ## ## Number of Fisher Scoring iterations: 2 9.2 Model selection When selecting the best model (out of the competing models), it is important to select the model with the highest predictive ability. Model selection is introduced in Module 4, where methods such as the AIC, BIC and LRT are used. For example, the AIC can be used with the generalised linear models fitted in the previous section as follows. #compute the AIC statistics AIC(fit1) ## [1] 5203.159 AIC(fit2) ## [1] 5046.416 It can be seen that fit2 has a notably smaller AIC value and therefore, that is the model which should be selected. However, these are not the only approaches that are available, with other methods more commonly used for Bayesian model selection. 9.2.1 Watanabe-Akaike Information Criterion (WAIC) The WAIC (also known as the Widely Applicable information criterion) is a generalised version of the AIC that is commonly used for Bayesian model selection. This approach is often preferred when comparing Bayesian and complex models as instead of averaging over a point estimate (like the AIC and BIC), it averages over the posterior distribution, returning more robust and accurate results. A value that can be used to compare the competing models is constructed through combining a penalty term derived from the estimated effective number of parameters with the expected log predictive densities for all the data points. When the data is not correlated, this method can be used as an alternative to cross-validation, and the method is only appropriate if there is no spatial or temporal auto-correlation. See Nature or Columbia for more information. When fitting a model with the inla() function, the argument control.compute can be included with a list of variables you wish to compute and return with the model. This is demonstrated in Module 6, wherein the argument is specified as control.compute = list(waic = TRUE)). This is seen in Module 6 with the following code. #fit the chosen model and set WAIC to be computed form_pop1a &lt;- log(Total_Pop) ~ x2 + x16 + x20 + x24 + +x31 + x36 + x40 mod1 &lt;- inla(form_pop1a, data = data.frame(Data_CMR_std), # Data_CMR family = &quot;normal&quot;, control.predictor = list(compute = TRUE), control.compute = list(dic = TRUE, waic = TRUE, cpo = TRUE, config = TRUE) ) To extract just the WAIC statistic from the model, either $waic[1] can be used after giving the model name, or the function summary() with the model of interest as the argument can be used to return the WAIC statistic alongside other potentially of interest information. Both of these options are given below. As with the standard AIC (and BIC), the WAIC is used for comparing nested models, where the model with the lower WAIC value is the better fitting model. #extracting WAIC mod1$waic[1] ## $waic ## [1] 3078.578 #using summary function summary(mod1) ## Time used: ## Pre = 0.466, Running = 0.642, Post = 0.199, Total = 1.31 ## Fixed effects: ## mean sd 0.025quant 0.5quant 0.975quant mode kld ## (Intercept) 6.815 0.016 6.784 6.815 6.847 6.815 0 ## x2 0.009 0.018 -0.026 0.009 0.044 0.009 0 ## x16 -0.083 0.018 -0.118 -0.083 -0.048 -0.083 0 ## x20 0.022 0.017 -0.011 0.022 0.056 0.022 0 ## x24 -0.002 0.017 -0.035 -0.002 0.031 -0.002 0 ## x31 -0.144 0.017 -0.178 -0.144 -0.110 -0.144 0 ## x36 -0.103 0.019 -0.141 -0.103 -0.065 -0.103 0 ## x40 -0.017 0.021 -0.058 -0.017 0.024 -0.017 0 ## ## Model hyperparameters: ## mean sd 0.025quant 0.5quant 0.975quant mode ## Precision for the Gaussian observations 2.49 0.089 2.32 2.49 2.67 2.49 ## ## Deviance Information Criterion (DIC) ...............: 3074.66 ## Deviance Information Criterion (DIC, saturated) ....: 1603.22 ## Effective number of parameters .....................: 9.00 ## ## Watanabe-Akaike information criterion (WAIC) ...: 3078.58 ## Effective number of parameters .................: 12.70 ## ## Marginal log-Likelihood: -1595.61 ## CPO, PIT is computed ## Posterior summaries for the linear predictor and the fitted values are computed ## (Posterior marginals needs also &#39;control.compute=list(return.marginals.predictor=TRUE)&#39;) 9.2.2 Deviance Information Criterion (DIC) Similarly to the WAIC, the DIC is commonly used for comparison of Bayesian models for model selection. This criterion is a hierarchical modelling generalisation of the AIC and is particularly useful in cases where the Markov chain Monte Carlo (MCMC) methods have been used for obtaining the posterior distributions of the models. The main difference from the AIC is that instead of the maximised log-likelihood value used, the DIC uses the log-likelihood evaluated at the Bayes estimate of \\(\\hat{\\theta}\\). Additionally, the penalty term in the AIC is replaced with the estimated effective number of parameters. As with the WAIC, the DIC can be computed within the inla() function by including the argument control.compute specified as control.compute = list(dic = TRUE)). This is also shown in Module 6 with the following code. #fit the chosen model and set DIC to be computed form_pop1a &lt;- log(Total_Pop) ~ x2 + x16 + x20 + x24 + x31 + x36 + x40 mod1 &lt;- inla(form_pop1a, data = data.frame(Data_CMR_std), family = &quot;normal&quot;, control.predictor = list(compute = TRUE), control.compute = list(dic = TRUE, waic = TRUE, cpo = TRUE, config = TRUE) ) The methods to extract the WAIC from the model are similar to those for extracting the DIC, for example, $dic[1] can be used following the model name or the summary() function with the model of interest as the argument. As an example, the former method is given below for the above model. Similarly to the WAIC statistic, DIC is used for comparing the fit of nested models and the model with the lowest DIC value is the better fitting model. #compute the DIC mod1$dic[1] ## $dic ## [1] 3074.664 For more information and a manual implementation in R, see Dean Markwick. 9.2.3 Conditional predictive ordinate (CPO) The conditional predictive ordinate is the leave-one-out cross-validation predictive density (also described as a cross-validatory criterion), conducting an observation-specific measure of the fit of the model. The CPO is computed for each observation, where when the model is fitted using all data but \\(y_i\\) (leave-one-out cross-validation), the CPO value is given as the posterior probability of observing that observation. The CPO allows for detection of unexpected or unusual observations, where whilst large CPO values correspond to the model fitting the observation well, a small CPO value indicates that the model fits that observation poorly, possibly indicating an outlier. This approach requires some MCMC simulation to sample from the posterior simulation, however, this is the only simulation required unlike with other cross-validation and posterior-predictive approaches, making it a more computationally efficient and attractive approach to use. As with both the WAIC and DIC, Module 6 provides an example of this being computed within the inla() function through using the argument control.compute = list(cpo = TRUE), where the example code is as follows. #fit the chosen model and set CPO to be computed form_pop1a &lt;- log(Total_Pop) ~ x2 + x16 + x20 + x24 + x31 + x36 + x40 mod1 &lt;- inla(form_pop1a, data = data.frame(Data_CMR_std), family = &quot;normal&quot;, control.predictor = list(compute = TRUE), control.compute = list(dic = TRUE, waic = TRUE, cpo = TRUE, config = TRUE) ) Similarly to the other model selection methods, the CPO can be extracted through using $cpo[1] or $cpo$cpo following the name of the chosen model. To identify any potential outlier observations, the CPO values can be plotted, with each value numbered to the corresponding observation index number for ease of identification. #set n to be the total number of observations n &lt;- nrow(Data_CMR) #extract the CPO values cpo_values &lt;- mod1$cpo$cpo #plot the CPO values plot(1:n, cpo_values, ylab = &quot;CPO&quot;, type = &quot;n&quot;) text(1:n, cpo_values, 1:n) Given the large number of observations, the potential outlier observations cannot be easily detected from the resulting plot, to simplify this, the \\(y\\)-axis can be limited to only show the very small CPO values as follows. #plot the CPO values plot(1:n, cpo_values, ylab = &quot;CPO&quot;, type = &quot;n&quot;, ylim = c(0, 0.01)) text(1:n, cpo_values, 1:n) The very small observations can then be extracted through selecting the ‘threshold’ for which values will be classified as outliers, and using the which() function to select the indices of each outlier value. #extract outliers which(cpo_values &lt; 0.005) ## [1] 142 225 228 231 234 260 268 319 368 412 438 713 744 745 756 757 808 865 890 940 ## [21] 1147 1234 1252 1302 1314 1334 1399 For more information on CPO, see Posterior Predictive Bayesian Phylogenetic Model Selection. 9.2.4 Probability Integral Transform (PIT) The probability integral transform (or predictive integral transform, PIT) is another criterion that can be used for model selection. For each observation, the PIT measures the probability of a new response value being lower than the observed response value using a model that uses the rest of the data (also a leave-one-out approach). In the case where the model represents the observations well, the resulting (ordered) PIT values should approximately follow a uniform distribution. The PIT is computed as part of the CPO within the inla() function, where it is confirmed whether or not the PIT is computed in the model in the sumamry() function, as seen below for the model given as an example in Module 6. #summary of chosen model for PIT summary(mod1) ## Time used: ## Pre = 0.466, Running = 0.642, Post = 0.199, Total = 1.31 ## Fixed effects: ## mean sd 0.025quant 0.5quant 0.975quant mode kld ## (Intercept) 6.815 0.016 6.784 6.815 6.847 6.815 0 ## x2 0.009 0.018 -0.026 0.009 0.044 0.009 0 ## x16 -0.083 0.018 -0.118 -0.083 -0.048 -0.083 0 ## x20 0.022 0.017 -0.011 0.022 0.056 0.022 0 ## x24 -0.002 0.017 -0.035 -0.002 0.031 -0.002 0 ## x31 -0.144 0.017 -0.178 -0.144 -0.110 -0.144 0 ## x36 -0.103 0.019 -0.141 -0.103 -0.065 -0.103 0 ## x40 -0.017 0.021 -0.058 -0.017 0.024 -0.017 0 ## ## Model hyperparameters: ## mean sd 0.025quant 0.5quant 0.975quant mode ## Precision for the Gaussian observations 2.49 0.089 2.32 2.49 2.67 2.49 ## ## Deviance Information Criterion (DIC) ...............: 3074.66 ## Deviance Information Criterion (DIC, saturated) ....: 1603.22 ## Effective number of parameters .....................: 9.00 ## ## Watanabe-Akaike information criterion (WAIC) ...: 3078.58 ## Effective number of parameters .................: 12.70 ## ## Marginal log-Likelihood: -1595.61 ## CPO, PIT is computed ## Posterior summaries for the linear predictor and the fitted values are computed ## (Posterior marginals needs also &#39;control.compute=list(return.marginals.predictor=TRUE)&#39;) The PIT values can be extracted from the model through using $cpo[2] or $cpo$pit following from the name of the chosen model, demonstrated in the below code, where the PIT values are plotted to test the fit of the model. #extract PIT values pit_values &lt;- mod1$cpo$pit #compute uniform quantiles uniform_quant &lt;- (1:n)/(n+1) #plot the PIT values plot(uniform_quant, sort(pit_values), xlab = &quot;Uniform quantiles&quot;, ylab = &quot;Ordered PIT values&quot;) abline(0,1) It can be seen that the model fits the data reasonably well, given that the resulting plot of the PIT values is approximately uniformly distributed with no obvious outlier values. For more information, see Bayesian inference with INLA. 9.3 Cross-validation An introduction to cross-validation methods is given in Module 4, where a frequentist approach is taken. This section will cover basic cross-validation methods, in addition to the k-fold and leave-one-out cross-validation methods previously covered, however, with the application being to geospatial data (the Cameroon dataset following on from Module 8), INLA models and a Bayesian approach. 9.3.1 Basic cross-validation To begin with the basic cross-validation process, the data first needs to be randomly split into two, a training set and a test set. In this case, the training dataset contains 80% of the data and the test set contains the remaining 20% of the data. #set the seed for reproducibility set.seed(100) #find how large 80% of the data is n_train &lt;- ceiling(0.8 * nrow(Data_CMR_std)) #sample the indices for the training data train_sample &lt;- sample(c(1:nrow(Data_CMR_std)), n_train) #sample the training data train_data &lt;- Data_CMR_std[train_sample, ] #sample the test data test_data &lt;- Data_CMR_std[-train_sample, ] Once the data has been split, the preferred model from above can be fitted on the training data (to ‘train’ the model). This can be done with the glm() as above. #fit the training model train_model &lt;- glm(formula = y ~ x2 + x16 + x20 + x24 + x31 + x36 + x40, family = gaussian, data = train_data) As discussed in Module 4, predicted values can be computed using two methods. The simplest way is to use the predict() function, including the training model and test dataset as arguments. Alternatively, the prediction can be done manually through finding the coefficients for the best model and then manually multiplying the model coefficients to the corresponding covariates from the test data. To demonstrate that both methods produce the comparable results, a data frame can be created to compare the predicted values from each method side-by-side. #predictions with function pred_w_func &lt;- predict(train_model, test_data) #predictions manually coeffs &lt;- coefficients(train_model) pred_manual &lt;- coeffs[1] + coeffs[2]*test_data$x2 + coeffs[3]*test_data$x16 + coeffs[4]*test_data$x20 + coeffs[5]*test_data$x24 + coeffs[6]*test_data$x31 + coeffs[7]*test_data$x36 + coeffs[8]*test_data$x40 #compare the predictions found using the two methods compare_preds &lt;- data.frame(Manual = pred_manual, Automatic = pred_w_func) head(compare_preds) ## Manual Automatic ## 7 -4.841221 -4.841221 ## 10 -5.499207 -5.499207 ## 14 -4.207881 -4.207881 ## 22 -5.169066 -5.169066 ## 25 -5.034028 -5.034028 ## 33 -4.560108 -4.560108 To further compare the results, a scatter plot and a box plot can be produced. #put the plots side-by-side par(mfrow = c(1,2)) #scatter plot plot(pred_manual, pred_w_func, col = c(&quot;blue&quot;, &quot;red&quot;), xlab = &quot;Manual&quot;, ylab = &quot;Automatic&quot;) #box plot boxplot(pred_manual, pred_w_func, col = c(&quot;blue&quot;, &quot;red&quot;)) Model fit metrics such as Mean Absolute Error (MAE), Root Mean Square Error (RMSE), bias and correlation can all be computed to help understand how well the chosen model works. To make it simpler to compare the different metrics, the below function created computes each of the aforementioned metrics through including the observed values and the predicted values as arguments. #create model metrics function mod_metrics &lt;- function(obs, pred){ residual = pred - obs MAE = mean(abs(residual), na.rm = TRUE) #Mean Absolute Error MSE = mean(residual^2, na.rm = TRUE) #Mean Square Error RMSE = sqrt(MSE) #Root Mean Square Error BIAS = mean(residual, na.rm = TRUE) #Bias CORR = cor(obs[!is.na(obs)], pred[!is.na(obs)]) #Correlation output &lt;- list(MAE = MAE , RMSE = RMSE, BIAS = abs(BIAS), CORR = CORR) return(output) } To begin with, the log-density can be included as the observed values with the manually predicted values from the chosen model also included as an argument. #compute the model metrics metrics &lt;- mod_metrics(test_data$LDensity, pred_manual) #remove the list structure and round the results to 4 d.p. met &lt;- round(unlist(metrics), 4) met ## MAE RMSE BIAS CORR ## 0.8507 1.1276 0.0938 0.5317 The metrics results indicate the model works well. For example, the range of MAE is from 0 to \\(\\infty\\), where a score of 0 would indicate that the resulting predictions are perfect, therefore a low value of 1.0116 is good. Similarly, the rest of the metrics are also small, particularly the bias and correlation, two values that you want to be as small as possible for a better fitting model. Repeated cross-validation can be used to better assess the model metrics through obtaining average values for the metrics. This is done in the code below where the start values are set to be 0, following the same process as before through randomly splitting the data, modelling the training data and predicting with the test data followed by computing the metrics. The mean value can then be computed for each of the metrics and compared to the results obtained above for when the process is only completed once. The results from the repeated CV support the prior conclusions that the model chosen works well in the case of this data. #run the cross validation multiple number of times and average the values MAE &lt;- 0 RMSE &lt;- 0 BIAS &lt;- 0 CORR &lt;- 0 #choosing here to run the validation 20 times for(i in 1:20){ #select the number of training samples to take to be 80% of the observations n_train &lt;- ceiling(0.8 * nrow(Data_CMR_std)) #sample the the indices for the training data train_sample &lt;- sample(c(1:nrow(Data_CMR_std)), n_train) #create the training dataset train_data &lt;- Data_CMR_std[train_sample, ] #create the test dataset to contain the remaining observations (20%) test_data &lt;- Data_CMR_std[-train_sample, ] #fit the training model using the previously selected covariates train_model &lt;- glm(formula = y ~ x2 + x16 + x20 + x24 + x31 + x36 + x40, family = gaussian, data = train_data) #save the coefficients from the training model coeffs &lt;- coefficients(train_model) #manually predict using the model coefficients pred_manual &lt;- coeffs[1] + coeffs[2]*test_data$x2 + coeffs[3]*test_data$x16 + coeffs[4]*test_data$x20 + coeffs[5]*test_data$x24 + coeffs[6]*test_data$x31 + coeffs[7]*test_data$x36 + coeffs[8]*test_data$x40 #obtain the metrics for the model using the model metrics function metrics &lt;- round(unlist(mod_metrics(test_data$LDensity, pred_manual)),4) #extract each of the metrics for the model and sum to result from #previous iteration to find mean metric value later MAE &lt;- MAE + metrics[1] RMSE &lt;- RMSE + metrics[2] MAE &lt;- MAE + metrics[3] CORR &lt;- CORR + metrics[4] } #create a data frame for the model metrics, averaged across the 20 iterations av_metrics &lt;- data.frame(MAE = MAE/20, RMSE = RMSE/20, BIAS = BIAS/20, CORR = CORR/20) #print the result (met_all &lt;- rbind(met, av_metrics)) #include in brackets to print result ## MAE RMSE BIAS CORR ## 1 0.850700 1.127600 0.0938 0.53170 ## MAE 0.909175 1.206045 0.0000 0.48217 9.3.2 K-Fold Cross-Validation K-fold cross-validation is a common choice of method for cross-validation for multiple reasons. Firstly, it is particularly useful in the case where the number of samples available is small given that this approach does not waste much data. Additionally, in comparison to alternative cross-validation approaches, k-fold cross-validation often results in models with less bias as it ensures that each data point is used for both training and validation. Therefore, this approach ensures that each of the observations from the original dataset are able to appear in both the training and test datasets. In this section, to demonstrate the process of k-fold cross-validation, both in-sample (data from the sample is used to develop the model that is used for the predictions) and out-of-sample (data that wasn’t used in the development of the model is used for the predictions), a function is created which allows for an automated process of the cross-validation. Firstly, a function to compute the model metrics must be created, this takes the observed and predicted values and computes the mean absolute error, mean square error, root mean square error, bias and the correlation coefficient. This function will aid in streamlining the cross-validation function. #create model metrics function model_metrics &lt;- function(obs, pred){ residual = pred - obs MAE = mean(abs(residual), na.rm = TRUE) #Mean Absolute Error MSE = mean(residual^2, na.rm = TRUE) #Mean Square Error RMSE = sqrt(MSE) #Root Mean Square Error BIAS = mean(residual, na.rm = TRUE) #Bias CORR = cor(obs[!is.na(obs)], pred[!is.na(obs)]) #Correlation Coefficient output &lt;- list(MAE = MAE, RMSE = RMSE, BIAS = abs(BIAS), CC = CORR) return(output) } Then, the function for the k-fold cross-validation can be created. For alternative datasets, the selected covariates should be changed to suit your data. The arguments needed for the function are as follows. dat: the input survey data containing all the variables n.folds: the number of test (k) folds to use mod: the best model of the full or reference data A: the projection matrix used in training the full data model cov: the fixed covariates from the selected model cov2: additional covariates to use in the out-of-sample training stack seed: a random sample seed used to make the results reproducible cross_validate &lt;- function(dat, n.folds, mod, form, A, cov, cov2, seed){ #set the seed for reproducibility set.seed(seed) #number of rows in the dataset N &lt;- nrow(dat) #sample IDs for the training data table(ind_train &lt;- factor(sample(x = rep(1:n.folds, each = floor(N / n.folds)), size = N))) #create a table for the training IDs table(as.numeric(ind_train)) dat$k_fold &lt;- as.numeric(ind_train) #vector for the longitude and latitude coordinates coords &lt;- cbind(dat$lon, dat$lat) #sort the unique folds k_uniq &lt;-sort(unique(dat$k_fold)) #--------------------------------------------------------------------------# # In-Sample # #--------------------------------------------------------------------------# #create variables for the metrics and predictions met_list_in &lt;- list() pred_list_in &lt;- list() for(i in 1:length(k_uniq)){ #cycle through each of the unique folds #keep track the current fold print(paste0(&quot;in-sample cross-validation using fold &quot;, i, sep = &quot;&quot;)) #select the test data for fold i test_ind &lt;- which(dat$k_fold == k_uniq[i]) dim(test &lt;- dat[test_ind, ]) #train and test data coordinates train_coords &lt;- coords test_coords &lt;- coords[test_ind,] #spatial random effects based on the full data best model sfield_nodes_mean &lt;- mod$summary.random$spatial.field[&#39;mean&#39;] field_mean &lt;- (A%*% as.data.frame(sfield_nodes_mean)[, 1]) #list of selected covariates fixed &lt;- mod$summary.fixed[&#39;Intercept&#39;, &#39;mean&#39;] #fixed effects for(covariate in cov){ fixed &lt;- fixed + mod$summary.fixed[paste0(covariate), &#39;mean&#39;] * test[,paste0(covariate)] } #add settlement type and region nested effects fixed = fixed + rnorm(nrow(test), 0, 1/mod$summary.hyperpar$mean[2]) + #add settlement type random effect rnorm(nrow(test), 0, 1/mod$summary.hyperpar$mean[5]) + #add uncorrelated spatial random effects mod$summary.random$IDsr[&#39;mean&#39;][test_ind,1] + field_mean[test_ind,1] dens_ht &lt;- exp(fixed) sum(pop_ht &lt;- dens_ht*test$bld) #scatter plots for each fold, uncomment for plots to show # par(mfrow = c(1,1)) # plot(test$obs, pop_ht, xlab = &quot;Observed&quot;, # ylab = &quot;Predicted&quot;, col = c(&#39;blue&#39;,&#39;orange&#39;), # pch = c(16,16), cex.axis = 1.5) # abline(0,1) # legend(&quot;topleft&quot;, c(&quot;Observed&quot;, &quot;Predicted&quot;), col = c(&quot;blue&quot;, &quot;orange&quot;), # pch = c(16,16), bty = &quot;n&quot;, cex = 1.5) #calculate fit metrics met_in &lt;- model_metrics(test$pop, pop_ht) met_list_in[[i]]&lt;- unlist(met_in) pred_list_in[[i]] &lt;- data.frame(obs = test$obs, pred = pop_ht, fold = rep(i, length(test$obs)), data = rep(&quot;insample&quot;, length(test$obs))) } met_list_in_dat &lt;- do.call(rbind,met_list_in) #fit metrics metrics_in &lt;- apply(met_list_in_dat, 2, mean) #predictions pred_list_in_dat &lt;- do.call(rbind,pred_list_in) #--------------------------------------------------------------------------# # Out-of-Sample # #--------------------------------------------------------------------------# #create variables for the model metrics and predictions met_list_out &lt;- list() pred_list_out &lt;- list() for(i in 1:length(k_uniq)){#cycle through each of the unique folds #keep track of current fold print(paste0(&quot;out-of-sample cross-validation using fold &quot;, i, sep = &quot;&quot;)) #select the train and test data for fold i train_ind &lt;- which(dat$k_fold != k_uniq[i]) test_ind &lt;- which(dat$k_fold == k_uniq[i]) dim(train &lt;- dat[train_ind, ]) dim(test &lt;- dat[test_ind, ]) #train and test data coordinates train_coords &lt;- coords[train_ind,] test_coords &lt;- coords[test_ind,] #create a projection matrix for training data Ae&lt;-inla.spde.make.A(mesh = mesh,loc = as.matrix(train_coords)); dim(Ae) covars_train &lt;- train[,c(cov, cov2)] stk_train &lt;- inla.stack(data = list(y = train$dens), #the response A = list(Ae,1), #the A matrix effects = list(c(list(Intercept = 1), #the Intercept iset), #the spatial index list(covars_train)), #the covariates tag = &#39;train&#39;) model &lt;-inla(form, #the formula data = inla.stack.data(stk_train,spde = spde), family = &#39;gamma&#39;, control.predictor = list(A = inla.stack.A(stk_train), compute = TRUE), control.compute = list(dic = TRUE, waic = TRUE, cpo = TRUE, config = TRUE), verbose = FALSE) summary(model) #extract spatial random effects from the full data best model sfield_nodes_mean &lt;- mod$summary.random$spatial.field[&#39;mean&#39;] field_mean &lt;- (A%*% as.data.frame(sfield_nodes_mean)[, 1]) fixed &lt;- model$summary.fixed[&#39;Intercept&#39;, &#39;mean&#39;] #fixed effects for(covariate in cov){ fixed &lt;- fixed + model$summary.fixed[paste0(covariate), &#39;mean&#39;] * test[,paste0(covariate)] } #add settlement type and region nested effects fixed = fixed + rnorm(nrow(test), 0, 1/model$summary.hyperpar$mean[2]) + #add settlement type random effect rnorm(nrow(test), 0, 1/model$summary.hyperpar$mean[5]) + #add uncorrelated spatial random effects mod$summary.random$IDsr[&#39;mean&#39;][test_ind,1] + field_mean[test_ind,1] dens_ht &lt;- exp(fixed) sum(pop_ht &lt;- dens_ht*test$bld) #scatter plots for each fold, uncomment for plots to show # par(mfrow = c(1,1)) # plot(test$obs, pop_ht, xlab = &quot;Observed&quot;, # ylab = &quot;Predicted&quot;, col = c(&#39;blue&#39;,&#39;orange&#39;), # pch = c(16,16), cex.axis = 1.5) # abline(0,1) # legend(&quot;topleft&quot;, c(&quot;Observed&quot;, &quot;Predicted&quot;), col = c(&quot;blue&quot;, &quot;orange&quot;), # pch = c(16,16), bty = &quot;n&quot;, cex = 1.5) #calculate fit metrics met_out &lt;- model_metrics(test$pop, pop_ht) met_list_out[[i]]&lt;- unlist(met_out) pred_list_out[[i]] &lt;- data.frame(obs = test$obs, pred = pop_ht, fold = rep(i, length(test$obs)), data = rep(&quot;outsample&quot;, length(test$obs))) } met_list_out_dat &lt;- do.call(rbind,met_list_out) #fit metrics metrics_out &lt;- apply(met_list_out_dat, 2, mean) #predictions pred_list_out_dat &lt;- do.call(rbind,pred_list_out) cv_mets &lt;- rbind(metrics_in, metrics_out) output &lt;- list( met_list_in_dat = met_list_in_dat, met_list_out_dat = met_list_out_dat, pred_dat = rbind(pred_list_in_dat, pred_list_out_dat), cv_metrics = rbind(metrics_in, metrics_out)) } To apply this function, the data from Cameroon is used as an example. In this example, the dataset is called dat, the number of folds utilised is 5, the chosen model is called mod4 with corresponding formula in the INLA model form4, projection matrix A and seed number 13235 (for reproducibility). Additionally, the vectors containing the selected covariate information, cov and cov2, are defined prior to running the function. #results from k-fold cross validation cov &lt;- c(&#39;x2&#39;, &#39;x3&#39;, &#39;x17&#39;, &#39;x21&#39;, &#39;x32&#39;, &#39;x34&#39;, &#39;x40&#39;, &#39;x42&#39;) cov2 &lt;- c(&quot;set_reg&quot;, &quot;set_typ&quot;, &quot;region&quot;, &quot;IDsr&quot;) cross_val &lt;- cross_validate(dat = dat, n.folds = 5, mod = mod4, form = form4, A = A, cov = cov, cov2 = cov2, seed = 13235) ## [1] &quot;in-sample cross-validation using fold 1&quot; ## [1] &quot;in-sample cross-validation using fold 2&quot; ## [1] &quot;in-sample cross-validation using fold 3&quot; ## [1] &quot;in-sample cross-validation using fold 4&quot; ## [1] &quot;in-sample cross-validation using fold 5&quot; ## [1] &quot;out-of-sample cross-validation using fold 1&quot; ## [1] &quot;out-of-sample cross-validation using fold 2&quot; ## [1] &quot;out-of-sample cross-validation using fold 3&quot; ## [1] &quot;out-of-sample cross-validation using fold 4&quot; ## [1] &quot;out-of-sample cross-validation using fold 5&quot; #in-sample metrics per fold cross_val$met_list_in_dat ## MAE RMSE BIAS CC ## [1,] 154.0561 327.7893 95.22379 0.9902840 ## [2,] 139.0922 196.5983 82.62810 0.9887830 ## [3,] 141.9625 224.6575 74.04167 0.9801556 ## [4,] 142.4847 211.9439 79.10194 0.9781099 ## [5,] 141.4359 223.6698 74.50207 0.9864906 #out-of-sample metrics per fold cross_val$met_list_out_dat ## MAE RMSE BIAS CC ## [1,] 160.7173 339.8842 61.73926 0.9836105 ## [2,] 141.5075 198.4901 69.18201 0.9874822 ## [3,] 177.8058 302.0186 119.58316 0.9721151 ## [4,] 169.6710 268.6756 140.74931 0.9750293 ## [5,] 132.1042 193.2967 59.91717 0.9854916 #combined averaged metrics cross_val$cv_metrics ## MAE RMSE BIAS CC ## metrics_in 143.8063 236.9318 81.09951 0.9847646 ## metrics_out 156.3612 260.4731 90.23418 0.9807457 #combined prediction data head(cross_val$pred_dat) ## obs pred fold data ## 1 1886 1685.1565 1 insample ## 2 908 978.4384 1 insample ## 3 545 631.0405 1 insample ## 4 768 862.1703 1 insample ## 5 915 976.8425 1 insample ## 6 912 1074.9687 1 insample tail(cross_val$pred_dat) ## obs pred fold data ## 4575 1291 1026.8244 5 outsample ## 4576 1028 849.8356 5 outsample ## 4577 283 228.0998 5 outsample ## 4578 346 329.3237 5 outsample ## 4579 691 720.8671 5 outsample ## 4580 373 399.1697 5 outsample 9.3.3 Useful resources WAIC: Nature WAIC: Columbia DIC: Dean Marwick CPO: Posterior Predictive Bayesian Phulogenetic Model Selection PIT: Bayesian inference with INLA INLA-SPDE approach: datascience+ "],["population-prediction-and-uncertainty-quantification.html", "10 Population Prediction and Uncertainty Quantification 10.1 Covariate stacking 10.2 Grid cell/pixel level prediction 10.3 Aggregation to area units of interest and uncertainty quantification 10.4 Useful resources", " 10 Population Prediction and Uncertainty Quantification This module looks at population prediction methods and associated uncertainty quantification methodology, starting with covariate stacking. Module 10 also covers posterior distribution simulation, aggregation to area units of interest and grid cell/level prediction. #install raster processing packages install.packages(&quot;terra&quot;) install.packages(&quot;tictoc&quot;) install.packages(&quot;exactextractr&quot;) #load raster processing packages library(tidyverse) library(terra) library(&quot;tictoc&quot;) library(feather) library(sf) 10.1 Covariate stacking In order to predict population values across the entire population across the entire country, the covariates available require stacking and the corresponding values of the covariates extracting. However, given that across a country, the entire area is not inhabited, only those points which are settled (inhabited) are of interest, so the covariate information extracted requires some manipulation to obtain only the settled raster values. This section explores these methods for the Cameroon dataset. Firstly, the rasters available that are going to be stacked need to be imported into the R environment. #import all rasters raster_list &lt;-list.files(path = raster_path4, pattern = &quot;.tif$&quot;, all.files = TRUE, full.names = FALSE) #see the rasters in the list raster_list ## [1] &quot;acled_conflict_data_2021_CMR_ed_masked.tif&quot; &quot;CMR_buildings_count.tif&quot; ## [3] &quot;CMR_buildings_total_area.tif&quot; &quot;CMR_Department.tif&quot; ## [5] &quot;CMR_dst_in_water_100m_2000_2012.tif&quot; &quot;CMR_esaccilc_dst140_100m_2015.tif&quot; ## [7] &quot;CMR_esaccilc_dst200_100m_2015.tif&quot; &quot;CMR_osm_dst_localroads.tif&quot; ## [9] &quot;CMR_osm_dst_railways.tif&quot; &quot;CMR_Regions.tif&quot; ## [11] &quot;CMR_Settlement_Classification.tif&quot; &quot;CMR_viirs_100m_2020.tif&quot; There are two approaches to stacking the covariates in R available. The first is to stack all the covariates together at once and then get the relevant raster values with the values() function from the terra package, including the covariate stack as an argument and specifying dataframe = TRUE to return the values as a data frame. This option however, is not always appropriate as extracting the values at once can take a long time and particularly in cases where there are more than a few covariates available, the code can be too much for many computers to handle. #stack all rasters stack_covariates &lt;- rast(paste0(raster_path4, c(raster_list))) #get raster values covs_raster_values &lt;- terra::values(stack_covariates, dataframe = TRUE) Alternatively, the covariates can be stacked in batches, which is the preferred method of covariate stacking as it does not have as high of a computational burden associated. To use this approach, first the desired size of the batches must be specified, in this case, the size is 2. Then, to cycle through each of the covariates available, a for() loop can be used to make the process more automated and hence more efficient. #define batch size batch_size &lt;- 2 However, before starting the loop, to write only the settled pixels (covariate values which are associated with a pixel where there is a building present) to file, the raster file for the building count can first be processed, with the building count values obtained with the values() function. Once the values are obtained, this covariate should be removed from the raster list to not repeat the processing in the loop. #read building count raster and get values b_count &lt;- rast(paste0(raster_path4, &quot;CMR_buildings_count.tif&quot;)) b_count &lt;- terra::values(b_count, dataframe = TRUE) #remove &quot;CMR_buildings_count.tif&quot; from list process_rasters_list &lt;- raster_list[-c(2)] process_rasters_list ## [1] &quot;acled_conflict_data_2021_CMR_ed_masked.tif&quot; &quot;CMR_buildings_total_area.tif&quot; ## [3] &quot;CMR_Department.tif&quot; &quot;CMR_dst_in_water_100m_2000_2012.tif&quot; ## [5] &quot;CMR_esaccilc_dst140_100m_2015.tif&quot; &quot;CMR_esaccilc_dst200_100m_2015.tif&quot; ## [7] &quot;CMR_osm_dst_localroads.tif&quot; &quot;CMR_osm_dst_railways.tif&quot; ## [9] &quot;CMR_Regions.tif&quot; &quot;CMR_Settlement_Classification.tif&quot; ## [11] &quot;CMR_viirs_100m_2020.tif&quot; The first step in the loop is to subset the available rasters to select your ‘batch’. Once you have the subset, the rast() function can be used to load that batch of covariates. As with the first approach, to obtain the raster values, the values() function is used. Since only the settled points are wanting to be written to file, the filter() and select() functions can be used to subset the covariate values and keep only those which correspond to settled points, using the building count raster and corresponding values as a reference. The final results can then be exported as a .feather file with the write_feather() function from the feather package. Whilst it it is not a necessity, the rm() function can be used to free up memory for the raster batches and their values created within the loop to help R work more efficiently. #loop through covariates in batches for (i in seq(1, length(process_rasters_list), batch_size)) { batch_covs &lt;- process_rasters_list[i:min(i + batch_size - 1, length(process_rasters_list))] #load batch of covariates rasters covs_raster &lt;- rast(paste0(raster_path4, batch_covs)) #get raster values covs_raster_values &lt;- terra::values(covs_raster, dataframe = TRUE) #write only settled pixels to file covs_raster_values &lt;- covs_raster_values %&gt;% cbind(b_count) %&gt;% filter(CMR_buildings_count &gt; 0) %&gt;% dplyr::select(-CMR_buildings_count) #write processed covariate values to a feather file feather_output_path &lt;- paste0(output_path, &quot;Processed_Covariates_&quot;, i, &quot;_to_&quot;, min(i + batch_size - 1, length(process_rasters_list)), &quot;.feather&quot;) feather::write_feather(covs_raster_values, feather_output_path) #free up memory rm(covs_raster, covs_raster_values); gc() } Once the raster values have been obtained, the files can be read back to the memory with the dir() function and then bound together. #read all files back to memory myfiles &lt;-dir(output_path,pattern = &quot;*.feather&quot;) myfiles ## [1] &quot;Processed_Covariates_1_to_2.feather&quot; &quot;Processed_Covariates_11_to_11.feather&quot; ## [3] &quot;Processed_Covariates_3_to_4.feather&quot; &quot;Processed_Covariates_5_to_6.feather&quot; ## [5] &quot;Processed_Covariates_7_to_8.feather&quot; &quot;Processed_Covariates_9_to_10.feather&quot; #bind the files raster_values &lt;- myfiles %&gt;% map(function(x) read_feather(file.path(output_path, x))) %&gt;% reduce(cbind) From the modelling in Module 8, it was found that for the Cameroon data, the covariates x2, x16, x20, x24, x31, x36, x40 were all statistically significant and had a notable relationship with the population density. Therefore, these are the covariates that are of interest for prediction. However, the names of x(.) are arbitrary, so the significant variables should be renamed for ease of understanding and aid in the interpretation of any results. The names of the variables can be obtained from the var_names.csv and manually input as below. #load the variable names file var_names &lt;- read.csv(paste0(data_path,&quot;var_names.csv&quot;)) var_names[c(2, 16, 20, 24, 31, 36, 40),] ## X var_names var_names2 ## 2 2 mean.acled_conflict_data_2021_CMR_ed_masked x2 ## 16 16 mean.CMR_dst_in_water_100m_2000_2012 x16 ## 20 20 mean.CMR_esaccilc_dst140_100m_2015 x20 ## 24 24 mean.CMR_esaccilc_dst200_100m_2015 x24 ## 31 31 mean.CMR_osm_dst_localroads x31 ## 36 36 mean.CMR_osm_dst_railways x36 ## 40 40 mean.CMR_viirs_100m_2020 x40 #rename variables raster_values1 &lt;- raster_values %&gt;% rename(x2 = acled_conflict_data_2021_CMR_ed_masked, x16 = CMR_dst_in_water_100m_2000_2012, x20 = CMR_esaccilc_dst140_100m_2015, x24 = CMR_esaccilc_dst200_100m_2015, x31 = CMR_osm_dst_localroads, x36 = CMR_osm_dst_railways, x40 = CMR_viirs_100m_2020) To estimate the population for the settled points, the x,y-coordinates of the centroids of each settled pixel needs to be obtained from the building count dataset. To obtain these coordinates, the raster with the building counts must be read into the environment and then used with the function xyFromCell() with arguments for the building count raster and a sequence from 1 to the number of cells in the raster. #read raster r1 &lt;- rast(paste0(raster_path4, &quot;CMR_buildings_count.tif&quot;)) #get the xy coordinate of the centroid of each pixel as a data frame coord &lt;- xyFromCell(r1, 1:ncell(r1)) head(coord) ## x y ## [1,] 8.498333 13.0775 ## [2,] 8.499167 13.0775 ## [3,] 8.500000 13.0775 ## [4,] 8.500833 13.0775 ## [5,] 8.501667 13.0775 ## [6,] 8.502500 13.0775 The coordinates can then be combined using the cbind() function with the building count values obtained above, before removing the unwanted objects (the values created above for r1, coord and b_count) from the R environment as they take up a lot of memory, making the processes less efficient. #cbind building count to coordinates stack_coord &lt;- cbind(b_count, coord) #remove unwanted object from memory rm(r1, coord, b_count); gc() ## used (Mb) gc trigger (Mb) max used (Mb) ## Ncells 8487207 453.3 22837581 1219.7 22837581 1219.7 ## Vcells 690256478 5266.3 1408294652 10744.5 2195989792 16754.1 Once the coordinates and corresponding building counts are combined, the filter() function can be used to filter out the desired (settled), so only the pixels containing buildings remain. #filter out only settled pixels stack_coord &lt;- stack_coord %&gt;% filter(CMR_buildings_count &gt; 0) To predict the population (either using only the intercept or also using covariates) the stack of building count values and corresponding coordinates need to be added to the original dataset, in this case done with the cbind() function. #cbind coordinates to original data raster_values1 &lt;- raster_values1 %&gt;% cbind(stack_coord) 10.1.1 Admin names It is often of interest to identify the population size in different areas of a country, such as in a specific region or department, rather than as the country as a whole. For ease of interpreting the results, it is beneficial to obtain the admin names and add them to the dataset. To add these admin names to the dataset, first the shapefiles containing the relevant information needs to be read into the R environment. #read in the regions and department datasets regions &lt;- st_read(paste0(shapefile_path, &quot;Region_SHP.shp&quot;)) ## Reading layer `Region_SHP&#39; from data source ## `F:\\Study\\WorldPop Training Materials\\GitBook\\wp_training_manual\\data\\CMR\\Shapefiles\\Region_SHP.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 10 features and 2 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 945997.2 ymin: 182845.8 xmax: 1802307 ymax: 1458913 ## Projected CRS: WGS 84 / World Mercator dept &lt;- st_read(paste0(shapefile_path, &quot;Departement_SHP.shp&quot;)) ## Reading layer `Departement_SHP&#39; from data source ## `F:\\Study\\WorldPop Training Materials\\GitBook\\wp_training_manual\\data\\CMR\\Shapefiles\\Departement_SHP.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 58 features and 3 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 945997.2 ymin: 182845.8 xmax: 1802307 ymax: 1458913 ## Projected CRS: WGS 84 / World Mercator The shapefiles containing the admin names need to be converted to a data frame to obtain the ID and region label (denoted in French as libelle for the Cameroon dataset). #convert region shapefile to data frame and get id and &#39;libelle&#39; regions &lt;- regions %&gt;% as_tibble() %&gt;% dplyr::select(id, libelle) %&gt;% rename(Regions = id, Regions_libelle = libelle) #rename id as regions #convert department shapefile to data frame and get id and &#39;libelle&#39; dept &lt;- dept %&gt;% as_tibble() %&gt;% dplyr::select(id, libelle) %&gt;% rename(Department = id, Department_libelle = libelle) #rename variables The data frames for region and department can then be joined with the dataset using the function full_join(). #join regions to data prediction_covs &lt;- full_join(raster_values1, regions, by = &quot;Regions&quot;) #join department to data prediction_covs &lt;- full_join(prediction_covs, dept, by = &quot;Department&quot;) The select() function can then be used to sort the data in order as follows. #sort data in order prediction_covs &lt;- prediction_covs %&gt;% dplyr::select(Regions, Regions_libelle, Department, Department_libelle, CMR_buildings_count, CMR_buildings_total_area, CMR_Settlement_Classification, starts_with(&quot;x&quot;), y) The x and y variables can be renamed as lat and lon to correspond with the latitude and longitude coordinates with the rename() function and use the filter() function to filter out any rows of data with NA for the settlement classification. #rename x and y as Lat and Lon and filter settlement classification with NA prediction_covs &lt;- prediction_covs %&gt;% rename(Lat = y, Lon = x) %&gt;% filter(!is.na(CMR_Settlement_Classification)) Finally, the write_rds() function can be used to export the data as follows. #export data to file write_rds(prediction_covs, paste0(output_path, &quot;CMR_prediction_stack.rds&quot;)) 10.2 Grid cell/pixel level prediction Gridded population data is beneficial as it allows for the integration with other datasets. Additionally, there is a major benefit that with gridded population data, population estimates can be calculated for various geographic units at a range of scales. For example EAs, wards, constituencies, districts Health catchments Within a specified distance of a feature (for example, within 5km of a health facility) The idea here is to use model parameter estimates from the fitted model to make predictions at the grid cell level with the extracted geospatial covariates at 100m resolution. The corresponding uncertainty estimates for the population predictions at various administrative levels can also be obtained. As with all other methods, the relevant packages should be installed and loaded. #install the relevant packages install.packages(&quot;kableExtra&quot;) install.packages(&quot;spdep&quot;) install.packages(&quot;INLA&quot;) #load the relevant packages library(INLA) library(kableExtra) library(sf) library(tidyverse) library(spdep) library(tmap) library(tictoc) library(terra) library(dplyr) For visualisation purposes of the results, the scientific notation for all the variables can be turned off with the following code. #turn off scientific notation options(scipen = 999) In Module 8, various models were fitted, ranging from the simple linear model to the Bayesian hierarchical models. In this section, the INLA SPDE approach is used to demonstrate how to make predictions using the more advanced Bayesian Geostatistical Model. 10.2.1 Pre-processing First the data should be loaded into the R environment. #load csv file Data_CMR &lt;- read.csv(paste0(data_path,&quot;Pop_Data_Complete.csv&quot;)) #EA shapefile ea_shp &lt;- st_read(paste0(data_path ,&quot;Pop_Data_Complete.gpkg&quot;)) ## Reading layer `Pop_Data_Complete&#39; from data source ## `F:\\Study\\WorldPop Training Materials\\GitBook\\wp_training_manual\\data\\CMR\\Pop_Data_Complete.gpkg&#39; ## using driver `GPKG&#39; ## Simple feature collection with 1592 features and 50 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 8.498025 ymin: 1.653369 xmax: 16.1904 ymax: 12.96129 ## Geodetic CRS: WGS 84 The centroid of the EA shapefile needs to be extracted as latitude and longitude coordinates and added into the Data_CMR .csv file. This can be done using the as() and st_geometry() functions to convert the shapefile to a spatial object and using the coordinates() function to extract and add the coordinates to the demographic data. #convert shapefile to a spatial object shp &lt;- as(st_geometry(ea_shp), &quot;Spatial&quot;) #add the lat-long coordinates to the data Data_CMR$long &lt;- coordinates(shp)[,1] #extract and add the longitude to the data Data_CMR$lat &lt;- coordinates(shp)[,2] #extract and add the latitude to the data #view first six rows of the data as a summary head(Data_CMR, 6) ## X EA_ID Department dept_libelle Regions region_libelle Total_Pop Settlement_Type ## 1 1 ABONG - MBANG_6 30 Haut Nyong 3 Est 890 1 ## 2 2 AFANLOUM_702 3 Mefou et Afamba 2 Centre 815 4 ## 3 3 AKOEMAN_3 49 Nyong et Soo 2 Centre 764 4 ## 4 4 AKOM II_702 25 Ocean 10 Sud 746 4 ## 5 5 AKONOLINGA_18 50 Nyong et Mfoumou 2 Centre 1109 4 ## 6 6 AKONOLINGA_700 50 Nyong et Mfoumou 2 Centre 1357 4 ## Total_Building_Count Total_Building_Area x1 x2 x3 x4 x5 x6 ## 1 286 32018.38 240337.7 86203.41 413765.8 139978.61 86203.41 184933.17 ## 2 395 38397.15 338154.9 36501.53 317968.1 36501.53 36501.53 74746.61 ## 3 367 33433.51 437903.8 54406.30 278816.8 72425.01 54406.30 72425.01 ## 4 269 24597.57 572474.0 65059.10 207275.2 167467.12 65059.10 171884.52 ## 5 286 39113.68 346930.5 47410.98 334817.6 80121.03 47410.98 80121.03 ## 6 402 30872.22 344902.1 55245.77 333230.8 76245.29 55245.77 78916.31 ## x7 x8 x9 x10 x11 x12 x13 x14 x15 x16 ## 1 184933.17 49.00933 0.8580208 0.5127055 1874.8511 124.96131 43.36115 663.00330 369.54599 66.4226456 ## 2 74746.61 94.78092 0.3557110 0.2034208 294.0987 102.34338 39.95544 96.79045 264.67972 28.3830357 ## 3 72425.01 88.32083 0.3629119 0.2126397 328.2499 90.47405 36.73468 104.04270 177.91858 22.1087456 ## 4 171884.52 399.27695 0.3817135 0.2085105 403.9308 100.72127 39.06967 128.10530 63.06371 29.8604965 ## 5 80121.03 65.29633 0.5584586 0.3535644 1485.4635 132.72063 45.55459 590.01727 262.23849 0.5587888 ## 6 78916.31 60.77288 0.3845364 0.2158287 318.3337 79.70691 34.87637 92.70645 264.28845 4.6560779 ## x17 x18 x19 x20 x21 x22 x23 x24 x25 ## 1 0.2506842 0.2168530 0.8513528 53.339569 571.1937 1.780275 -0.05533693 118.72293 0.001490207 ## 2 0.3522204 -0.2481708 11.8486433 11.467803 493.3647 30.267328 26.14026642 77.73151 0.023819378 ## 3 2.0923870 -1.9790570 33.6847458 55.099686 515.8448 38.471977 22.51198578 201.92075 0.024310285 ## 4 7.1534176 -7.0361733 65.0350876 65.647385 483.9221 62.489433 34.99201965 320.03601 0.045389920 ## 5 0.6619807 -0.3192018 0.7926053 1.597502 531.0815 4.531395 0.59316117 115.86395 0.014609964 ## 6 0.4313623 -0.3258749 8.7227964 5.226479 525.6237 9.087230 7.94695139 107.66605 0.018198114 ## x26 x27 x28 x29 x30 x31 x32 x33 x34 ## 1 0.01200000 0.00004149579 297.4118 2296.646 288.1877 14.69919 225.3033 390.2323 476.4956 ## 2 0.02870464 0.00004402846 297.6783 35368.785 47777.6641 11518.32129 8068.7568 48683.5234 26756.2227 ## 3 0.02861884 0.00005605924 297.4679 21204.406 28928.2246 1605.29187 38670.0625 40177.6914 31366.0879 ## 4 0.04607061 0.00010642956 298.1458 34093.812 40311.1211 9626.27930 6508.7632 35146.3125 43208.0781 ## 5 0.03405531 0.00004095252 297.7811 1161.185 1691.4470 630.90527 37276.1211 1713.5939 2773.5483 ## 6 0.02470175 0.00004092990 297.7823 7917.528 8837.5303 996.53833 29232.8477 9127.7480 9087.2295 ## x35 x36 x37 x38 x39 x40 x41 long lat ## 1 2.410922 99782.04 435.05762 722.6469 694.9709 0.8488668 499.7907 13.17765 3.985047 ## 2 8.193155 34135.90 6064.46387 20564.3906 732.0835 0.1883050 387.9030 12.10438 4.195674 ## 3 14.047594 51499.27 10335.01562 5314.2729 684.1978 0.1645098 315.3396 11.57054 3.214603 ## 4 13.598531 102436.87 23514.07422 8531.0508 603.2908 0.2087497 201.3288 10.45882 2.733207 ## 5 1.513200 77467.59 87.84882 354.3566 647.6537 0.3821937 392.5313 12.23236 3.776276 ## 6 3.314217 73533.62 3984.64355 4101.4194 691.9840 0.2072315 393.2006 12.22670 3.853698 Using the piper operator and the mutate() function, the population density can be calculated, which is required later in the process, for example, in the model fitting. #compute the density Data_CMR &lt;- Data_CMR %&gt;% mutate(Density = Total_Pop/Total_Building_Count) As seen previously, the covariates need to be standardised so that they are on the same scale, which will aid in the modelling process. The same function as seen in Module 8 (Section 4.2) is given below, and can then be applied to standardise the covariates. This resulting value is also known as a z-score. #standardisation function for model covariates stdise &lt;- function(x) { stds &lt;- (x - mean(x, na.rm = TRUE))/sd(x, na.rm = TRUE) return(stds) } #create new data frame to contain standardised covariates Data_CMR_std &lt;- Data_CMR #standardise the covariates only cov_vars &lt;- paste0(&quot;x&quot;, 1:41) Data_CMR_std[, cov_vars] &lt;- apply(Data_CMR[, cov_vars], 2, stdise) #summary of covariates before and after standardisation head(Data_CMR[,cov_vars[1:5]]) ## x1 x2 x3 x4 x5 ## 1 240337.7 86203.41 413765.8 139978.61 86203.41 ## 2 338154.9 36501.53 317968.1 36501.53 36501.53 ## 3 437903.8 54406.30 278816.8 72425.01 54406.30 ## 4 572474.0 65059.10 207275.2 167467.12 65059.10 ## 5 346930.5 47410.98 334817.6 80121.03 47410.98 ## 6 344902.1 55245.77 333230.8 76245.29 55245.77 head(Data_CMR_std[,cov_vars[1:5]]) ## x1 x2 x3 x4 x5 ## 1 -0.62880443 1.10518418 1.5292650 0.2777396 0.89020837 ## 2 -0.13168532 -0.01029845 0.7477583 -0.8029266 -0.09342136 ## 3 0.37525087 0.39154679 0.4283667 -0.4277586 0.26092475 ## 4 1.05915270 0.63063252 -0.1552609 0.5648168 0.47174994 ## 5 -0.08708668 0.23454744 0.8852148 -0.3473849 0.12248313 ## 6 -0.09739513 0.41038730 0.8722700 -0.3878613 0.27753826 For the processes later, it is important that there are no NA values for the population density. Therefore, from the (standardised) demographic data, the NA values should be removed from the Density, which can be done using the drop_na() function. #remove NA values in Density Data_CMR_std &lt;- Data_CMR_std %&gt;% drop_na(Density) 10.2.2 INLA-SPDE approach In order to fit the Bayesian SPDE model with INLA, a mesh needs to be constructed using the same approach as in Module 8 (Section 6.1). To begin with, the coordinates of the centroids should be defined. #define centroid coordinates coords &lt;- cbind(Data_CMR_std$long, Data_CMR_std$lat) summary(dist(coords)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.000 1.681 2.784 3.540 5.523 11.214 Then the inla.nonconvex.hull() function can be used to define the boundary with the arguments for points (the 2D point coordinates), convex (the desired extension radius), concave (the minimal concave curvature radius) and resolution (the internal computation resolution). This is then followed by the inla.mesh.2d() function with arguments for the boundary (defined with inla.nonconvex.hull()), max.edge (adjusts the mesh node sizes between the inner and outer meshes), offset (adjusts the distance between the outer and inner meshes), cutoff (controls the minimum size of the triangles allowed). #boundary construction bnd &lt;- inla.nonconvex.hull(points = coords, convex = -0.03, concave = -0.05, resolution = c(100, 100)) #mesh construction meshb &lt;- inla.mesh.2d(boundary = bnd, max.edge = c(0.1, 1), offset = c(0.05, 1), cutoff = 0.003) The resulting mesh can be plotted with the plot() function. Here it can be seen visually if the values in the above functions need to be adjusted to be more suitable. #plot mesh plot(meshb) points(coords, col = &quot;red&quot;) #number of vertices meshb$n ## [1] 11692 As in Module 8, Section 6, once the mesh is constructed, the SPDE can be built using the inla.spde2.matern() function, with the mesh constructed above as an argument, along with the arguments alpha and constr. #build the SPDE spde &lt;- inla.spde2.matern(mesh = meshb, alpha = 2, constr = TRUE) The next step laid out in Module 8 is to create the projection matrix with the inla.spde.make.A() function as follows. #construct the projection matrix A &lt;- inla.spde.make.A(mesh = meshb, loc = coords) The next step in the posterior distribution simulation process is to specify the spatial effect. This is done through using the function inla.spde.make.index() with an argument specifying the base name of the effect (in this case \"spatial_effect\"), and n.spde (the size of the model, extracted from the spde constructed above). #specify the spatial effect indexs &lt;- inla.spde.make.index(name = &quot;spatial_effect&quot;, n.spde = spde$n.spde) From the covariate selection process, the covariate that were chosen as the most significant or “best” can be extracted from the (standardised) demographic dataset and added to a new variable for the covariates with the select() function from the dplyr package. Below, there is an additional covariate selected for Settlement_Type which is used as a random effect in the subsequent modelling. #select covariates covs &lt;- Data_CMR_std %&gt;% dplyr::select(x3, x4, x7, x16, x20, x31, x37, x40, Settlement_Type) The inla.stack() function can then be used to stack the data, ready for model fitting, with the arguments data (a list containing the response variable), A (a list containing the projection A matrix created earlier and 1 to make the list of covariates), effects (a list containing the Intercept, the spatial index and the list of covariates) and tag (a quick name to call upon). #stack the data for model fitting stk.e &lt;- inla.stack(data = list(y = Data_CMR_std$Density), A = list(A,1), effects = list(c(list(Intercept = 1), indexs), list(covs) ), tag = &#39;est&#39;) Before fitting the INLA model, it is best to specify the model formula as seen before, in this case, there are the selected covariates, as well as random effects for the spatial_effect and the Settlement_Type. Following the model formula specification, the model is fitted with the inla() function, specifying the following arguments. formula: the pre-defined model formula. data: the data stack. family: the likelihood family, in this case a gamma distribution is used, however, a lognormal distribution could also be used. control.predictor: computes the marginals of the linear predictor. control.compute: a list of logical statements for computing model diagnostics. verbose: logical statement indicating whether the function should run in a verbose mode. control.inla: a list containing the control variables. #specify model formula &lt;- y ~ -1 + Intercept + x3 + x4 + x7 + x16 + x20 + x31 + x37 +x40 + f(spatial_effect, model = spde)+ f(Settlement_Type, model = &quot;iid&quot;) #fit the inla model with a gamma distribution res &lt;- inla(formula = formula, data = inla.stack.data(stk.e, spde = spde), family = &#39;gamma&#39;, control.predictor = list(A = inla.stack.A(stk.e), compute = TRUE), control.compute = list(dic = TRUE, waic = TRUE, cpo = TRUE, config = TRUE), verbose = FALSE, control.inla=list(int.strategy = &quot;grid&quot;, diff.logdens = 4, strategy = &quot;laplace&quot;, npoints = 21)) summary(res) ## Time used: ## Pre = 1.15, Running = 98.2, Post = 1.4, Total = 101 ## Fixed effects: ## mean sd 0.025quant 0.5quant 0.975quant mode kld ## Intercept 1.185 0.080 1.031 1.184 1.345 1.184 0 ## x3 -0.034 0.059 -0.149 -0.034 0.083 -0.034 0 ## x4 -0.107 0.057 -0.220 -0.107 0.006 -0.107 0 ## x7 -0.102 0.066 -0.233 -0.102 0.025 -0.102 0 ## x16 -0.061 0.035 -0.130 -0.061 0.007 -0.061 0 ## x20 0.105 0.037 0.032 0.105 0.177 0.105 0 ## x31 -0.061 0.025 -0.110 -0.061 -0.011 -0.061 0 ## x37 -0.036 0.031 -0.096 -0.036 0.025 -0.036 0 ## x40 0.032 0.038 -0.043 0.032 0.106 0.032 0 ## ## Random effects: ## Name Model ## spatial_effect SPDE2 model ## Settlement_Type IID model ## ## Model hyperparameters: ## mean sd 0.025quant 0.5quant 0.975quant mode ## Precision-parameter for the Gamma observations 3.42 0.143 3.17 3.41 3.73 3.37 ## Theta1 for spatial_effect -2.88 0.130 -3.14 -2.88 -2.63 -2.88 ## Theta2 for spatial_effect 2.12 0.141 1.84 2.12 2.40 2.12 ## Precision for Settlement_Type 1043.92 1934.071 80.42 514.99 5397.20 189.05 ## ## Deviance Information Criterion (DIC) ...............: 6611.44 ## Deviance Information Criterion (DIC, saturated) ....: 1966.32 ## Effective number of parameters .....................: 313.78 ## ## Watanabe-Akaike information criterion (WAIC) ...: 6798.76 ## Effective number of parameters .................: 352.24 ## ## Marginal log-Likelihood: -3493.70 ## CPO, PIT is computed ## Posterior summaries for the linear predictor and the fitted values are computed ## (Posterior marginals needs also &#39;control.compute=list(return.marginals.predictor=TRUE)&#39;) 10.2.2.1 Predictions In-sample predictions can be extracted using the inla.stack.index() function and the corresponding predicted density (computed through extracting summary.linear predictor[index, \"mean\"] from the above model) added into a new data frame with the data.frame() function. #extract predictions index &lt;- inla.stack.index(stk.e, &quot;est&quot;)$data #compute predicted density and include in new data frame in_sample &lt;- data.frame(predicted_density = exp(res$summary.linear.predictor[index, &quot;mean&quot;])) The observed population and observed density values from the (standardised) demographic data can be selected in order to be used later in the in-sample model metric assessment. #select observed population, density and building count metrics_data &lt;- Data_CMR_std %&gt;% dplyr::select(Total_Pop, Density, Total_Building_Count) %&gt;% cbind(in_sample) The predicted density found above is multiplied by the Total_Building_Count extracted from the observed data to obtain the predicted population. #predicted population = predicted density x observed total building count metrics_data &lt;- metrics_data %&gt;% mutate(predicted_population = predicted_density * Total_Building_Count) #rename variables in proper format metrics_data &lt;- metrics_data %&gt;% rename(observed_population = Total_Pop, observed_density = Density) Finally, the model performance metrics can be computed for both the density and total population. Similar to as in Module 9, metrics such as the bias, MSE, RMSE and correlation are of interest, in addition to the inaccuracy and imprecision. #density metrics density_metrics &lt;- metrics_data %&gt;% mutate(residual = observed_density - predicted_density) %&gt;% summarise( Bias= mean(residual), Imprecision = sd(residual), Inaccuracy = mean(abs(residual)), mse = mean((residual)^2), rmse = sqrt(mse), Corr = cor(observed_density, predicted_density)) density_metrics %&gt;% kable() Bias Imprecision Inaccuracy mse rmse Corr 0.7522486 21.23032 2.618071 451.0086 21.23696 0.8579087 #total population metrics pop_metrics &lt;- metrics_data %&gt;% mutate(residual = observed_population - predicted_population) %&gt;% summarise( Bias= mean(residual), Imprecision = sd(residual), Inaccuracy = mean(abs(residual)), mse = mean((residual)^2), rmse = sqrt(mse), Corr = cor(observed_population, predicted_population)) pop_metrics %&gt;% kable() Bias Imprecision Inaccuracy mse rmse Corr -157.4186 1335.959 518.6163 1808445 1344.784 0.4172116 Later in this section, the chosen fitted model will be used to make predictions for the whole of Cameroon at 100m resolution. In order to do this, the stacked covariates at the grid cell level are required and hence need to be loaded first. #load our stacked covariate pred_covs &lt;- readRDS(paste0(data_path, &quot;CMR_prediction_stack.rds&quot;)) #check variable names names(pred_covs) ## [1] &quot;Country&quot; &quot;Regions&quot; &quot;Regions_libelle&quot; ## [4] &quot;Department&quot; &quot;Department_libelle&quot; &quot;CMR_buildings_count&quot; ## [7] &quot;CMR_buildings_total_area&quot; &quot;Settlement_Type&quot; &quot;x3&quot; ## [10] &quot;x4&quot; &quot;x40&quot; &quot;x7&quot; ## [13] &quot;x16&quot; &quot;x20&quot; &quot;x31&quot; ## [16] &quot;x37&quot; &quot;Long&quot; &quot;Lat&quot; Once the stacked covariates are loaded, they need to be scaled. This can be done using the stdsize() function created earlier. #scale stacked covariates. vars &lt;- c(&quot;x3&quot;, &quot;x4&quot;, &quot;x7&quot;, &quot;x16&quot;, &quot;x20&quot;, &quot;x31&quot;, &quot;x37&quot;, &quot;x40&quot;) pred_covs[, vars] &lt;- apply(pred_covs[,vars], 2, stdise) #check scaled covariates head(pred_covs) ## Country Regions Regions_libelle Department Department_libelle CMR_buildings_count ## 1 1 4 Extrême Nord 27 Logone et Chari 2 ## 2 1 4 Extrême Nord 27 Logone et Chari 4 ## 3 1 4 Extrême Nord 27 Logone et Chari 4 ## 4 1 4 Extrême Nord 27 Logone et Chari 3 ## 5 1 4 Extrême Nord 27 Logone et Chari 2 ## 6 1 4 Extrême Nord 27 Logone et Chari 1 ## CMR_buildings_total_area Settlement_Type x3 x4 x40 x7 x16 x20 ## 1 31.92027 4 3.652087 -0.1820496 -0.2584178 0.3503400 -0.9924022 -0.5053741 ## 2 59.91387 4 3.618600 -0.2862564 -0.2450828 0.3398376 -1.2439477 -0.2432839 ## 3 69.81126 4 3.617716 -0.2871699 -0.2450828 0.3398094 -1.2346939 -0.2478792 ## 4 52.94994 4 3.647733 -0.1728132 -0.2523058 0.3305358 -1.1412598 -0.6290872 ## 5 26.55466 4 3.578709 -0.3229593 -0.1988979 0.3473500 -1.2439477 -0.4677334 ## 6 11.27224 4 3.578503 -0.3236958 -0.2096400 0.3463284 -1.2439477 -0.4657714 ## x31 x37 Long Lat ## 1 6.158775 -0.9666826 14.18500 13.04333 ## 2 4.844915 -0.9666826 14.29583 13.03833 ## 3 4.874303 -0.9666826 14.29583 13.03750 ## 4 6.319944 -0.9666826 14.16500 13.03417 ## 5 3.567499 -0.9666826 14.28917 12.99917 ## 6 3.567499 -0.9666826 14.29000 12.99917 10.2.3 Posterior distribution simulation Whilst it is possible to make predictions within INLA through adding covariates to the observed data as seen in Module 8, in this case, the dataset is very large and it is then preferable to make predictions outside of INLA to save computational time. In order to perform grid cell level prediction, first posterior distribution simulation must take place, simulating posteriors from the parameter estimates from the chosen fitted model above. For demonstrative purposes, here only 100 posteriors will be simulated, however, the default in INLA is to sample 1000 posteriors which takes notably more computational time. Samples can be taken from the posterior distribution through using the function inla.posterior.sample() with arguments n (the number of samples to be taken), result (the inla object, in this case the inla model res), seed (for reproducible results) and num.threads (the number of outer and inner threads). #sample from posterior in model samples &lt;- inla.posterior.sample(n = 100, result = res, seed = 1234, num.threads = &quot;1:1&quot;) To easily store the various model parameters, a function can be created with the functions inla.posterior.sample.eval() and function(). Within the function, get() can be used, which searches by name for an object, in this case, it is used to search for the covariates of interest. #create a function to store the various model parameters sam.extract &lt;- inla.posterior.sample.eval( (function(...) { beta.1 &lt;- get(&quot;x3&quot;) beta.2 &lt;- get(&quot;x4&quot;) beta.3 &lt;- get(&quot;x7&quot;) beta.4 &lt;- get(&quot;x16&quot;) beta.5 &lt;- get(&quot;x20&quot;) beta.6 &lt;- get(&quot;x31&quot;) beta.7 &lt;- get(&quot;x37&quot;) beta.8 &lt;- get(&quot;x40&quot;) prec.1 &lt;- get(&quot;Settlement_Type&quot;) return(c(Intercept, beta.1, beta.2,beta.3, beta.4, beta.5, beta.6, beta.7, beta.8, prec.1)) }), samples) Once the function has been created, it can be used to return the summarised posteriors as follows. #summarised posteriors print(round(dig = 4, rowMeans(sam.extract))) ## fun[1] fun[2] fun[3] fun[4] fun[5] fun[6] fun[7] fun[8] fun[9] fun[10] fun[11] fun[12] fun[13] ## 1.1828 -0.0266 -0.1047 -0.1005 -0.0558 0.1056 -0.0622 -0.0428 0.0270 0.0933 -0.0032 -0.0421 -0.0427 The posteriors can then be saved as new objects which will make them easier to identify and use in the later steps. For the random effect for settlement types, an index can be assigned to the posteriors for the 4 different settlement types available in the data. #save posteriors as new object to make them easier to identify Intercept &lt;- sam.extract[1, ]#intercept betas &lt;- sam.extract[2:9, ]#betas alpha_settlement_type &lt;- sam.extract[10:13, ]#random effect for settlement types #assign index to the posteriors for the settlement type alpha_settlement_type &lt;- alpha_settlement_type %&gt;% as_tibble() %&gt;% mutate(Settlement_Type = c(1, 2, 3, 4)) To make the predictions, posteriors are assigned to Settlement_Type using the select() function from the dpylr package and the left_join() function. #assign posteriors to Settlement_Type predict_settlement_type &lt;- pred_covs %&gt;% dplyr::select(Settlement_Type)%&gt;% left_join(alpha_settlement_type, by = c(&quot;Settlement_Type&quot;)) %&gt;% dplyr::select(-Settlement_Type) The spatial random component also needs to be obtained from the chosen fitted model. In order to obtain this component, the longitude (Long) and Latitude (Lat) need to be obtained from the predicted covariates (pred_covs) and then project the mesh (meshb) that was created earlier to these locations. #get the spatial effect parameter in the SPDE xy coordinate of predictions coord1 &lt;- cbind(pred_covs$Long, pred_covs$Lat) #remake the A matrix for prediction Aprediction &lt;- inla.spde.make.A(mesh = meshb, loc = as.matrix(coord1)) dim(Aprediction) ## [1] 1327458 11692 The next step is to get the spatial effect parameter from the model through, this can be done through extracting the spatial_effect from the summary.random part of the model and specifying that the mean will be subset. #get the spatial effect parameter from the model sfield_nodes &lt;- res$summary.random$spatial_effect[&#39;mean&#39;] dim(sfield_nodes) ## [1] 11692 1 The resulting spatial effect parameter can be given as a data frame and combined with the projected matrix Aprediction to estimate the values for the spatial component. #estimate the values for the spatial component in the prediction covs spatial_field &lt;- (Aprediction %*% as.data.frame(sfield_nodes)[, 1]) Predictions for the covariates using their coefficients (the betas) that were extracted above can then be made, once again through using the select() function to obtain the covariate information. It is important to replace the NA values with 0 as otherwise numerical issues can occur given that R often works better with zeroes than with missing values. Given that anything (the betas in this case) multiplied by 0 is still 0, the final result is not changed by changing the missing values to zero. #extract covariates and convert it to a matrix cov_fixed &lt;- pred_covs %&gt;% dplyr::select(x3, x4, x7, x16, x20, x31, x37, x40) %&gt;% #avoid numerical issues with the following line mutate_at(vars(starts_with(&quot;x&quot;)), ~replace(., is.na(.), 0)) %&gt;% as.matrix() dim(cov_fixed) ## [1] 1327458 8 To predict the (fixed effect) covariates, the estimated beta parameters can be multiplied with their respective covariates and then converted to a data frame with the as_tibble() function. #predict fixed effect covariates cov_fixed &lt;- cov_fixed %*% betas #convert to data frame cov_fixed &lt;- as_tibble(cov_fixed) Given that there is now a value for the intercept, fixed effect (based on the covariates), random effect for the settlement type and the spatial random effect, the model component can be added for the prediction. Since a gamma distribution was used in the fitting of the model, for the predicted posterior density estimates, the predicted density must be back transformed with the exponential function. #predicted posterior density predicted_density &lt;- exp(Intercept + cov_fixed + predict_settlement_type + spatial_field[,1]) The mutate() function is then used to add the building count for each grid, followed by estimating the predicted population. #add building count for each grid predicted_density &lt;- predicted_density %&gt;% mutate(buidling_count = pred_covs$CMR_buildings_count) #estimate predicted population predicted_pop &lt;- predicted_density %&gt;% mutate_at(vars(starts_with(&quot;sample&quot;)), ~ . * buidling_count) %&gt;% dplyr::select(-buidling_count) In order to compute the predicted population for the entire study, the predicted population using the mean needs to be summarised, as well as finding the quantiles which correspond to the uncertainty, where the summarising can be done using the summarise() function. #total predicted population and uncertainty CMR_Total_Pop &lt;- predicted_pop %&gt;% apply(2, sum, na.rm = TRUE) %&gt;% as_tibble()%&gt;% summarise(mean_population = round(mean(value)), upper_quantile = round(quantile(value, probs=0.975)), lower_quantile = round(quantile(value, probs =0.025))) CMR_Total_Pop %&gt;% kable() mean_population upper_quantile lower_quantile 31438878 35486909 27569181 10.3 Aggregation to area units of interest and uncertainty quantification The focus of this section is on quantifying the uncertainty resulting from high-resolution population estimates through using a Bayesian hierarchical modelling framework. In addition to providing the point estimates, the model generates full posterior distributions for each 100 grid cell which enables a probabilistic understanding of the population size. These uncertainty measures are able to be aggregated across any spatial scale and are crucial for informed decision-making, particularly in regions where the census data is sparse or outdated. Traditional census-based estimates often overlook or fail to report their inherent uncertainties, which may lead users into assuming false precision. To estimate the population totals and uncertainty at the various admin levels, the following code can be used. First, obtain the region names and cbind() them to the predicted population. #get region names region_names &lt;- pred_covs %&gt;% dplyr::select(Regions_libelle) #cbind names to predictions region_estimates &lt;- cbind(predicted_pop, region_names) %&gt;% as_tibble() Then, for easy processing, the names of the regions can be grouped with the group_by() function and split the data with the group_split() function. #group and split the data region_estimates &lt;- region_estimates %&gt;% group_by(Regions_libelle) %&gt;% group_split() The following for loop is then used to get the estimates and and the credible intervals for each of the regions. #get estimates and credible intervals OUT &lt;- list() for(dd in 1:length(region_estimates)){ df &lt;- region_estimates[[dd]] #get the ID of the current region being processed typro &lt;- unique(df$Regions_libelle) print(typro) df &lt;- df %&gt;% dplyr::select(starts_with(&quot;sample&quot;)) %&gt;% apply(2, sum, na.rm = TRUE) OUT[[dd]] &lt;- c(Health_Area_Names = typro, mean = mean(df), lower_quantile = quantile(df, 0.025), upper_quantile = quantile(df, 0.975), median = quantile(df, 0.500)) #print(OUT) } ## [1] &quot;Adamaoua&quot; ## [1] &quot;Centre&quot; ## [1] &quot;Est&quot; ## [1] &quot;Extrême Nord&quot; ## [1] &quot;Littoral&quot; ## [1] &quot;Nord&quot; ## [1] &quot;Nord Ouest&quot; ## [1] &quot;Ouest&quot; ## [1] &quot;Sud&quot; ## [1] &quot;Sud Ouest&quot; region_population &lt;- do.call(rbind, OUT) region_population %&gt;% kable() Health_Area_Names mean lower_quantile.2.5% upper_quantile.97.5% median.50% Adamaoua 1656233.13457392 1397448.19572542 1975569.98280791 1634858.39162699 Centre 5240829.83168103 4169786.03848485 6728065.26384402 5246897.68185082 Est 1469676.34802464 1264836.64270952 1694937.1175424 1460197.68508379 Extrême Nord 5321809.9097725 4456588.19561609 6295653.85743602 5231717.09842551 Littoral 5306061.36502468 3301251.8711992 7325935.42445482 5126804.49795949 Nord 3464449.10907342 2944790.23293535 4051975.0819395 3447535.52901116 Nord Ouest 1410147.10329959 1138622.90622669 1737330.6527433 1394107.61986643 Ouest 2906398.85726453 2549972.16639807 3297339.00776236 2905888.44559432 Sud 1155513.29605593 932777.803044887 1483673.28523653 1148944.86372355 Sud Ouest 3507758.91094667 2554022.47544692 4698940.44909301 3438697.55742504 Finally, the regional estimates can be exported as a .csv file with the write.csv() function. #export the regional estimate as a .csv file write.csv(region_population, paste0(output_path, &quot;Regional Estimate.csv&quot;)) 10.3.1 Rasterising the predictions at grid cell level The final stage of grid cell level prediction is to rasterise the resulting predicted population to a 100m resolution and find the corresponding credible intervals for the uncertainty quantification before exporting the results. To begin with this final stage, the pixel level predictions are summarised as follows. #summarise pixel level predictions #mean population mean_population &lt;- rowMeans(predicted_pop, na.rm = TRUE) #median population median_population &lt;- apply(predicted_pop, 1, FUN = function(x) quantile(x, probs = 0.5, na.rm = TRUE)) #standard deviation of population std_population &lt;- apply(predicted_pop, 1, sd) #lower quantile for credible interval lower_quantile &lt;- apply(predicted_pop, 1, FUN = function(x) quantile(x, probs = 0.025, na.rm = TRUE)) #upper quantile for credible interval upper_quantile &lt;- apply(predicted_pop, 1, FUN = function(x) quantile(x, probs = 0.975, na.rm = TRUE)) #uncertainty quantification uncertainty = (upper_quantile - lower_quantile)/mean_population #coefficient of variation coe_var = std_population/mean_population The resulting mean and median populations can be summed for the overall population size estimates. #sum predictions sum(median_population, na.rm = TRUE) ## [1] 30995326 sum(mean_population, na.rm = TRUE) ## [1] 31438878 Before the rasterisation, the predictions must be combined with the cbind() function to the xy coordinates. #cbind predictions to xy coordinates pixel_predictions &lt;- cbind(mean_population, median_population, std_population, lower_quantile, upper_quantile, uncertainty, coe_var) %&gt;% as_tibble() %&gt;% mutate(x = pred_covs$Long, y = pred_covs$Lat) The country raster can then be loaded and used as a mastergrid for the rasterisation process. It is important to note that this is just one method of carrying out the rasterisation process, and there are other methods available. #load country raster r1 &lt;- rast(paste0(data_path, &quot;CMR_Regions.tif&quot;)) plot(r1) The above predictions are then converted with the st_as_st() function to an sf object and the st_crs() function is used to set the CRS of the resulting sf object. #convert pixel_predictions to an sf object pop_sf &lt;- st_as_sf(pixel_predictions, coords = c(&quot;x&quot;, &quot;y&quot;)) #set the CRS of the sf object st_crs(pop_sf) &lt;- 4326 The final step before the rasterisation is to re-project to the raster spatial reference with the st_transform() object. #re-project to raster spatial reference pop_sf &lt;- st_transform(pop_sf, crs = st_crs(r1)) Finally, the rasterise() function is used to rasterise the mean population as well as the upper and lower limits contained within the sf object. For each result, the function writeRaster() is used to export the results. Whilst only the mean population and credible interval limits are rasterised here, other population measures such as the standard deviation, coefficient of variation and median population can also be rasterised in the same way, #rasterise mean population mean_pop_raster &lt;- rasterize(pop_sf, r1, field = &quot;mean_population&quot;) plot(mean_pop_raster) writeRaster(mean_pop_raster, paste0(output_path, &quot;total_population_raster.tif&quot;), overwrite = TRUE, names = &quot;Population&quot;) #rasterise Upper upper_pop_raster &lt;- rasterize(pop_sf, r1, field = &quot;upper_quantile&quot;) plot(upper_pop_raster) writeRaster(upper_pop_raster, paste0(output_path, &quot;population_Upper.tif&quot;), overwrite = TRUE, names = &quot;Population_Upper&quot;) #rasterise lower lower_pop_raster &lt;- rasterize(pop_sf, r1, field = &quot;lower_quantile&quot;) plot(lower_pop_raster) writeRaster(lower_pop_raster, paste0(output_path, &quot;population_Lower.tif&quot;), overwrite = TRUE, names = &quot;Population_Lower&quot;) It is good practice to check whether the rasters have the resolution, extent and coordinate reference, this can be done simply by printing the rasters themselves and checking their summaries as seen below. For these two rasters, the resolution, extent and coordinate references are identical, and therefore the rasters are spatially aligned and can be used for the predictions. #mean population raster mean_pop_raster ## class : SpatRaster ## dimensions : 13710, 9231, 1 (nrow, ncol, nlyr) ## resolution : 0.0008333333, 0.0008333333 (x, y) ## extent : 8.497917, 16.19042, 1.652917, 13.07792 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat WGS 84 (EPSG:4326) ## source(s) : memory ## varname : CMR_Regions ## name : last ## min value : 0.6243253 ## max value : 6223.3237246 #country raster r1 ## class : SpatRaster ## dimensions : 13710, 9231, 1 (nrow, ncol, nlyr) ## resolution : 0.0008333333, 0.0008333333 (x, y) ## extent : 8.497917, 16.19042, 1.652917, 13.07792 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat WGS 84 (EPSG:4326) ## source : CMR_Regions.tif ## name : Regions ## min value : 1 ## max value : 10 10.4 Useful resources Overall: Bayesian Modelling of Spatio Temporal Data with R Handling raster data: R as GIS for Economists INLA-SPDE approach: Introduction to INLA for geospatial modelling Posterior distribution simulation: Geospatial Health Data: Modeling and Visualization with R-INLA and Shiny Uncertainty quantification: National population mapping from sparse survey data: A hierarchical Bayesian modeling framework to account for uncertainty "],["age-sex-disaggregation.html", "11 Age-Sex Disaggregation 11.1 Disaggregation of population totals by age-sex proportions 11.2 Useful resources", " 11 Age-Sex Disaggregation This is the final module and covers disaggregation of population totals by age-sex proportions for instances where there is age-sex data available as well as instances where there isn’t. 11.1 Disaggregation of population totals by age-sex proportions Data disaggregation can be defined as the process of breaking data down into some predefined sub-categories and is helpful in the understanding of disparities and targeting interventions. Two common sub-categories are age and sex. Age-sex disaggregation is of importance for several reasons. Firstly, it aids in the ability to make informed decisions for specific age and sex groups for policy making. Secondly, it can help with the effective allocation of resources, by allocating resources based on the specific needs of different groups of individuals. Finally, it is beneficial for monitoring and evaluation of different programmes through assessing the impact and progress of each programme. There are three main methods for age-sex disaggregation, given as follows. Surveys and censuses: data is collected from questionnaires such as the Demographic and Health Surveys (DHS) and censuses. Administrative records: Data is collected from different institutions such as enrolment records from schools or patient data from hospitals. Sample surveys: data is collected from a representative sample such as the labour force surveys or health surveys. Several challenges are faced when conducting age-sex disaggregation. Issues can arise if the data is of poor quality, for example, if the data available is incomplete or inaccurate. Additionally, there can be privacy concerns regarding how to ensure the confidentiality and security of the data. The last main concern is that of constraints on the resources, as often funding is limited as well as a constrained technical capacity. Data availability is another thing that needs to be considered as there are two cases of data availability, either there is age-sex data available, or there isn’t. 11.1.1 Unavailable age-sex data If the age-sex data is unavailable, there are a few options on which method to use in order to obtain this data. Statistical methods can be used to impute the missing information, for example, multiple imputation or regression modelling. Information from a related data source can be used as a proxy to estimate the missing information, for example, if there is age-sex data available at national level but not at a lower level, the national level age-sex distribution, often from a census, can be used on the data of interest. If there are known ratios or proportions from similar datasets available, then these ratios or proportions can be allocated to the aggregated data to estimate the distribution of age and sex. Multiple related data sources can be combined to create a synthetic population, for example, estimating the age-sex structure of the population using population pyramids. Multiple partial datasets can be merged to form a more complete dataset, for example, merging survey data with administrative records. Derive age-sex disaggregated data for smaller geographic areas or population segments based on a larger scale through using small area estimation methods. Machine learning models can be trained using other attributes in the given dataset in order to predict the age and sex, for example, decision trees and random forests. Prior information and uncertainty can be incorporated in the estimation processes using Bayesian methodology to provide a probabilistic framework for disaggregating data. 11.1.2 Available age-sex data If the age-sex data is available, there are statistical methods and techniques available to refine the age distributions, which are particularly useful when dealing with incomplete or grouped data. For this, the DemoTools package in R is of use, along with the following methods. Penalised Composite Link Model (PCLM): uses a penalised likelihood approach to distribute aggregated age data into narrower age intervals. This is particularly useful to refine the age distributions from census or survey data when the intervals of age are wide. Sprague Multipliers: often used in demographic studies when the population data needs smoothing by interpolating the age distributions from broad age intervals into single-year intervals. Beers’ Ordinary and Modified Methods: creates detailed age distributions with single-year intervals from grouped data by fitting a polynomial curve to the existing age distribution. Karup-King Method: fits a polynomial curve to the age distribution to distribute the population counts into narrower age categories. Zigzag Method: often improves the accuracy when used alongside other methods as it is a smoothing technique that adjusts the age data to reduce the number of irregularities. 11.1.2.1 Age-sex disaggregation When there is age-sex data available, the data can be disaggregated with the function given later in this section. However, first, the required functions of raster and peanutButter need to be loaded. An optional package is the tictoc package which can be used to time how long the end function takes to run. #load packages library(tictoc) library(raster) #devtools::install_github(&#39;wpgp/peanutButter&#39;) library(peanutButter) Then, the data needs to be input into the R environment and settings specified. #input data population &lt;- raster(paste0(data_path, &quot;pop_hat_total.tif&quot;)) dept &lt;- raster(paste0(data_path, &quot;CMR_Department.tif&quot;)) proportions &lt;- read.csv(paste0(data_path, &quot;CMR_2020_agesex.csv&quot;), stringsAsFactors = FALSE) #settings country &lt;- &#39;CMR&#39; #country code version &lt;- &#39;v1_0&#39; woprname &lt;- paste0(country, &#39;_population_&#39;, version,&#39;_&#39;) Once the data has been input, the department raster can be plotted to aid in visualising the dataset. #plot departments raster plot(dept) Before creating the function which performs the age-sex disaggregation itself, a function that creates a population grid for each of the age-sex groups, returning the corresponding group population can be created, as seen below. Within the age-sex disaggregation function, the demographic() function from WorldPop’s peanutButter package is required. Whilst the function can be called with peanutButter::demographic(), for understanding how age-sex disaggregation works, it is important to understand the key functions. Therefore, the required arguments, key steps and a breakdown of the function itself are given below. The arguments that need to be included in the function are as follows. population: the raster file for the population (population in the Cameroon example). group_select: the group which you wish to create the population grid for (group in the Cameroon example). regions: the raster file for the regions (dept in the Cameroon example). proportions: the .csv file for the proportions of each age-sex category (`proportions in the Cameroon example). Within the function itself, there are 6 key steps. Format the proportions data by appropriately naming the first column id and renaming each of the rows to be the correct ID number. Sum the proportions for the selected group. If the ‘group’ is one age-sex combination, then there will be only one proportion available. If multiple age-sex combinations are selected, sum the individual corresponding proportions for an overall proportion of that group. Reduce the columns to contain only the ID numbers and the relevant proportions. Save the raster coordinate system with the crs() function and extract the minimum and maximum x and y coordinates of the selected region to save the extent of the boundary. Convert the regions raster and proportions to a matrix, then rasterise the group proportions. Compute the age-sex population through multiplying the population by the group proportion. #function to create population grid for each age-sex group demographic &lt;- function(population, group_select, regions, proportions){ #format proportions names(proportions)[1] &lt;- &#39;id&#39; row.names(proportions) &lt;- proportions$id #sum the proportions for the selected group if(length(group_select)==1){ proportions$prop &lt;- proportions[, group_select] } else { proportions$prop &lt;- apply(proportions[,group_select], 1, sum, na.rm=TRUE) } #reduce cols proportions &lt;- proportions[, c(1, ncol(proportions))] #save the raster coordinate system crs1 &lt;- raster::crs(regions) #save the extent of the boundary (minimum and maximum x and y coordinates) ex1 &lt;- raster::xmin(regions) ex2 &lt;- raster::xmax(regions) ex3 &lt;- raster::ymin(regions) ex4 &lt;- raster::ymax(regions) #convert the regions raster to matrix regions &lt;- raster::as.matrix(regions) #proportions to matrix group_proportion &lt;- regions group_proportion[] &lt;- NA for(id in proportions$id){ group_proportion[which(regions == id)] &lt;- proportions[as.character(id), &#39;prop&#39;] } #rasterise the group proportions group_proportion &lt;- raster::raster(group_proportion, crs = crs1, xmn = ex1, xmx = ex2, ymn = ex3, ymx = ex4) #calculate age-sex population group_population &lt;- population * group_proportion return(group_population) } The next step is to create the age-sex disaggregation function itself. For ease of understanding, the required arguments and the key steps of the function are given below, before the details of the function itself. It is important to note that the population information for each age-sex category is of interest, so a full breakdown of the population is needed. However, it is also of interest in many instances to have information on children (both sexes) under the age of 1, under the age of 5 and under the age of 15 in addition to females of reproductive age between 15 and 49, so the population information for these specific groups is also computed within the function. The required arguments for the age-sex disaggregation function (woprAgeSexRasters()) are as follows. country: the country code for the country that the dataset is for. version: the current version/copy of the disaggregation being carried out. outdir: the out directory where the results will be saved. population: the raster dataset for the population. regions: the raster dataset for the regions. proportions: the .csv file for the proportions of each age-sex category. The key steps within the age-sex disaggregation function are. Create the out directory with the dir.create() function. Create a variable that contains the names of each age-sex category (group). Iterate between each of the groups, performing the demographic() function for each group, saving the results as a raster with the writeRaster() function from the raster package. Compute the population information through stacking the age-sex for each of the other groups of interest (all children under 1, under 5 and under 15 as well as females from age 15 to 49). #age-sex disaggregation function woprAgeSexRasters &lt;- function(country, version, outdir, population, regions, proportions){ #create out directory dir.create(outdir, recursive = TRUE, showWarnings = FALSE) woprname &lt;- paste0(country, &#39;_population_&#39;, version, &#39;_agesex_&#39;) #create the groups for the age-sex combinations groups &lt;- c(paste0(&#39;f&#39;, c(0, 1, seq(5, 80, 5))), #female + age combinations paste0(&#39;m&#39;, c(0, 1, seq(5, 80, 5)))) #male + age combinations #iterate between each of the groups for(group in groups){ #create the out file path cat(paste0(group, &#39;\\n&#39;)) outfile &lt;- file.path(outdir, paste0(woprname,group, &#39;.tif&#39;)) if(!file.exists(outfile)){ #create and save a raster that contains the demographic information #for each group raster::writeRaster(x = demographic(population = population, group_select = group, regions = regions, proportions = proportions), filename = outfile, overwrite = TRUE) } } #create the raster files for all children (both sexes) under age 1 cat(&#39;U1 \\n&#39;) agesex_stack &lt;- raster::stack(file.path(outdir, paste0(woprname,c(&#39;m0&#39;, &#39;f0&#39;), &#39;.tif&#39;))) result &lt;- raster::stackApply(agesex_stack, raster::nlayers(agesex_stack), fun = sum) result[is.na(population)] &lt;- NA raster::writeRaster(result, file.path(outdir, paste0(woprname, &#39;under1.tif&#39;)), overwrite = TRUE) rm(agesex_stack, result); gc() #create the raster files for all children (both sexes) under age 5 cat(&#39;U5 \\n&#39;) agesex_stack &lt;- raster::stack(file.path(outdir, paste0(woprname,c(&#39;m0&#39;, &#39;f0&#39;, &#39;m1&#39;, &#39;f1&#39;), &#39;.tif&#39;))) result &lt;- raster::stackApply(agesex_stack, raster::nlayers(agesex_stack), fun = sum) result[is.na(population)] &lt;- NA raster::writeRaster(result, file.path(outdir, paste0(woprname, &#39;under5.tif&#39;)), overwrite = TRUE) rm(agesex_stack, result); gc() #create the raster files for all children (both sexes) under age 15 cat(&#39;U15 \\n&#39;) agesex_stack &lt;- raster::stack(file.path(outdir, paste0(woprname,c(&#39;m0&#39;,&#39;f0&#39;,&#39;m1&#39;,&#39;f1&#39;, &#39;m5&#39;,&#39;f5&#39;,&#39;m10&#39;, &#39;f10&#39;), &#39;.tif&#39;))) result &lt;- raster::stackApply(agesex_stack, raster::nlayers(agesex_stack), fun = sum) result[is.na(population)] &lt;- NA raster::writeRaster(result, file.path(outdir, paste0(woprname, &#39;under15.tif&#39;)), overwrite = TRUE) rm(agesex_stack, result); gc() #create the raster files for females aged 15 - 49 cat(&#39;F15-49 \\n&#39;) agesex_stack &lt;- raster::stack(file.path(outdir, paste0(woprname, paste0(&#39;f&#39;, seq(15,45,5)), &#39;.tif&#39;))) result &lt;- raster::stackApply(agesex_stack, raster::nlayers(agesex_stack), fun = sum) result[is.na(population)] &lt;- NA raster::writeRaster(result, file.path(outdir, paste0(woprname, &#39;f15_49.tif&#39;)), overwrite = TRUE) rm(agesex_stack, result); gc() } Finally, with the necessary packages installed, data set-up and functions defined, the age-sex disaggregation function can be applied to the chosen dataset, in this case the Cameroon data. Whilst it is not necessary to time how long the function takes to complete, since it does take a long time to finish running due to the large size of population datasets and rasters, it can be of interest to use the tic() and toc() functions to see how long it takes for the function to finish running. #apply age-sex disaggregation function to Cameroon data tic() woprAgeSexRasters(country = country, version = version, outdir = output_path, population = population, regions = dept, proportions = proportions) toc() 11.2 Useful resources Aggregate demographic analysis: DemoTools Penalised Composite Link Model: Efficient Estiamtion of Smooth Distributions From Coarsely Grouped Data Sprague Multipliers: Estimating school-age populations by applying Sprague multipliers to raster data Beers’ Ordinary and Modified Methods: Create the Beers ordinary or modified coefficient matrix Karup-King Method: Karup-King-Newton method of population count smoothing Zigzag Method: Smoothing with DemoTools peanutButter package: WorldPop GitHub peanutButter package: peanutButter: An R package to produce rapid-response gridded population estimates from building footprints, version 0.1.0 "],["exercise-solutions-by-module.html", "12 Exercise Solutions by Module 12.1 Module 1 12.2 Module 2 12.3 Module 3 12.4 Module 4 12.5 Module 5", " 12 Exercise Solutions by Module 12.1 Module 1 12.1.1 General exercises Exercise: Which of the following will give a different output from the other 3? Hint: Look at the help file for the log() function. log(x=6, base=4) log(4, 6) log(base=4, x=6) log(6, 4) Solution: ?log, 2. log(4,6) Exercise: Create a vector that goes from 5 to 25 by increments of 1. Solution: 5:25 (or some variation) Exercise: Create another vector that goes from 5 to 25 by increments of 1, but using a different method. Call this vector V1. Solution: V1 &lt;- seq(5,25,by=1) (or some variation) Exercise: Extract the 5th element of V1. Solution: V1[5] Exercise: Remove the 7th element from the vector V1 and call this new vector V2. Solution: V2 &lt;- V1[-7] Exercise: How many elements in your vector, V1, are greater than 11? Solution: sum(V1&gt;11) = 14 Exercise: Create a matrix that contains the sequence of numbers from 1 to 16, going up in increments of 1. Let the matrix have 4 rows and have the matrix elements fill by row. Call this matrix M1. Solution: M1 &lt;- matrix(data=1:16, nrow=4, byrow=T) Exercise: Extract the 3rd and 4th rows and the 1st and 2nd columns of your matrix, M1. Call this matrix M2. Solution: M2 &lt;- M1[c(3,4), c(1,2)] Exercise: Find the row sums of the matrix M2. Solution: apply(M2, 1, sum) (or rowSums(M2)) 12.1.2 End of module exercises Exercise 1: Use R to find the value of the square root of 25. (\\(\\sqrt(25)\\)) Hint: Look at the help file for the function sqrt(). Solution 1: sqrt(25) Exercise 2: Use R to find the value of the exponential of \\(6\\times 14 -3\\). (\\(\\exp(6\\times 14-3)\\)) Solution 2: exp(6*4-3) Exercise 3: Use R to find the value of the absolute value of \\(7\\times 3 - 4\\times 9 - 30\\). (\\(|7 \\times 3 - 4 \\times 9 - 30|\\)) Solution 3: abs(7*3-4*9-30) Exercise 4: Use R to find the value of \\(\\frac{73-42}{3}+2\\times\\left(\\frac{36}{4}-17\\right)\\). Give your answer to 2 decimal places. Solution 4: (73-42)/3+(2*((36/4)-17)) Exercise 5: How many elements does the following vector have? seq(from = 0, to = 100, by = 3.14) Solution 5: length(seq(0,100,by=3.14)) Exercise 6: Look at the help file for the function rnorm() and use this function to generate 100 random numbers with mean 10 and standard deviation 5. Solution 6: rnorm(n = 100, mean = 10, sd = 5) 12.2 Module 2 12.2.1 General exercises Exercise: What are the defaults for the header argument in the functions read.table() and read.csv()? header = T in read.table() and header = T in read.csv() header = T in read.table() and header = F in read.csv() header = F in read.table() and header = T in read.csv() header = F in read.table() and header = F in read.csv() Solution: header = F in read.table() and header = T in read.csv() 12.2.2 End of module exercises Exercise 1: How many cares have a miles per gallon (mpg) of less than 15 in the mtcars data? Solution 1: library(datasets) cars &lt;- mtcars sum(cars$mpg&lt;15) = 5 Exercise 2: How many cars have exactly 4 cylinders (cyl) in the mtcars data? Solution 2: sum(cars$cyl==8)=14 Exercise 3: What is the mean value of horsepower (hp) to 2 decimal places in the mtcars data? Solution 3: round(mean(cars$hp), 2)=146.69 Exercise 4: What car has the lowest miles per gallon (mpg) value for cars with 8 cylinders (cyl) in the mtcars data? Solution 4: cars[which.min(cars$mpg),]=Cadillac Fleetwood Exercise 5: What is the median miles per gallon (mpg) value for cars with 8 cylinders (cyl) in the mtcars data? Solution 5: median(cars$mpg[cars$cyl==8])= 15.2 Exercise 6: What car has the highest weight (wt) for each amount of cylinders (cyl) in the mtcars data? Solution 6: unique(cars$cyl cars[which.max(cars$wt[cars$cyl==4]),]=Mazda RX4 Wag cars[which.max(cars$wt[cars$cyl==6]),]=Hornet 4 Drive cars[which.max(cars$wt[cars$cyl==8]),]=Duster 360 Exercise 7: Create a bar plot that shows the number of cars with each gear type (gear) in the mtcars data. Solution 7: gear_types &lt;- table(cars$gear) barplot(gear_types, names.arg=c(\"Type 3\", \"Type 4\", \"Type 5\"), xlab = \"Gear types\", ylab = \"Total count\", main = \"Distribution of gear types\") Exercise 8: Create a bar plot that shows the number of cars with each gear type (gear) with the distribution of cylinders (cyl) for each type in the mtcars data. Add a legend for the cylinders. Solution 8: cyl_gear_types &lt;- table(cars$cyl, cars$gear) colours &lt;- c(\"#004C92\", \"#FF0000\", \"#FFC600\") barplot(cyl_gear_types, names.arg=c(\"Type 3\", \"Type 4\", \"Type 5\"), xlab = \"Gear types\", ylab = \"Total count\", main = \"Distribution of gear types vs cylinders\", col = colours) legend(\"topright\", rownames(cyl_gear_types), cex = 1.3, fill = colours) Exercise 9: Create a scatter plot to show the relationship between weight (wt) and miles per gallon (mpg) in the mtcars data. Solution 9: plot(x = cars$wt,y = cars$mpg, xlab = \"Weight\", ylab = \"Miles per gallon\", xlim = c(1,6), ylim = c(10,35), main = \"Weight vs miles per gallon\") ggplot(cars, aes(x = wt, y = mpg)) + geom_point(col=\"#004C92\")+ labs(x=\"Weight\", y=\"Miles per gallon\")+ ggtitle(\"Weight vs miles per gallon\") Exercise 10: Create a scatter plot to show the relationship between weight (wt) and miles per gallon (mpg) by cylinder (cyl) in the mtcars data. Solution 10: plot(cars$wt, cars$mpg, col = cars$cyl, xlab=\"Weight\", ylab=\"Miles per gallon\", main=\"Weight vs miles per gallon by cylinder\") lapply(cars$cyl, function(x) { abline(lm(mpg ~ wt, cars, subset = (cyl == x)), col = x)}) legend(\"topright\", rownames(cyl_gear_types), col = c(4,6,8), pch = 1, bty = \"n\") ggplot(cars, aes(x = wt, y = mpg, group=cyl, col=cyl)) + geom_point() + geom_smooth(method=\"lm\", se=FALSE)+ labs(x=\"Weight\", y=\"Miles per gallon\")+ ggtitle(\"Weigth vs miles per gallon by cylinder\") 12.3 Module 3 12.3.1 General exercises Exercise: How can you add multiple layers, such as polygons for states and points for health facilities, to a single map using tmap? By using tm_shape() and tm_fill() By using tm_shape() and tm_dots() By using multiple tm_shape() functions with different elements By using tm_shape() and tm_lines() Solution: 3. By using multiple tm_shape() functions with different elements 12.3.2 End of module exercises Exercise 1: What is the primary purpose of the .prj file in a shapefile set, and which package is commonly used to handle this file type in R? Stores map projection information, handled by raster package. Stores attribute information, handled by sp package Stores projection information, handled by sf package Stores metadata information, handled by terra package Solution 1: 3. Stores projection information, handled by sf package Exercise 2: Create a map of Nigeria that combines the raster layer for building count and the vector layer for region. Solution 2: library(terra) library(sf) library(tmap) #load data building_raster &lt;- rast(nga_building_count.tif) #raster data regions_vector &lt;- st_read(nga_regions.shp) #vector data #both are in same CRS if (st_crs(regions_vector)$epsg != crs(building_raster, describe=TRUE)$code) { regions_vector &lt;- st_transform(regions_vector, crs(building_raster)) } #plot tmap tmap_mode(&quot;plot&quot;) tm_shape(building_raster) + tm_raster(style = &quot;quantile&quot;, palette = &quot;YlOrRd&quot;, title = &quot;Building Count&quot;) + tm_shape(regions_vector) + tm_borders(lwd = 1, col = &quot;black&quot;) Exercise 3: Create a choropleth map that shows the variation in population density for Nigeria at region level. Export the results as a .png file. Solution 3: library(sf) library(tmap) library(dplyr) #load data nigeria_regions &lt;- st_read(nga_regions.shp) #calculate population density nigeria_regions &lt;- nigeria_regions %&gt;% mutate(pop_density = nga_pop / nga_area) #create choropleth map tmap_mode(&quot;plot&quot;) choropleth_map &lt;- tm_shape(nigeria_regions) + tm_polygons(&quot;pop_density&quot;, style = &quot;quantile&quot;, palette = &quot;Blues&quot;) + tm_layout(title = &quot;Nigeria: Population Density by Region&quot;) #save as PNG tmap_save(choropleth_map, filename = &quot;nigeria_population_density.png&quot;, width = 2000, height = 1500, dpi = 300) Exercise 4: How do you plot a raster dataset using tmap, ensuring it can handle large datasets efficiently? tm_shape(raster_data) + tm_fill() tm_shape(raster_data) + tm_raster() tm_shape(raster_data) + tm_polygons() tm_shape(raster_data) + tm_dots() Solution 4: 2. tm_shape(raster_data) + tm_raster() 12.4 Module 4 12.4.1 General exercises Exercise: Identify the dependent and independent variables in the following sentence: ‘A researcher investigates the effects of school density on the grades of its pupils.’ Solution: Dependent variable: the grades of the pupils, independent variable: school density Exercise: Fit a Poisson GLM using the ccancer dataset exploring the relationship between the count of cancer deaths and the covariates for the cancer site and region in Canada. Is the relationship between the dependent and independent variables significant? Solution: ccancer_glm_exercise &lt;- glm(Count ~ Site + Region, family = \"poisson\", data = ccancer) summary(ccancer_glm_exercise) Yes, the relationships are significant. Exercise: What is the expected birth weight for a baby boy at a gestational age of 39.5 weeks? Solution: \\[Weight = -1447.24 -163.04\\times 0+120.89\\times 39.5 = 3327.92g\\] 12.5 Module 5 12.5.1 General exercises Exercise: Following on from the example above, what is the probability that an individual that is vaccincated but has a high fever is infected with disease A? Solution: 0.19 Exercise: Following on from the example above, what is the conditional probability of drawing a purple button given that the button drawn is not blue? Solution: \\[ \\begin{aligned} P(A|B)&amp;=\\frac{P(A\\cap B)}{P(B)}\\\\ &amp;= \\frac{\\text{number of pruple buttons}/\\text{total number of buttons}}{\\text{blue}/\\text{total number of buttons}}\\\\ &amp;=\\frac{4/10}{6/10}\\\\ &amp;=\\frac{2}{3}=0.67. \\end{aligned} \\] Exercise: Following on from the example above, what would the probability be of selecting 2 men from the island without replacement? Solution: \\[ \\begin{aligned} P(M_1 \\cap M_2)&amp;=P(M_1)P(M_2|M_1)\\\\ &amp;= \\frac{\\text{number of men}}{\\text{total number of people}}\\times\\frac{\\text{number of men left}}{\\text{total number of people left}}\\\\ &amp;=\\frac{4}{10}\\times\\frac{3}{9}\\\\ &amp;=\\frac{2}{15}. \\end{aligned} \\] ## Module 6 12.5.2 General exercises Exercise: Following on from the example above, what is the probability that someone who is vaccinated is from region C? Solution: \\[P(A|Vaccinated) = \\frac{P(C)P(Vaccinated|C)}{P(Vaccinated)} = \\frac{0.3 \\times 0.74}{0.728} = 0.305\\] Exercise: Create a histogram of posterior values for hospital D. Solution: #histogram of posterior values for hospital D ggplot(data = predicted_values_df , aes(x = D)) + geom_histogram(bins = 20, color = &quot;#00b4d8&quot;, alpha = 1, fill = &quot;#1d3557&quot;) + theme_bw() + labs(title = &quot;Posterior Predicted values for D&quot;, x = &quot;Posteriors&quot;, y = &quot;Frequency&quot;) Exercise: Create a density plot of the posterior values for hospital G. Solution: #density plot of posterior values for hospital G ggplot(data = predicted_values_df , aes(x = G)) + geom_density(alpha = 1, fill = &quot;#1d3557&quot;) + theme_bw() + labs(title = &quot;Posterior predicted Values for G&quot;, x = &quot;Posteriors&quot;, y = &quot;Density&quot;) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
