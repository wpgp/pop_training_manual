
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#data path for macOS
data_path <- "data/"
#output path for macOS
output_path <- "output_data/"


raster_path1 <- paste0(data_path, "Continuous Rasters/")
raster_path2 <- paste0(data_path, "Categorical Rasters/")
raster_path3 <- paste0(data_path, "Settlement Rasters/")
shapefile_path <- paste0(data_path, "Shapefiles/")
mgrid_path <- paste0(data_path, "Mastergrid/")
```

# Introduction to Statistical Modelling with Implementation in R

This module contains an introduction to the concepts of statistical modelling, covering the key types of models, starting with simple linear regression and progressing to more complex models with implementation in R. Module 4 also introduces various methodologies for model selection, model prediction and cross-validation.

## Concept of statistical modelling

### Overview of statistical modelling

Statistical modelling, also known as **regression analysis**, can be described as a statistical technique used for exploring the relationship between a **dependent variable** (also called the outcome or response variable) and one or more **independent variables** (also called the explanatory variable or covariates). Data is viewed as being generated by some random process from which conclusions can be drawn. Statistical models help to understand these processes in greater depth, something that can be of interest for multiple reasons.
  
  - Future predictions can be made from understanding the random process
  - Decisions can be made based on the inference from the random process
  - The random process itself may be of scientific interest

For example, statistical modelling can be used to find out about the relationship between rainfall and crop yields, or the relationship between unemployment and poverty. 


Suppose for $i=1,\cdots,n$ observations we have the observed responses 
$$
\boldsymbol{y}=\begin{pmatrix}
y_1 \\ y_2 \\ \vdots \\ y_n
\end{pmatrix},
$$
where each $y_i$ has an associated vector of values for $p$ independent variables

$$
x_i=\begin{pmatrix}
x_{i1} \\ x_{i2} \\ \vdots \\ x_{in}
\end{pmatrix}.
$$


The observed responses $\boldsymbol{y}$ are assumed to be realisations of the random variables denoted by 
$$
\boldsymbol{Y}=\begin{pmatrix}
Y_1 \\ Y_2 \\ \vdots \\ Y_n
\end{pmatrix}.
$$
Alternatively, the random variable $Y_i$ is the predicted (or fitted) value of the observed response $y_i$.

The use of either upper or lower case is important in statistical modelling as the case used indicates to whether the variable is fixed (e.g. the observed responses $\boldsymbol{y}$ ) or random (e.g. the predicted responses $\boldsymbol{Y}$ ). The variability in the independent variable(s) is not modelled, therefore, it is treated as a fixed (not random) variable, hence given in lower case, $x_i$.



### Correlation

The relationship between variables can be determined by the extent to which they are related, also known as **correlation**. Correlation can either be positive, negative, or it does not exist (no correlation). What each type of correlation means is as follows:

- Positive: as the value of one variable increases, the value of another increases
- Negative: as the value of one variable decreases, the value of another decreases
- No correlation: the value of one variable does not affect the value of another variable

A visual example of correlation can be seen in the figure below.

<!-- ![Visual examples of the different types of correlation](/Volumes/worldpop/Projects/WP000008_UNFPA_THA_Phase2/Working/Pop_Modelling_Training_Manual/Module4/images/correlation.png) -->

<!-- ![Visual examples of the different types of correlation](/Projects/WP000008_UNFPA_THA_Phase2/Working/Pop_Modelling_Training_Manual/Module4/images/correlation.png) -->

```{r image correlation, echo=FALSE, fig.align='center', out.width="80%", fig.cap = "Visual examples of the different types of correlation"}
knitr::include_graphics("figures/4_images/correlation.png")
```  

<div 
  class="boxed" style="background-color: lightyellow; text-align: left; padding: 10px;"> 
  **Exercise:** Identify the dependent and independent variables in the following sentence:

A researcher investigates the effects of school density on the grades of its pupils.
  
</div>

### Scaled covariates
When using more than one covariate in the modelling, it is important that they are given in the same units for comparison purposes. In the cases where they are not in the same units, covariates can be scaled to ensure the results are comparable.

The two main methods for scaling are as follows:

- Centring: subtract each value from the mean value
- Z-score: subtract each value from the mean value and divide by the standard deviation

### Uncertainty

Uncertainty and error is unavoidable when estimating and predicting. The less error in your predictions, the more reliable the results are, therefore, it is important to measure the error margin when modelling. This can be done by measuring how close (or far) the predicted value is from the mean value. Alternatively, confidence intervals (in frequentest/classical statistics) or credible intervals (in Bayesian statistics) can be used. In this chapter, the focus is on frequentest statistics so confidence intervals will be used, Bayesian statistics will be introduced in Module 6.

## Simple regression

There are some important assumptions required for simple regression modelling, given as follows.

  1. Normality of the response/residuals (e.g. histograms, Q-Q plot)
  2. Linear relationship between response and predictors (e.g. scatter plot, residuals vs fitted plot)
  3. Homoscedasticity - constant variance of the residuals (e.g. spread-location plot)
  4. Independence between predictors - no multicollinearity (e.g. use `Corr(X1, X2)` function, should be approximately equal to 1)


To check that the response follows the normality assumption, plots can be used. If the normality assumption holds, the shape of the histogram will resemble the shape of a bell curve, as seen in the example below using the `birth` data. This dataset has the dependent variable `Weight` for the birth weight (in grams) of 24 newborn babies with 2 independent variables, `Sex` for the sex and `Age` for the gestational age (in weeks) of the babies, where `Sex` is a categorical variable with values


$$\text{Sex} = 
\begin{cases}
1 & \text{if male, or} \\
2 & \text{if female}.
\end{cases}$$

```{r birth weight data}
#create birth weight dataset
birth <- data.frame(
  Sex= c(1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2),
  Age=c(40,38,40,35,36,37,41,40,37,38,40,38,40,36,40,38,42,39,40,37,36,38,39,40), 
  Weight=c(2968,2795,3163,2925,2625,2847,3292,3473,2628,3176,3421,2975,3317,
           2729,2935,2754,3210,2817,3126,2539,2412,2991,2875,3231))
                      
```

```{r normality check histogram}
#plot a histogram of the birth weights
hist(birth$Weight, breaks=10, 
     main="Histogram of Birth Weight",
     xlab = "Birth Weight")
```

Alternatively, a Q-Q (quantile-quantile) plot can be used to assess the validity of the normality assumption, which plots the theoretical quantiles against the sample quantiles. If the normality assumption holds, the points on the plot will approximately follow a straight line. To plot this in `R`, the function `qqnorm()` can be used with an argument for the response that is being assessed, with the function `qqline()` being used with the response as an argument to add a reference line, making it easier to see whether the relationship is in fact linear. This is demonstrated below with the `birth` dataset, where it can be seen that the normality assumption holds given that the points on the Q-Q plot approximately follow the straight line given. However, in the case where the normality assumption does not hold, transformations such as the log-transformation can be used. If transformations are not appropriate, such as when the data is count or binary, alternative methods of modelling are required, leading to generalised linear modelling discussed later in this module.

```{r normality assumption qq plot}
#qq plot of the birth weight data
qqnorm(birth$Weight)

#add a reference line to the plot
qqline(birth$Weight, col="blue")
```


Using the plotting techniques discussed in Module 2, exploratory analysis can be conducted on the data, checking the assumption of a linear relationship between the response and predictors. Through using the `plot()` function with the response and the predictor you wish to test as arguments, if the resulting scatter plot has a linear trend, then this assumption is met.

```{r scatter plot linear assumption}
#scatter plot for linear assumption
plot(birth$Age, birth$Weight,
     xlab = "Gestational age (in weeks)",
     ylab = "Birth weight (in grams)",
     main = "Gestational age vs birth weight")
```

The plot of gestational age vs birth weight shows that there is a positive correlation between the two variables, indicating that as gestational age increases, the birth weight also increases, where the points roughly follow a straight line indicating that the assumption of linearity between the response and predictor is met. 

Another way to test this assumption is to use the `pairs()` function in `R`, where when the dataset of interest is included as an argument, it provides a matrix of scatter plots that show the relationship of each combination of variables available in the data. 

```{r pairs}
#use the pairs function for the birth dataset
pairs(birth)
```

The scatter plots for `Age` and `Weight` indicate that the relationship between the variables is approximately linear and therefore the assumption is met. However, if this assumption was not upheld, then appropriate transformations could be used, for example, a log-transformation. This is discussed more in the non-linear regression modelling section. 

Alternative ways of checking assumptions require for the model to be fitted first, these methods will be discussed in the following sections. 

### Linear regression
Simple linear regression is a regression model which has a linear relationship due to the dependent variable depends only on one independent variable, alternatively, the independent variable is conditioned only on one dependent variable. A key concept of the simple linear regression model is that it is assumed each response follows a normal distribution, $Y_i \sim N(\mu_i, \sigma^2)$. The simple linear regression model can be written as
$$ Y_i = \beta_0 + \beta_1 x_i + \epsilon_i,$$

where $\beta_0$ is the $y-$intercept, $\beta_1$ is the slope and the errors are independent and identically distributed as $\epsilon\sim N(0, \sigma^2)$ for $i=1,...,n$. The error can be described as the random difference between the value of $Y_i$ and $\beta_0 +\beta_1 x_i$ which is the value of its conditional mean.


An example of a simple linear regression can be seen below with the `birth` dataset. The birth weight of each baby can be modelled using either the gestational age or sex of the individual for a simple linear regression model. In this example we will focus on predicting the birth weight of a baby using the gestational age.

$$\text{Weight}_i = \beta_0 + \beta_1\text{Age}_i + \epsilon_i$$
where $\epsilon_i \sim Normal(0, \sigma)$.




To do linear modelling in `R`, the function `lm()` can be used with an argument for the model formula. When fitting a simple linear regression model, the function `glm()`, used for fitting generalised linear models, yields identical results to the function `lm()`. Generalised linear models will be discussed later in the module. 

```{r model birth weight by age}
#fit a linear model for birth weight by gestational age
birth_simple_lm <- lm(Weight ~ Age, data = birth)
birth_simple_lm

#fit the linear model using the function glm
birth_simple_glm <- glm(Weight ~ Age, data = birth)
birth_simple_glm #results are identical to the results from the lm function
```


```{r image lm summary, echo=FALSE, fig.align='center', out.width="80%", fig.cap = "Reading the coefficients from the model output"}
knitr::include_graphics("figures/4_images/coefficient output.png")
```  


The output of the model gives the resulting coefficients. `(Intercept)` corresponds to the $y-$intercept $\beta_0$, in this case $\beta_0=-1485.0$ meaning that if $\text{Age}=0$ then the birth weight would be -1485.0g. The coefficient `Age` corresponds to the slope $\beta_1$, in this case $\beta_1=115.5$, meaning that for each (1) unit increase in `Age`, the birth weight will increase by 115.5g. 



Including the linear model as an argument in the function `summary()` in R provides summary statistics for the linear model. These statistics include the previously given coefficients, as well as the corresponding standard errors and p-values (probability values) for the coefficients (among other information). This function is beneficial for exploring whether the independent variables are statistically significant and improve the model, as a small p-value (typically chosen to be less than 0.05) indicates that there is a statistically significant relationship between variables.

```{r summary lm}
#print summary for linear model
summary(birth_simple_lm)
```

```{r image coef summary, echo=FALSE, fig.align='center', out.width="80%", fig.cap = "Interpreting the summary output"}
knitr::include_graphics("figures/4_images/coefficient summary.png")
```  

In this case, it can be seen that the p-value for the `Age` covariate is $<0.05$ and is therefore statistically significant to the 5% level and improves the model (and therefore should remain in the model).


A line of best fit using the linear model can be added to the above plot through using the function `abline()`, adding the entire linear model as an argument of the function.

```{r plot birth weight gestational age with lm line}
#plot birth weight against gestational age
plot(birth$Weight ~ birth$Age,
     xlab = "Gestational age (in weeks)",
     ylab = "Birth weight (in grams)",
     main = "Gestational age vs birth weight") 
#add line for linear model
abline(birth_simple_lm)
```


To create diagnostic plots for the linear model to check the required assumptions, the function `plot()` can be used with the model as the argument. This creates four plots, each having a different purpose. To produce only one of the four plots, add an argument for the plot index you wish to show (the fourth plot is combined of Cook's distance and Residuals vs Leverage plots, to index the fourth plot, include `5` as an argument).

```{r plot model diagnostic}
#model diagnostic plot
par(mfrow = c(2, 2))
plot(birth_simple_lm)
```

The first plot is the **Residuals vs Fitted** plot, used to check the linearity assumption. 

```{r residuals plot}
#residuals plot
plot(birth_simple_lm, 1)
```

In this plot, there should be no pattern and the red line should be approximately horizontal at zero for the normality assumption to hold. The residuals plot for the birth data is quite horizontal and is based around zero, however, there is a slight indication of a pattern meaning that there could be some problem with the linear model. This problem could be many things, possibly indicating that the relationship is not linear and instead quadratic for example. 

The second plot is the **Normal Q-Q** plot, similar to that produced by the `qqnorm()` function discussed previously, but now is used to check the normality assumption of the residuals. 

```{r normal q-q}
#normal Q-Q plot
plot(birth_simple_lm, 2)
```

The plot for the `birth` data does show some problems with the normality assumption given that the points to not all approximately fall on the reference line. This means that the normality assumption cannot be assumed.

The third plot is the **Scale-Location** plot, or the **Spread-Location** plot, used to verify the homoscedasticity (homogeneity of variance). For the assumption to be upheld, the line should be approximately horizontal with the points equally dispersed. 

```{r scale-location}
#scale-location plot
plot(birth_simple_lm, 3)
```

In this case, the points are approximately equally spread out with the reference line being mostly horizontal, indicating that the homoscedasticity assumption may be upheld. 

Finally, the fourth plot is the **Residuals vs Leverage** plot, used for identifying outlier points that have high leverage. These are points that may impact the results of the regression analysis if they are included or excluded, although not all outliers are influential to alter the results. This plot identifies the 3 most extreme values.

```{r residuals vs leverage}
#residuals vs leverage plot
plot(birth_simple_lm, 5)
```

This plot identifies the 3 most extreme points (#4, #8 and #21), where point number 4 is identified as influential through using the measure of Cook's distance. There is evidence that this point will alter the results of the regression analysis so there should be some consideration whether to include this point or not. 



<!-- The population dataset `sim_population.csv` introduced in Module 2 can be used for demonstrating these methods too. For example, the relationship between the number of buildings in an area and the corresponding population can be explored through fitting a simple linear regression model to the data and plotting the results with a line of best fit. -->


<!-- ```{r sim pop data, echo=FALSE} -->
<!-- #import simulated population data -->
<!-- Pop_data <- read.csv(file = paste0(data_path, "sim_population.csv"), header = T) -->

<!-- #fit a simple linear regression model -->
<!-- pop_simple_lm <- lm(pop ~ buildings, data = Pop_data) -->
<!-- summary(pop_simple_lm) -->

<!-- #plot the relationship -->
<!-- plot(Pop_data$pop~Pop_data$buildings, -->
<!--      xlab = "Number of buildings", -->
<!--      ylab = "Population", -->
<!--      main = "Building count vs population")  -->
<!-- abline(pop_simple_lm) -->
<!-- ``` -->




<!-- <div -->
<!-- class="boxed" style="background-color: lightyellow; text-align: left; padding: 10px;">  -->
<!--   **Exercise:** Import the dataset `sim_population.csv` into your R script. Fit a linear model exploring the relationship between mean temperature and elevation. Produce a scatter plot of the relationship and add a line of best fit.  -->
<!-- </div> -->




### Polynomial regression

The data is not always best described by a linear relationship between the dependent and independent variables, for example, there could be a quadratic relationship between the variables. 

The following are examples of polynomial regression models:
  
  - A quadratic function: $$ Y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \epsilon_i.$$
  - A polynomial of degree 4: $$Y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3 + \beta_4 x_i^4+ \epsilon_i.$$

For example, to fit a simple polynomial regression model with a quadratic function to the birth weight dataset used above, the quadratic term needs to be created and added to the data, then the function `lm()` can be used inputting the model formula. Alternatively, the quadratic term can be included in the formula within the function `I()` which lets R know to include that term as a separate term within the model. Another option is to use the function `poly()` with arguments for the independent variable and the degree of polynomial wanted, making the code more efficient when higher order polynomials are used in particular, instead of typing out a long equation with many terms. 

```{r quadratic lm}
#create the quadratic term and add to data before modelling
birth$Age2 <- birth$Age^2
birth_quad_lm1 <- lm(Weight ~ Age + Age2, data = birth)
summary(birth_quad_lm1)

#alternatively, include quadratic term within I()
birth_quad_lm2 <- lm(Weight ~ Age + I(Age^2), data = birth)
summary(birth_quad_lm2) #produces the same model

#alternatively, include quadratic term within poly()
birth_quad_lm3 <- lm(Weight ~ poly(x = Age, degree = 2, raw = TRUE),
                     data = birth)
summary(birth_quad_lm3) #produces the same model
```

The output from the `summary()` function works the same for non-linear models as it does for linear, with the coefficient estimates corresponding to the values of $\alpha$ for the intercept and the $\beta$ value(s) for the covariate(s). In this case, it can be seen that adding the quadratic term for age does not improve upon the linear model given that the p-value (`Pr(>|t|)`) is not statistically significant. This conclusion is reasonable given that the line of best fit for the linear model fits the birth weight data well and the data does not show a quadratic trend. 


The welding dataset below contains information from the Welding Institute in Abingdon, providing $n=21$ measurements of currents in amps with the corresponding minimum diameter of the weld. Given that the diameter of the weld depends on the amount of current, `Current` is the independent variable and `Diameter` is the dependent variable. 


```{r welding data}
#create welding dataset
welding <- data.frame(Current = c(7.82, 8.00, 7.95, 8.07, 8.08, 8.01, 8.33, 
                                  8.34, 8.32, 8.64, 8.61, 8.57, 9.01, 8.97, 
                                  9.05, 9.23, 9.24, 9.24, 9.61, 9.60, 9.61), 
                      Diameter = c(3.4, 3.5, 3.3, 3.9, 3.9, 4.1, 4.6, 4.3, 4.5, 
                                   4.9, 4.9, 5.1, 5.5, 5.5, 5.6, 5.9, 5.8, 6.1,
                                   6.3, 6.4, 6.2))
```

The welding data can be modelled in the same way as the birth weight dataset, using the function `lm()`, as seen in the example below.

```{r model welding}
#simple linear model
weld_simple_lm <- lm(Diameter ~ Current, data = welding)
summary(weld_simple_lm)

#quadratic model
weld_quad_lm <- lm(Diameter ~ Current + I(Current^2), data = welding)
summary(weld_quad_lm)

```


Unlike with the birth weight quadratic model, the addition of a quadratic term to the model for the welding data improves the fit of the model, given that both the linear and quadratic terms are statistically significant at the 5% significance level. This improved fit can be demonstrated graphically using the function `predict()` with arguments for the model you wish to predict from and a new dataset with a range of values you wish to predict the values of the dependent variable from (typically a sequence of evenly spaced numbers from the minimum to maximum values of your independent variable).

```{r welding quad predict and plot}
#made a new dataset
weld.new <- data.frame(Current = seq(from = min(welding$Current), 
                                     to = max(welding$Current),
                                     length.out = 100))

#use the predict function
pred_simple_lm <- predict(weld_simple_lm, newdata = weld.new)
pred_quad_lm <- predict(weld_quad_lm, newdata = weld.new)


#basic plot for relationship between variables 
plot(Diameter ~ Current, data = welding)
#add lines for each of the sets of predicted values
lines(pred_simple_lm ~ weld.new$Current, col = "blue", lty = 2, lwd = 2)
lines(pred_quad_lm ~ weld.new$Current, col = "red", lty = 1, lwd = 2)
#add a legend for clarity
legend("topleft", c("Linear", "Quadratic"),
       col = c("blue", "red"), lty = c(2,1), lwd = 2)

```

The plot showing lines of best fit for both the simple linear model and the quadratic model demonstrate the improved fit of the quadratic model, with the added flexibility of the curve matching the trend of the data better. 

This process can be extended for including higher degrees of polynomials in the regression models, although it is important to be wary of **overfitting** the model to the data as this risks the model only having use for inference to the original dataset. 

### Non-linear regression
The relationships being explored are not always best described by a linear relationship. In these cases, the data can be transformed, for example using logarithms, square roots and exponentials, to fit a non-linear model which is more flexible, potentially explaining the relationship between variables better. For a regression model to be non-linear, $Y$ must be a non-linear function of the parameters (e.g. $\beta_0$ and $\beta_1$), however, $Y$ can still be a linear function of the covariates $x$.

The following are examples of non-linear regression models:

  - Squared value of the $\beta$ coefficient: $$Y_i = \beta_0 + \beta_1^2x_i + \epsilon_i.$$
  - Logarithmic: $$\log(Y_i) = \beta_0 + \beta_1 x_i + \epsilon_i$$
  which implies $$Y_i = \exp(\beta_0 + \beta_1 x_i + \epsilon_i)=\exp(\beta_0)\exp(\beta_1 x_i)\exp(\epsilon)$$
  a relationship which is multiplicative, meaning that a unit increase in $x_i$ corresponds to $Y_i$ being multiplied by a value of $\exp(\beta x_i)$, instead of an additive effect of $\beta x_i$ like with a linear model.
  - Square root: $$Y_i^{1/2}=\beta_0 + \beta_1 x_i + \epsilon_i.$$
  - Negative reciprocal: $$-\frac{1}{Y_i}= \beta_0 + \beta_1 x_i + \epsilon_i.$$

The function `nls()` can be used for non-linear regression models and estimate the parameters via a non-linear least squares approach (a non-linear approach to finding the line of best fit for the given data). To demonstrate this approach, the Michaelis-Menten equation for kinetics given below can be used, given that there is a non-linear relationship between the dependent variable and the parameters. 

$$
Y_i = \frac{\beta_0 x_i}{\beta_1+x_i}
$$

```{r non-linear regression}
#simulate some data
set.seed(100)
x<-1:100
y<-((runif(1,20,30)*x)/(runif(1,0,20)+x)) + rnorm(100,0,1)

#model the data using the function nls(), if no start values are given, a 
#warning may occur, but R will just choose the start values itself instead
nonlinear_mod <- nls(y ~ a*x/(b+x)) 
#summary of the non-linear model
summary(nonlinear_mod)
```
The summary function works in the same way as for the linear models, providing the estimated values of the model parameters, in this case, $\beta_0=6.4946$ and $\beta_1 =1.0765$.

To visualise this equation with the non-linear regression model fitted, the function `plot()` can be used as with the linear models, with the addition of the function `lines()` as used with the polynomial regression models with an argument for the x-axis values and the predicted values.

```{r plot non linear}
#plot the data
plot(x,y)
#add a line of best fit
lines(x, predict(nonlinear_mod), col = "red", lty = 1, lwd = 2)
#add a linear regression line for comparison purposes
abline(lm(y ~ x), col = "blue", lty = 2, lwd = 2)
#add a legend for clarity
legend("bottomright", c("Non-linear", "Linear"),
       col = c("red", "blue"), lty = c(1,2), lwd = 2)
```

It can be seen in the plot that the non-linear line fits the data very well, much better than the simple linear regression model added to the plot for comparison purposes.

## Multiple regression
Multiple regression can be described as an extension of simple regression where you still only have one dependent variable but there are multiple independent variables. For $p$ independent variables, the model can be written as

$$ Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \beta_p x_{ip} + \epsilon_i,$$
where $\epsilon_i \sim N(0, \sigma^2)$ for $i=1,...,n$.


An important assumption of multiple regression modelling is **multicollinearity**, meaning that it is assumed that the independent variables are not highly correlated with one another. If this assumption is not met, it can make identifying which variables better explain the dependent variable better much more challenging.

The `birth` dataset can be used to demonstrate multiple linear regression given that there are 2 independent variables included in the data, `Sex` and `Age`, modelled as follows.
$$ Weight_i =\beta_0 + \beta_1 Sex_i + \beta_2 Age_i + \epsilon_i$$ 


To perform multiple linear regression in R, the function `lm()` can be used in the same way as for simple linear regression, however, with the additional variables given in the formula as in the code below. 

```{r multiple linear regression}
#multiple linear regression
birth_multi_lm1 <- lm(Weight ~ Sex + Age, data = birth)
summary(birth_multi_lm1)
```
The output from the summary function shows that both independent variables are statistically significant at the 5% significance level, and hence birth weight depends on both sex and gestational age of the baby. Interpreting the results is done in the same way as for simple models, with the estimates corresponding to the coefficients as follows:

  - $\alpha$=-1447.24
  - $\beta_1$=-163.04 
  - $\beta_2$=120.89
  
Through using the function `update()`, you can add or remove variables from a model without needing to re-fit the model yourself. This is particularly useful when you have a model with many parameters, where instead of needing to type out the model again with each of the parameters, you can simply update the existing model to either add another parameter or remove a parameter if it is not needed. 

To remove a variable from a model, you use the function in the form `update(model, ~. - term)`. For example, to update the model given above to remove the covariate `Age`, you would use the below code.

```{r update models remove}
#remove the Age covariate from the multiple linear regression model
birth_multi_lm2 <- update(birth_multi_lm1, ~. - Age)
birth_multi_lm2
```

Alternatively, if you wish to add a variable, you use the formula in the form `update(model, ~. + term)`. For example, to add the term for Age back into the model, you would use the below code, which results in the same model as originally fitted. 

```{r update models add}
#remove the Age covariate from the multiple linear regression model
update(birth_multi_lm2, ~. + Age)
```


The `update()` function also allows for the data being modelled to be updated through adding an argument for `data = `. This is demonstrated in the code below. 

```{r update data}
#create an example dataset
y <- c(1:20)
x1 <- y^2
z1 <- y*3
update_example1 <- data.frame(x = x1, y = y, z = z1)

#fit linear model
example_mod1 <- lm(y ~ x + z, data = update_example1)
summary(example_mod1)

#create new dataset
x2 <- y^3
z2 <- y*4
update_example2 <- data.frame(x = x2, y = y, z = z2)

#update the dataset in the model
example_mod2 <- update(example_mod1, data = update_example2)
summary(example_mod2)
```


## Generalised linear regression

For simple regression modelling, there is the assumption of normality for the dependent variable, however, this assumption is not always met, for example, with count data (e.g. number of people with a disease) which is often modelled with a Poisson distribution, or binary data (e.g. beetles killed or not killed) which is often modelled with a Bernoulli distribution. In these cases of non-normal data, alternative models are required, which is where generalised linear modelling is beneficial with its relaxed distributional assumption. 

The generalised linear model is written in the form
$$g(\mu_i) = \eta_i = \boldsymbol{x}_i^T \boldsymbol{\beta},$$
where $\mu_i=E(Y_i)$ is the expected value of $Y_i$, $\eta_i$ is the linear predictor and $g(\mu_i)$ is the **link function** between the distribution of $\boldsymbol{Y}$ and the linear predictor. 

An important assumption for generalised linear regression is that the dependent variable $\boldsymbol{Y}$ is assumed to be independent and a member of the exponential family (e.g. normal, Poisson, Bernoulli, geometric, exponential, ...).

The link function depends on the distribution of the data type and the dependent variable, where the table below provides the three main link functions and the corresponding data types and distributions. 

<div style="width: 100%; text-align: center; display: flex; justify-content: center;">

+-------------+--------------------------------+---------------------------------------+---------+
+-------------+--------------------------------+---------------------------------------+---------+
| Data type   | Response family                | Link                                  | Name    |
+=============+================================+=======================================+=========+
| Continuous  |Normal/Gaussian/log-normal/gamma| $g(\mu)=\mu$                          |Identity |
+-------------+--------------------------------+---------------------------------------+---------+
| Count       |Poisson                         |$g(\mu)=\log(\mu)$                     |Log      |
+-------------+--------------------------------+---------------------------------------+---------+
| Binary      |Bernoulli/binomial              |$g(\mu)=\log\left(\frac{p}{1-p}\right)$|Logit    |
+-------------+--------------------------------+---------------------------------------+---------+
+-------------+--------------------------------+---------------------------------------+---------+
</div>

To fit generalised linear models in R, the function `glm()` can be used, in a very similar way to the function `lm()` seen in earlier sections, but with the addition of a (exponential) `family` argument. The default family is normal, hence why if no family is specified, the functions `glm()` and `lm()` produce identical models. However, if the data is not normal, the exponential family that the data is a member of must be specified. 


When dealing with count data, the Poisson log-linear model is most commonly used, taking the following form.
$$ Y_i \sim Poisson(\mu_i), \text{ } \log(\mu_i)=\boldsymbol{x}_i^T \boldsymbol{\beta}$$
It is important to note that the Poisson distribution assumes that the mean and variance are equal. If over-dispersion (the variance is greater than the mean) is present, a negative-binomial model may be preferable.


To demonstrate generalised linear modelling with count data, the `ccancer` dataset from the package `GLMsData` can be utilised. This dataset gives the count of deaths (`Count`) due to cancer within three different regions of Canada (`Region`), providing additional covariates for the gender of each individual (`Gender`) and the site of the cancer (`Site`). More information on this dataset can be found through using the help function.


```{r install GLMsData, eval=FALSE, warning=FALSE}
#install GLMsData package
install.packages("GLMsData")
```

```{r load GLMsData, message=FALSE, warning=FALSE}
#load the GLMsData package
library(GLMsData)
```


```{r ccancer data}
#import ccancer dataset
data(ccancer)
head(ccancer)
```

To model this data, the function `glm()` can be used again, but with specifying the family argument as `family = poisson` as follows, where the following model is an intercept only model, including a `1` instead of any independent variables.

```{r ccancer model intercept}
#fit the glm for the ccancer data to explore the effect of gender on the 
#count of cancer deaths
ccancer_glm1 <- glm(Count ~ 1, family = "poisson", data = ccancer)
summary(ccancer_glm1)
```

Given that there are covariates included in this dataset, it is important to explore the relationship they may have with the response. The following model contains a main effect for gender, exploring the relationship between the gender of individuals and cancer deaths. 

```{r ccancer model gender}
#fit the glm for the ccancer data to explore the effect of gender on the 
#count of cancer deaths
ccancer_glm2 <- glm(Count ~ Gender, family = "poisson", data = ccancer)
summary(ccancer_glm2)
```

It can be seen in this model that the inclusion of the covariate `Gender` is statistically significant, therefore there is a relationship between the gender of an individual and the count of cancer deaths in Canada. 


<div
  class="boxed" style="background-color: lightyellow; text-align: left; padding: 10px;">
  **Exercise:** Fit a Poisson GLM using the `ccancer` dataset exploring the relationship between the count of cancer deaths and the covariates for the cancer site and region in Canada. Is the relationship between the dependent and independent variables significant?
</div>


Another example of generalised linear modelling with count data can be seen as follows using the `hodgkins` dataset which contains information on 583 patients with Hodgkin's disease. Within this information is the number of patients (`count`) with each combination of the histological type of disease (`type`) and the response to the treatment (`rtreat`).

```{r hodgkins dataset}
hodgkins <- data.frame(count = c(74, 18, 12, 68, 16, 12, 154, 54, 58, 
                                 18, 10, 44),
                       type = c("Lp", "Lp", "Lp", "Ns", "Ns", "Ns", "Mc",
                                "Mc", "Mc", "Ld", "Ld", "Ld"),
                       rtreat = c("positive", "partial", "none", "positive",
                                  "partial", "none", "positive", "partial",
                                  "none", "positive", "partial", "none"))
```

The information on the patients has been cross-classified, where the covariates `type` and `rtreat` are categorical variables with multiple levels each. 

```{r hodgkins glm}
#fit a glm to the hodgkins dataset including both covariates in the model
hodgkins_glm1 <- glm(count ~ type + rtreat, family = poisson, data = hodgkins)
summary(hodgkins_glm1)
```
Alternatively, a generalised linear model can be fitted with both the main effects and an interaction. This model is known as the full or saturated model, where the interaction term can be included in addition to the main effects using a colon `:` between the variables you wish to include an interaction term for. This method is beneficial for when you just want to include an interaction, not necessarily the corresponding main effects, however, if you want to include both the interaction and corresponding main effects to a model, an asterisk `*` can be used between the chosen covariates. Both these methods are demonstrated below and return the same model.

```{r hodgkins glm sat}
#fit the saturated Poisson GLM to the hodgkins dataset with a colon
hodgkins_glm2 <- glm(count ~ type + rtreat + type:rtreat, family = poisson,
                     data = hodgkins)
summary(hodgkins_glm2)
#fit the saturated Poisson GLM to the hodgkins dataset with an asterisk
hodgkins_glm3 <- glm(count ~ type*rtreat, family = poisson, data = hodgkins)
summary(hodgkins_glm3)
```

It can be seen from looking at the results from both the main effects model and the saturated model, particularly the *p*-values, that the terms in the saturated model are more statistically significant, indicating that the saturated model is a better fit for the data. 
Additionally, through testing whether the interaction term is needed, you are able to test whether the covariates are independent from one another or whether they are correlated/associated. In this case, the inclusion of the interaction term improves the model and all interaction terms are statistically significant, therefore there is evidence that the covariates `type` and `rtreat` are independent from one another.

If the data is binary, meaning that there are only two possible outcomes, the family can be specified as `family = binomial` to fit a binomial logistic regression model with a logit link, the default link for a binomial family which assumes that the errors follow a logistic distribution. Alternatively, a probit link can be used, through changing the family argument to be `family = binomial(link="probit")`, which instead assumes that the errors follow a normal distribution, however, this is used less frequently. 

The form of a binomial logistic regression model is given as
$$Y_i|n_i, p_i \sim Binomial(n_i, p_i), \text{ logit}(p_i)=\log\left(\frac{p_i}{1-p_i}\right).$$

This can be seen in the code below using the `beetles` dataset, $Y_i$ is the number of beetles `killed`, $n_i$ is the number of beetles `exposed` and $\boldsymbol{x}_i$ is the `dose`. 

```{r beetle data}
beetles <- data.frame(dose = c(1.6907, 1.7242, 1.7552, 1.7842, 1.8113, 1.8369, 
                             1.861, 1.8839), 
                      exposed = c(59, 60, 62, 56, 63, 59, 62, 60),
                      killed = c(6, 13, 18, 28, 52, 53, 61, 60))
```

There are two ways to fit the binomial logistic regression model. Firstly, is to model the proportion of "successes" (in this example, it is the proportion of beetles killed) and weight by the number of trials (in this example, it is the number of beetles exposed). 

```{r binonmial logistic regression1}
#compute the proportion of beetles killed
beetles$prop_killed <- beetles$killed / beetles$exposed

#fit a binomial logistic regression model
beetles_glm_props <- glm(prop_killed ~ dose, data = beetles, family = binomial,
                  weights = exposed)
summary(beetles_glm_props)
```

Alternatively, the independent variable can be given as a matrix with two columns, one for the number of "successes" and the other for "failures" (in this example, a success is a beetle killed and a failure is a beetle not killed). 

```{r binonmial logistic regression2}
#fit a binomial logistic regression model with two columns to response
beetles_glm_matrix <- glm(cbind(killed, exposed - killed) ~ dose, 
                          data = beetles, family = binomial)
summary(beetles_glm_matrix)
```
As seen from the summaries for the binomial logistic regression models from each approach, the approaches yield identical results, so it is unimportant which approach is taken. 

<!-- <div  -->
<!--   class="boxed" style="background-color: lightyellow; text-align: left; padding: 10px;">  -->
<!--   **Exercise:** Fit a linear model of proportion of beetles killed using `dose` as a dependent variable, adding weights for the number of beetles exposed in each group and plot the results. Does this model fit the data well? -->
<!-- </div> -->


## Model predictions 

### Predictions with the formula and coefficients

Values of the dependent variable can be predicted through inputting the coefficient estimates found from the model summaries into the model formulae, given values of the independent variable(s).

To demonstrate predictions using just the model formula and the resulting coefficient estimates, firstly the simple linear regression model with the `birth` dataset will be used. As a reminder, the model was given as follows, fitted with the `lm()` function in `R`.

$$\text{Weight}_i = \beta_0 + \beta_1\text{Age}_i + \epsilon_i$$


The coefficients from the linear model given by the `summary()` function can then be used to predict the value of the birth weight for a baby at a given gestational age.

```{r summary birth lm}
#summary of the birth weight simple linear regression model
summary(birth_simple_lm)
```
Given the coefficient estimates from the model, the birth weight of a baby for a given gestational age can be predicted using the following formula.

$$ \text{Weight}_i = -1485.0 + 115.5 \times \text{Age}_i$$

Therefore, for example, a baby of gestational age 37.5 weeks, the birth weight is predicted as $$Weight_i=-1485.0 + 115.5 \times 37.5 = 2846.25g.$$




This method of prediction does not just work on simple linear regression models but can be used for regression modelling in general. For example, the multiple regression model for the `birth` dataset `birth_multi_lm1`, which has the following formula.

$$ Weight_i =\beta_0 + \beta_1 Sex_i + \beta_2 Age_i + \epsilon_i$$ 

If for example, you wanted to predict the birth weight of a baby girl at a gestational age of 38 weeks, you would use the following formula.

$$Weight = -1447.24 -163.04\times 1+120.89\times 38 = 2983.54g$$

<div
class="boxed" style="background-color: lightyellow; text-align: left; padding: 10px;"> 
  **Exercise:** What is the expected birth weight for a baby boy at a gestational age of 39.5 weeks?
</div>

This method of prediction also works with generalised linear models using the results from a model fitted with the `glm()` function. To demonstrate this, the Poisson GLM fitted to the `ccancer` dataset exploring the relationship between gender and counts of cancer deaths is used. As a reminder, the model summary output is as follows.

```{r ccancer glm summary}
#GLM for ccancer data gender vs count
summary(ccancer_glm2)
```

Using the resulting coefficient estimates from this model in the formula below to estimate the count of cancer deaths for a given gender. Since the log-link was used for this model, the coefficients require exponentiation in order to transform the log-count to just the count. 

$$
\begin{aligned}
\log(\hat{\mu}) &= 6.621406 + 0.157000 \times x_1 \\
\hat{\mu} &= \exp( 6.621406 + 0.157000 \times x_1)
\end{aligned}
$$
where $x_1=0$ if the individual is female and $x_1=1$ if the individual is male. 

Using this formula, the expected count of cancer deaths for women in from the dataset is $\hat{\mu}=exp( 6.621406 + 0.157000)=879$ and for men is $\hat{\mu}=exp( 6.621406)=751$.

### Predictions with functions

As mentioned in previous sections, you can also predict values using functions, particularly the `predict()` function. To use this, include the model you wish to predict from as an argument, in addition to values of data you wish to predict the values of the dependent variable from as `newdata`.

The use of this function is demonstrated in the code below, where for the `birth` dataset, birth weight is predicted for a range of values from the quadratic model, `birth_quad_lm2`, fitted with the `lm()` function.

Firstly, a new dataset needs to be created. This dataset needs to contain the value(s) for which the dependent variable will be predicted using. If for example you wish to estimate the birth weight of a baby with gestational ages 36.5, 37.5 and 38.5, you can create a new dataset containing just these values, as in the code given below. 

```{r predict birth newdata}
#made a new dataset
birth.new1 <- data.frame(Age = c(36.5, 37.5, 38.5))
birth.new1
```
Then, the predict function can be used, including the name of the model, `birth_quad_lm2`, and the new dataset to predict from, `birth.new1`.

```{r predict birth}
#use the predict function
birth_predict_quad1 <- predict(birth_quad_lm2, newdata = birth.new1)
```

The values of these predictions can be given as a table for ease of viewing using the `data.table()` function in the `data.table` package. To use this function, include the variables you wish to display as arguments and assign the variables names in the function for clarity.

```{r install data.table, eval=FALSE, warning=FALSE}
#install data.table package
install.packages("data.table")
```

```{r load data.table, message=FALSE, warning=FALSE}
#load the data.table package
library(data.table)
```

```{r datatable predictions}
#create a table for the predictions
data.table(birth.new1, Weight = birth_predict_quad1)
```


Alternatively, a range of values can be given as the new dataset for assessing the fit of a given model. To demonstrate this, the birth weight will be predicted for a sequence of gestational age values, starting from the minimum observed age to the maximum observed age with 50 values in total. 

```{r predict birth newdata 2}
#made a new dataset
birth.new2 <- data.frame(Age=seq(from = min(birth$Age), 
                                   to = max(birth$Age),
                                   length.out = 50))
head(birth.new2)
```

Then, the predict function can be used, including the name of the model, `birth_quad_lm2`, and the new dataset to predict from, `birth.new2`.

```{r predict birth 2}
#use the predict function
birth_predict_quad2 <- predict(birth_quad_lm2, newdata = birth.new2)
birth_predict_quad2
```

To demonstrate the fit of this model visually, create a plot depicting the relationship between the two variables being explored, with the predicted values added to the plot. If the model fits the data well, the predicted values line should match the trend of the data well. 

For comparative purposes, a line is added for the simple linear regression for this data to demonstrate the difference in fit of the two models and how the quadratic model fits the data better. 

```{r predict plot}
#basic plot for relationship between variables 
plot(Weight ~ Age, data = birth)
#add lines for each of the sets of predicted values
lines(birth_predict_quad2 ~ birth.new2$Age, col = "red", lty = 1, lwd = 2)
lines(predict(birth_simple_lm, newdata = birth.new2) ~ birth.new2$Age, 
      col = "blue", lty = 2, lwd = 2)
#add a legend for clarity
legend("topleft", c("Quadratic", "Linear"),
       col = c("red", "blue"), lty = c(1,2), lwd = 2)
```

As with the other method for predictions, this method also works with generalised linear models fitted with the `glm()` function. To demonstrate this, the binomial logistic regression model fitted to the `beetles` dataset is used. 

For example, predicting in the same way as before but for doses of 1.7 and 1.8, the following code would be used.
```{r predict beetles 1}
#create a new data frame to predict with doses of 1.7 and 1.8
beetles_newdata1 <- data.frame(dose = c(1.7, 1.8))

#predict function
predict(beetles_glm_props, newdata = beetles_newdata1)
```

However, these predicted values are not what is expected for the probability of death. This is due to the default `type` of the predict function being `type = link` which returns predicted values of $\text{logit}(p(x))$. To return the predicted values of the probabilities instead, `type = "response"` needs to be added as an argument, as follows.

```{r predict beetles 2}
#predict the probabilities
beetles_predict_glm <- predict(beetles_glm_props, newdata = beetles_newdata1,
                               type = "response")
beetles_predict_glm
```

These predictions are much more what you would expect for probabilities, given that they are between 0 and 1. For more information on probability, see Module 5.

The fit of this GLM can be assessed in the same way as for the models fitted using the `lm()` function, through predicting values for a wider range of values and fitting the predicted values to a plot. To demonstrate the goodness-of-fit of the GLM and why it is important to fit a GLM over a simple linear regression model, a linear model is fitted with weights added for the number of beetles exposed, with the line of best fit of this linear regression model also added to the plot.

```{r beetles glm fit plot}
#fit a simple linear regression model for the beetles data
beetles_simple_lm <- lm(prop_killed ~ dose , data = beetles, weights = exposed)

#create a new dataset
beetles_newdata2 <- data.frame(dose = seq(min(beetles$dose) - .2, 
                                          max(beetles$dose) + .2,
                                          length = 100))

#plot the relationship between the dosage and proportion of beetles killed
plot(prop_killed ~ dose, data = beetles, ylim = c(-0.5, 1.5), xlim = c(1.6, 2),
     ylab = "Proportion killed", xlab = "dose")
#add a line of best fit for the simple linear regression model
abline(beetles_simple_lm, col = "blue", lwd = 2, lty = 2)
#add a line for the predicted values from the GLM
lines(beetles_newdata2$dose,  predict(beetles_glm_props, 
                                      newdata = beetles_newdata2,
                                      type = "response"), lwd = 2, col = "red")
abline(h = c(0,1), lty = 2)
legend("topleft", c("Linear model", "Logistic GLM"), lty = c(2,1), lwd = 2,
       col = c("blue", "red"))
```

It is clear to see from this plot that the GLM fits much better than the linear model, given that for the linear model, at a dosage of 1.6, the proportion of beetles killed is actually negative, and that for a dosage of 1.9, the proportion of beetles killed is greater than 1, neither are logistically possible proportions. However, for the GLM, the proportion of beetles killed is always between the values of 0 and 1. For example, for the GLM, at a dosage of 1.6, instead of having a negative proportion, the proportion is just very close to 0 meaning that it is unlikely for any beetles to be killed, with the opposite occurring at a dosage of 1.9 where the proportion is very close to 1. Beyond the linear model not being realistically feasible for certain values of dosage, the line of best fit also does not fit the trend of the observed points as well as the GLM.

## Model selection

### Accuracy and precision
One of the main goals when modelling is to produce the best fitting models with the least amount of error. One type of error is observational error, made up of **accuracy** and **precision**, and can be used to measure results. Accuracy can be defined as the distance between the observed/estimated results and the true values, and precision can be defined as the spread of the observed/estimated results. In an ideal situation, you would want the observations to be close together and close to the true values. 

A visual representation of accuracy and precision can be seen in the figure below, where the graphs depict the 4 different combinations of the observational error types.

```{r image accuracy and precision, echo=FALSE, fig.align='center', out.width="80%", fig.cap = "Visual examples of accuracy and precision"}
knitr::include_graphics("figures/4_images/accuracy and precision.png")
```  

There are multiple ways of testing the accuracy and precision of a model, for example the Akaike information criterion which assesses the goodness-of-fit of a model, which could be described as assessing the comparative accuracy when paired with at least one other model. This method and others are be discussed in this module.


### Akaike information criterion
The **Akaike information criterion** (**AIC**) estimates the proportional quality of one model compared to another through assessing the quality predictions. It uses the bias-variance trade-off to identify which model is preferred for the given data, taking into consideration the accuracy of the model's predictions via the log-likelihood and the complexity of the model via a penalty term for the number of parameters within the model. Formally, the AIC is given as
$$\text{AIC}=-2\ell + 2p,$$
where $\ell$ is the log-likelihood of a model and $p$ is the number of parameters in the model. 

Given that the AIC statistic is essentially a measure of both bias and variance, when comparing two models fitted to the same data, the model with the smallest AIC value is the better fitting model for the data.

To find the AIC statistic in `R` for a given model, the value can found using the function `AIC()` with the desired model included as an argument, or if fitting a GLM, the function `summary()` provides the AIC statistic in the output.

```{r image AIC, echo=FALSE, fig.align='center', out.width="80%", fig.cap = "Reading the AIC statistic from a GLM sumamry"}
knitr::include_graphics("figures/4_images/AIC summary.png")
```  

To demonstrate the `AIC()` function, the AIC statistics can be found for both the linear and quadratic regression models for the birth weight data as follows. 
```{r AIC birthweight}
#AIC value of linear regression model
AIC(birth_simple_lm)

#AIC value of quadratic regression model
AIC(birth_multi_lm1)
```
The AIC statistic for the more complex model is smaller, supporting previously made conclusions that the multiple regression model is a better fitting regression model for the birth weight data compared to the simple linear regression model.

### Bayesian information criterion

The **Bayesian information criterion** (**BIC**), is an alternative information to the AIC,and also uses the bias-variance trade-off. Unlike the AIC however, it uses the number of observations in the dataset in the computation of the penalty term. As a result of this difference, the BIC has a larger penalty term than the AIC and penalises complex models more than the AIC. The formula for the BIC is given below. 

$$\text{BIC}=-2\ell + p \times \log(n),$$
where $\ell$ is the log-likelihood of a model, $p$ is the number of parameters in the model and $n$ is the number of observations in the dataset.

To compute the BIC statistic in `R` for a given model, the `BIC()` function can be used in much the same way as for computing the AIC statistic. Demonstrated below with the birth weight data.

```{r BIC birthweight}
#BIC value of linear regression model
BIC(birth_simple_lm)

#BIC value of quadratic regression model
BIC(birth_multi_lm1)
```

These results agree with previous conclusions, that the more complex model, the multuple regression model, fits the data best given that the BIC value for this model is smaller.

Given the differences in the AIC and BIC, for a smaller dataset, the AIC might be more appropriate since it doesn't penalise the more complex models as harshly. However, if the dataset is large, then the BIC may be more appropriate for preventing overfitting. 

### R-squared statistic
The $R^2$ or **R-squared** statistic, also called the **coefficient of determination**, is a measure of goodness-of-fit of a given regression model through measuring the proportion of variance from the dependent variable that is explained by the independent variable(s). 

There are two main components of the R-squared statistic, the sum of squares of the residuals and total sum of squares, both of which measure variation in the data, where squared values are used to account for fitted values being both above and below the true values. 

To compute these measures of variation, let $\boldsymbol{y}=y_1,...,y_n$ be a dataset with corresponding fitted values $\boldsymbol{\hat{y}}=\hat{y}_1,...,\hat{y}_n,$. The residuals can be described as the estimates of unobservable error, or the difference between the observed and fitted values, then given as $r_i= y_i - \hat{y}_i$.

The sum of squares of the residuals (the sum of the squared distance between the observed values and the fitted values) is computed through summing the squared values of the residuals as follows:
$$RSS = \sum_{i=1}^n (y_i-\hat{y}_i)^2 = \sum_{i=1}^n r_i^2.$$

The total sum of squares (the sum of the squared distance between the observed values and the overall mean) follows the same structure as follows:
$$TSS = \sum_{i=1}^n (y_i - \bar{y})^2,$$
where $\bar{y}= \sum_{i=1}^n y_i$ is the overall mean of the observed values.


The R-squared statistic is then computed using the following formula:
$$ R^2 = 1-\frac{RSS}{TSS},$$
and takes a value between $0$ and $1$. 

Given that the better fitting a model is, the smaller the difference between the observed and fitted values is and hence the smaller the RSS value is, a better fitting model will have a larger R-squared value, with the perfectly fitting model having an RSS value of 0 and an R-squared value of 1. Therefore, when comparing the goodness-of-fit of two or more models, the model with the R-squared statistic value closest to 1 is the better fitting model for the given data.

The values for the R-squared statistics from models in `R` can be found directly through using the `summary()` function using the code `summary()$r.squared`.

For example, this function can be used with the linear and quadratic regression models for the welding data to find out which model fits the data better. 
```{r rsquare welding}
#R-squared statistic for the linear regression model
summary(weld_simple_lm)$r.squared

#R-squared statistic for the quadratic regression model
summary(weld_quad_lm)$r.squared
```
Whilst both of the R-squared statistics are high, the value for the quadratic model is slightly higher, supporting previous conclusions that the quadratic model fits the data better and hence the quadratic model should be used.

### Analysis of variance
Analysis of variance (**ANOVA**) is a statistical test used to assess the relationship between the dependent variable and one or more independent variables.  

To produce an ANOVA table, the function `anova()` can be used in `R`. If only one model is given as the argument, it will indicate as to whether the terms in the given model are significant. This is demonstrated below with the simple linear model for the `welding` data. 

```{r anova 1}
#produce an ANOVA table for simple model for welding data
anova(weld_simple_lm)
```

```{r image ANOVA 1, echo=FALSE, fig.align='center', out.width="80%", fig.cap = "Understanding the ANOVA table"}
knitr::include_graphics("figures/4_images/anova1.png")
``` 

It can be seen from the results that the *p*-value is very small, and much smaller than the standard 5% significance level indicating that current does have an impact on diameter, and therefore the term for current should remain in the model.

However, you can also test which model fits the data best by including multiple models as arguments.  This is demonstrated in the `R` code below, with a comparison between the simple linear model and quadratic model for the `welding` dataset.

```{r anova}
#produce an ANOVA table for the simple and quadratic models for the welding data
anova(weld_simple_lm,weld_quad_lm)
```

```{r image ANOVA 2, echo=FALSE, fig.align='center', out.width="80%", fig.cap = "Understanding the ANOVA table"}
knitr::include_graphics("figures/4_images/anova2.png")
``` 

The *p*-value given in the ANOVA table is smaller than $0.05$, which is the typical significance value chosen, indicating that there is evidence that the addition of the quadratic term is significant. Therefore, there is evidence that the quadratic model better explains the relationship between the diameter and current comparatively to the simple model.


### Likelihood ratio testing
Likelihood ratio testing can be used to compare the fit of two models when the models are nested. In other words, if one model is a special case of the other where at least one parameter is removed from the model. The likelihood ratio test (LRT) helps to decide whether to reject the null hypothesis or not where the null hypothesis assumes that the nested model is at least as good as the more complex model.

The likelihood ratio test (LRT) statistic can be computed using the following formula 
$$ \lambda = - 2 \times (\ell(\text{model 1}) - \ell(\text{model 2})),$$
where model 1 is nested in model 2 and $\hat{\ell}()$ is the log-likelihood for the model given in the brackets.

To perform the likelihood ratio test, a constant, $c$, is chosen to determine the significance level of the test. If the corresponding *p*-value to $\lambda$ is less than $c$, then there is evidence to reject the null hypothesis and the complex model is preferred, otherwise, if the *p*-value is greater than or equal to $c$, there is evidence to not reject the null hypothesis and the nested model is preferred. 

The LRT can be conducted manually, through first computing the value of $\lambda$ through using the function `logLik()` in the equation with an argument for the chosen model to compute the log-likelihood of the chosen model.

```{r lambda}
#compute lambda
llmod1 <- logLik(birth_simple_lm)
llmod2 <- logLik(birth_multi_lm1)
lambda <- -2*(llmod1 - llmod2)
```

The corresponding *p*-value can then be computed through using the function `pchisq()` which computes the chi-squared distribution function for the arguments included. In this case, to compute the *p*-value, input the value of $\lambda$, the degrees of freedom (the difference between the degrees of freedom for the nested and complex models, given by the `logLik()` function) and set `lower.tail = FALSE` as arguments. 

```{r lrt p-value}
#compute the corresponding p-value
pchisq(lambda[1], df = 1, lower.tail = FALSE)
```

Given that the resulting *p*-value is less than $0.05$, the most common significance level, there is evidence that the more complex model is preferred and that the null hypothesis should be rejected.

Alternatively, the function `lrtest()` within the package `lmtest` can be used to perform a likelihood ratio test. An example of this with the `birth` dataset is as follows.

```{r install lmtest, eval=FALSE, warning=FALSE}
#install lmtest package
install.packages("lmtest")
```

```{r load lmtest, message=FALSE, warning=FALSE}
#load the lmtest package
library(lmtest)
```

```{r lrtest}
#perform a likelihood ratio test on the simple and multiple regression models
lrtest(birth_simple_lm, birth_multi_lm1)
```

It can be seen that the results from the function `lrtest()` are the same as computing the LRT manually, with the *p*-values being the same. Once again, there is therefore evidence that the null hypothesis should be rejected and that the more complex, multiple regression model is preferred, supporting the evidence of the AIC results. 

The final most common way that a LRT can be done in `R` is through using the `anova()` function again, but this time specifying `test = "LRT"` as an additional argument.

```{r anova lrt}
#likelihood ratio test with anova
anova(birth_simple_lm, birth_multi_lm1, test = "LRT")
```
This method is conducted slightly differently, hence the slight variation in the *p*-value, however, the test is also valid and also indicates that the more complex model is preferred. 


## Stepwise regression

It is always important to find out which model fits the data best. When the data only has one or two covariates, using the methods discussed so far this module, it can be a simple process of fitting each of the models and using the evaluation methods to select the best fitting model. However, once the data has more than a couple covariates available, this process becomes more lengthy. This is when stepwise regression comes in useful, although it does not guarantee to select the best model. 

This regression is a step-by-step iterative regression that looks at how the fit of a model changes when a variable is added or removed (depending on which direction you go in), testing the significance of the variable, in an automated process to select the best model with the data available. There are three approaches that can be taken with stepwise regression, the first being the way that people typically take manually and that is **forward stepwise regression**. For the forward approach, the process starts with the intercept-only model, adding one term at a time, testing its significance and keeping that term in the model if it is significant. The second approach is **backward stepwise regression**. For the backward approach, the same idea is used but the process starts with the saturated (full) model, removing terms one at a time and testing whether that term was significant through its impact on the model and the model's fit. Finally is the **bidirectional**, or **both-ways stepwise regression**, which is a combination of both forward and backward regression to test which terms should be included or excluded. This is done by starting with the intercept-only model and adding sequentially adding terms that are deemed statistically significant, and after each new term is added, any terms which are no longer statistically significant are removed. 


There are multiple ways of conducting stepwise regression in `R`, however, the most common approach is to use either the function `stepAIC()` from the `MASS` package or the function `step()` from the `stats` package, both functions used in the same way (although `step()` is a simplified version of `stepAIC()`). These methods of stepwise regression use the AIC by default to choose the best fitting model, with a model (either a `lm()` or `glm()` object) inputted as the object argument and the direction used chosen by adding the argument `direction = ` and inputting one of `"both"`, `"backward"` or `"forward"` as the direction of choice. 

```{r install MASS, eval=FALSE, warning=FALSE}
#install MASS package
install.packages("MASS")
```

```{r load MASS, message=FALSE, warning=FALSE}
#load the MASS package
library(MASS)
```

To fit the full or saturated model in `R`, instead of needing to type out each of the terms manually, after the tilde (`~`), you can put a full stop (`.`) in place of the terms which will add main effects for each covariate available in the data. 

The `mtcars` dataset from the `datasets` package used introduced in Module 2 will be used to demonstrate the different approaches to stepwise regression given that there are many covariates available in the data. To find out more about the dataset itself, search for the `mtcars` help file with `?mtcars`.

```{r mtcars dataset}
#'mtcars' is a data set available in the 'datasets' package with data on 
#11 different aspects of auto mobiles for 32 auto mobiles from the 1974 Motor 
#Trend US magazine
library(datasets)

#information on the dataset in the 'Help' pane
?mtcars

#load data and assign to 'cars_data'
cars_data <- mtcars
```


### Forward stepwise regression


To perform forward stepwise regression, you need to start from the intercept-only model as terms are added sequentially. Therefore, the first step is to fit the intercept-only model to the data. 

```{r intercept-only model cars_data}
#fit the saturated model
cars_initial <- lm(mpg ~ 1, data = cars_data)
summary(cars_initial)
```

Then, to use the `stepAIC()` function, the direction argument needs to be specified as `direction = "forward"` and the range of models (lower and upper) to be assessed specified in the `scope` argument. If no `scope` argument is specified, the initial model is used as upper model, so to explore more than just the intercept-only model with forward stepwise regression, the initial model should be included as the `lower` scope and the saturated model as the `upper` scope. Therefore, the saturated model should also be fitted prior to performing the stepwise regression

```{r saturated model cars_data}
#fit the saturated model
cars_saturated <- lm(mpg ~ ., data = cars_data)
summary(cars_saturated)
```



```{r forward stepwise regression}
#fit the forwards stepwise regression
cars_forward <- stepAIC(cars_initial, direction = "forward",
                        scope = list(lower = cars_initial, 
                                     upper = cars_saturated))
```

The output from the stepwise regression demonstrates the process clearly, starting with no covariates in the model and adding each of the covariates to the intercept-only model one at a time, selecting the covariate which results in the lowest AIC value, in this case `wt`.
The process is then repeated, adding each of the remaining covariates one at a time to the intercept and main effect for weight model, selecting the model which results in the lowest AIC value, in this case `cyl`. Repeating this process until adding any more covariates no longer improves the fit of the model results in the final model.

It can be seen that the resulting model from the forward stepwise regression includes the covariates `wt`, `cyl` and `hp` as terms in the linear model, meaning that the weight, number of cylinders and the horsepower of the car all improve the fit of the model according to the AIC when exploring the relationship between the covariates and the response, miles per gallon. 

```{r sumnmary forward stepwise}
#summary of the forward stepwise regression model 
summary(cars_forward)
```

The output from the summary of the forward stepwise regression model indicates that whilst this model produced the lowest AIC value, not all of the terms are statistically significant using the *p*-values at the 95% significance level. It is important to explore this when using stepwise regression as you do not want to include covariates unnecessarily in your model. 

### Backward stepwise regression

To perform backward stepwise regression, use the `stepAIC()` function, adding the argument `direction = "backward"`. Backward stepwise regression starts from the saturated model so this model should be inputted as the object, and since the saturated is the upper model, there is no need to add an argument for `scope`.
```{r backward stepwise regression}
#fit the backward stepwise regression
cars_backward <- stepAIC(cars_saturated, direction = "backward")
```

The output from the backward stepwise regression also demonstrates the process, starting with the saturated model and removing one covariate at a time, selecting the model which results in the lowest AIC value, repeating this process until removing another covariate from the model no longer decreases the AIC value. 

The resulting model from backward stepwise regression includes the covariates `wt`, `qsec` and `am`, meaning that weight, 1/4 mile time and transmission all improve the fit of the model according to the AIC value. This is a different model produced than when using forward stepwise regression, which is also why it is important to still look at the summary output for the model to assess whether the terms themselves are statistically significant and hence whether they have a notable impact on the response. 

```{r summary backward stepwise}
#summary of backward stepwise model
summary(cars_backward)
``` 

In this case, the summary output indicates that each of the terms included in the backward stepwise regression model are all statistically significant at the 95% level and hence should be included in the model as they impact the response variable.

### Both ways stepwise regression 
To perform both ways stepwise regression, use the `stepAIC()` function, adding the argument `direction = "both"`.
```{r both ways stepwise regression}
#fit the both ways stepwise regression
cars_both <- stepAIC(cars_saturated, direction = "both")
```
The results from the output of the both ways stepwise regression are the same as that from the backward stepwise regression in this instance, indicating that the model with the covariates `wt`, `qsec` and `am` results in the lowest AIC value. 

```{r summary both ways stepwise}
#summary of both ways stepwise model
summary(cars_both)
```
Whilst the results are the same as previously seen, it is still important to check the significance of the covariates in the final model to ensure that you are not including covariates which are not significant at the 95% (or otherwise chosen) significance level as this would indicate that the covariate does not have a significant impact on the response variable. In this case, the summary output indicates that each of the terms are statistically significant and therefore should not be removed from the model. 


## Cross-validation
As mentioned previously, when choosing a model, it is important to avoid overfitting. This occurs when the model of interest is fitted too well on the given data and does not perform well on any other, unseen data. Cross-validation (CV) is a resampling method that is used to assess the performance of the model of interest on unseen data. The general idea using the validation approach is to divide the available dataset into 2 sets, the **training set** and the **validation set**. The former is used to fit the model, essentially *training* the model using the training dataset, where the resulting fitted values are used to predict responses for the observations that are in the validation dataset. These predictions are then used to test the generalised fit of the model of interest. This results in a much more generalised assessment of the model's performance which gives a better idea of how it will perform with a new unseen dataset, instead of just the dataset used to train the model. There are multiple ways to perform cross-validation, in this section, *k*-fold cross-validation and leave-one-out cross-validation (LOOCV) will be discussed.


### *k*-fold cross-validation

One of the more widely-used approaches to cross-validation is that of the *k*-fold approach. To perform *k*-fold CV, the available dataset is partitioned into $K$ approximately equal-sized, non-overlapping (sub)sets, known as **folds**. Commonly, the choice of $K$ is either 5 or 10. 

Once the data is partitioned, for each $k=1,\cdots K$, fold $k$ is removed from the overall data and the model is fitted using the training set which is made up of the remaining $K-1$ folds Once the model has been fitted, the predictions can be found using the validation set which is the $k$th fold that was originally removed. After this process is repeated for each of the $K$ folds, the results are combined to find the cross-validation error where the lower the value of the CV error the better the generalised fit of the model.


The process is demonstrated visually in the below figure, where the complete dataset is partitioned into $K=5$ folds. 
```{r image k-fold, echo=FALSE, fig.align='center', out.width="80%", fig.cap = "Visual example of k-fold cross-validation"}
knitr::include_graphics("figures/4_images/k-fold cv.png")
```  


To demonstrate the method in `R`, the `Auto` dataset from the `ISLR` package will be used. This dataset contains information on 392 vehicles, where for this example, the covariates for miles per gallon (`mpg`) and horsepower (`horsepower`) are of interest. 

```{r install ISLR, eval=FALSE, warning=FALSE}
#install ISLR package
install.packages("ISLR")
```

```{r load ISLR, message=FALSE, warning=FALSE}
#load the ISLR package
library(ISLR)
```

To compute the *k*-fold cross-validation error, the function `cv.glm()` from the `boot` package can be used, with arguments for the dataset, the model being tested and the number of folds. From the results, the `delta[1]` coefficient can be subset in order to extract the CV error from the results. Since the partitioning into the folds is a random process, to obtain reproducible results, the seed needs to be set with the function `set.seed()`.

```{r install boot, eval=FALSE, warning=FALSE}
#install boot package
install.packages("boot")
```

```{r load boot, message=FALSE, warning=FALSE}
#load the boot package
library(boot)
```



In this data, the response (`mpg`) is assumed to follow a normal distribution and hence a GLM can be fitted to explore the relationship between the miles per gallon and horsepower with the default value for the family argument. Additionally, the number of folds for this example is chosen to be $K=10$.

```{r k-fold CV}
#load the Auto dataset
data(Auto)

#set the seed
set.seed(100)

#fit a linear model 
Auto_simple_glm <- glm(mpg ~ horsepower, data = Auto)

#find the k-fold CV error for this model fit
kfold_CV_error1 <- cv.glm(Auto, Auto_simple_glm, K = 10)$delta[1]
kfold_CV_error1
```


However, since there is no reference point for the value of CV error besides the smaller the error the better, it is important to fit alternative models to select which model has the best generalised fit. 

Below is code which fits a series of models with the `polynomial()` function including polynomials for the horsepower argument ranging from degree 1 to degree 5


```{r k-fold CV poly}
#set the seed
set.seed(100)

#create the empty variable 
kfold_CV_error2 <- c()

for (i in 1:5){
  Auto_poly_glm <- glm(mpg~poly(horsepower,5), data = Auto) 
  kfold_CV_error2[i] <- cv.glm(Auto, Auto_poly_glm, K = 10)$delta[1]
}

kfold_CV_error2

#which model has the lowest error
which.min(kfold_CV_error2)

#plot the results for a visual illustration
plot(kfold_CV_error2, xlab = "Polynomial degree", ylab = "k-fold CV error")
```

The results from the *k*-fold cross-validation indicate that the quadratic model has the lowest CV error and is therefore the model that should be used. 

### Leave-one-out cross-validation

Leave-one-out cross-validation can be described as a special case of *k*-fold cross-validation in the instance where the number of folds chosen is equal to the number of observations in the available dataset, $K=n$. This leads to each subset, or fold, containing a single observation only. This means that instead of removing a set of observations from the dataset each time the model is fitted, a singular observation is removed and the remaining data is used for fitting the model. Given the nature of this approach, it is not appropriate when datasets are very large due to the high computational cost associated to fitting the same number of models as observations in the data. 

This process is demonstrated visually in the figure below. 


```{r image LOOCV, echo=FALSE, fig.align='center', out.width="80%", fig.cap = "Visual example of leave-one-out cross-validation"}
knitr::include_graphics("figures/4_images/loocv.png")
``` 


To perform LOOCV in `R`, the process is very similar to that of *k*-fold CV fitting a GLM and then using the function `cv.glm()` to perform the cross-validation. However, instead of defining the argument `K`, this is left blank as the default value is set equal to $n$, the number of observations in the dataset. This is demonstrated below with the `Auto` dataset again. 

```{r LOOCV}
#set the seed
set.seed(100)

#fit a linear model 
Auto_simple_glm <- glm(mpg ~ horsepower, data = Auto)

#find the k-fold CV error for this model fit
LOOCV_error1 <- cv.glm(Auto, Auto_simple_glm)$delta[1]
LOOCV_error1
```

As with the *k*-fold cross-validation approach, the value of the CV error of one model on its own does not indicate as to which model is best. The same example as given for the *k*-fold approach is provided below, where the LOOCV error is given for multiple polynomial models ranging from degree 1 to degree 5. 

```{r LOOCV poly}
#set the seed
set.seed(100)

#create the empty variable 
LOOCV_error2 <- c()

for (i in 1:5){
  Auto_poly_glm <- glm(mpg ~ poly(horsepower, 5), data = Auto) 
  LOOCV_error2[i] <- cv.glm(Auto, Auto_poly_glm)$delta[1]
}

LOOCV_error2

#which model has the lowest error
which.min(LOOCV_error2)

#plot the results for a visual illustration
plot(LOOCV_error2, xlab = "Polynomial degree", ylab = "LOOCV error")
```

The results for this approach indicate that the simple linear regression model is best for this data, however, the plot demonstrates that there is much less of a difference in the error between each of the different polynomial GLMs compared the corresponding *k*-fold errors. 


## Hierarchical regression
<!-- The data modelled so far this module has all been of the same *level* due to **complete pooling**, where it is assumed the data cannot be grouped or ordered and therefore is treated together. The case of **no pooling** is when data can be grouped and a model is fit for each group individually, leading to the models only performing for the observed groupings. Somewhere in between these two pooling types is **partial pooling** which is where hierarchical modelling comes into play, where the data can be grouped (with a hierarchical data structure) but information is still shared between groups allowing for both between-group and within-group variability to be accounted for.  -->

The data modelled so far this module has all been of the same *level*, and the assumption of independence between observations is upheld. However, when data is multi-level/hierarchical, clustered or longitudinal, this assumption is not upheld and alternative modelling methods are required. This is where linear mixed modelling (incorporating both fixed and random effects) and hierarchical regression are used, enabling for the relationship between different variables at different levels to be explored.

### Data structure

A hierarchical data structure is where level 1 is **nested** in level 2, which is nested in level 3 and so on, where information can be shared between groups (also called **clusters**). When this information is shared, information on both between-group and within-group variability is provided, where typically the variation between groups is due to factors which are not measured but differ from one group to another. 

This hierarchical data structure is seen for example, when there is data on people within settlements within regions (3 levels), households within communities (2 levels) or pupils within schools within districts (3 levels). The former example is given in the below figure as a visual example of what a hierarchical data structure looks like.

```{r image hierarchical school structure, echo=FALSE, fig.align='center', out.width="80%", fig.cap = "Visual example of a hierarchical data structure"}
knitr::include_graphics("figures/4_images/hier data.png")
``` 

It is important to account for the different levels in hierarchical data in order to make reliable and correct inferences. For example, if the vaccine status of individual people within a region is of interest where the people are grouped by settlements, each settlement forms a **cluster**. It is reasonable to assume that the vaccine status within a settlement (or cluster) is not independent, but is actually correlated. This can be due to a number of factors, for instance it is reasonable to assume that people within the same settlement will have similar (physical) access to the medical centre and therefore vaccines due to having approximately the same distance to a medical centre. Another factor could be that people within the same settlement can be more likely to hold similar beliefs to one another compared to those from another settlement.

Another example of this importance can be seen in the school example where pupils are nested within schools within districts. If the response is the pupil's performance in school, this will not only depend on the individual pupil's own characteristics and intelligence, but also the school they are in and the district that the school is in. For example, private schools and grammar schools typically have better performance than standard state schools, and schools in wealthy districts typically also have better performance than schools in districts with overall lower socio-economic status. 

This example of pupils being nested within schools is seen in the `exam` dataset where there is information on different test scores, including GCSE scores (`GCSEscore`), London Reading test scores (`LRTscore`), and Verbal Reasoning test score band (`VRTsband`, levels for top 25%, middle 50% and bottom 25%) for pupils in different schools (`SchoolID`) in addition to the sex (`Sex`, levels for male and female) of each pupil and the type of school (`sch.type`, levels for mixed, male-only and female-only schools).


```{r exam data}
#import the exam dataset
exam <- read.table(paste0(data_path,'exam.txt'), sep = " ", header = FALSE,
                colClasses = c("factor","double","double","factor",
                             "factor","factor"))

#set column names
colnames(exam) <- c("SchoolID","GCSEscore","LRTscore","Sex","VRTband","sch.type")

#set levels of factor variables
levels(exam$Sex) <- c("Male","Female")
levels(exam$VRTband) <- c("Top25","Middle50","Bottom25")
levels(exam$sch.type) <- c("Mixed","Males-only","Females-only")
```

To perform hierarchical modelling in `R`, functions from the `lme4`, `nlme` or `glme` packages can be used. They both perform similar tasks but do have some differences, detailed below.
  
  - `lme4`
    - `lmer()`: fits linear mixed-effects models, similar to the `lm()` function but allows for random effects
    - `glmer()`: fits generalised linear mixed-effects models, similar to the `glm()` function but allows for random effects
    - `nlmer()`: fits non-linear mixed-effects models
  - `nlme`
    - `lme()`: fits linear mixed-effects models, similar to the `lm()` function but allows for random effects
    - `nlme()`: fits non-linear mixed-effects models
  -`glme`
    -`glme()`: fits generalised linear mixed-effects models, similar to the `glm()` function but allows for random effects



To begin with, the `lme` function will be used and has the general structure of `lme(y~x, random = ~)` where `y~x` relates to the fixed part of the model and `random=~` relates to the random part of the module. The default method of fitting the model is `REML`, which is a restricted maximum likelihood approach used to have unbiased variance components, however, this can be changed to use the maximum likelihood approach through adding the argument `method = 'ML'`. 


```{r install nlme, eval=FALSE, warning=FALSE}
#install nlme package
install.packages("nlme")
```

```{r load nlme, message=FALSE, warning=FALSE}
#load the nlme package
library(nlme)
```

### Variance partition coefficient

One method of identifying how important group level differences are is looking at the **variance partition coefficient (VPC)**, which identifies how much of the total variance is due to the level 2 variance, being the difference between groups. 

For the random intercepts modelling, the VPC is computed using the following formula.

$$\text{VPC} = \frac{\text{Level 2 variance}}{\text{Total variance}}$$


Given that the VPC is a proportion, it must be between 0 and 1, $0 \leq \text{VPC} \geq 1$, where

$$
\text{VPC} = 
\begin{cases}
0 & \text{ if there is no group effect } (\sigma^2_u=0), \\
1 & \text{ if there is no within group differences } (\sigma^2_e=0)
\end{cases}
$$


```{r VPC example, echo=FALSE, fig.align='center', out.width="80%", fig.cap = "Visual example of VPC equalling 0 and 1 respectively"}
knitr::include_graphics("figures/4_images/VPC.png")
``` 


### Random intercepts modelling

Random intercepts modelling takes into account that observations come from different groups through using a random group level effect, where the general notation for the form of the (2-level) model is as follows.

$$ y_{ij} = u_j +e_{ij}, $$
where $i$ is the index for level 1, $j$ is the index for level 2 and

  - $u_j$: group level effect (random effect)
    - Normally distributed: $u_j \sim N(0, \sigma^2_u)$  where $\sigma^2_u$ is the between-groups variance
    - Independence: $\text{cov}(u_j, u_{k})=0$
  - $e_{ij}$: individual level error (random error)
    - Normally distributed: $e_{ij} \sim N(0, \sigma^2_e)$   where $\sigma^2_e$ is the within-groups variance
    - Independence: $\text{cov}(e_{ij}, e_{kj})=0$
    
It is also assumed that there is independence between the random effects and random errors ($\text{cov}(e_{ij}, u_{j})=\text{cov}(e_{ij}, u_{k})=0$) and independence between the group level effects ($u$) and the covariates ($X$).

The random effect for the group level accounts for the difference between groups by "shifting" the intercept up or down by a random amount $u_j$ characteristic of each group. 



Using the `exam` dataset, an empty 2-level random intercepts model can be fitted to explore the GCSE scores of pupils by different schools. If no covariates are included, an empty model is fitted, taking the following form.

$$ \text{GCSE}_{ij} = u_j + e_{ij}.$$

```{r exam empty RI}
#fit the empty random intercepts model to the exam dataset
exam_emptyRI <- lme(GCSEscore ~ -1, random = ~1|SchoolID, method = 'ML', 
                    data = exam)
summary(exam_emptyRI)
```

```{r empty lme summary, echo=FALSE, fig.align='center', out.width="80%", fig.cap = "Interpreting the summary output"}
knitr::include_graphics("figures/4_images/emptyRI.png")
``` 

To compute the estimated total variance for the two-level random intercepts model the variance components for both the random effects and the error terms must be summed, where both components are provided by the summary output for the model. 

$$\hat{\sigma}_u^2 +\hat{\sigma}_e^2 = 0.411^2 + 0.921^2 = 1.017.$$
To investigate the significance of the group level differences, the VPC can be computed using the above total variance and the level 2 variance as follows.

$$ \text{VPC} =\frac{\sigma^2_u}{\sigma^2_u+\sigma^2_e} = \frac{0.411^2}{1.017} = 0.166,$$

which can be interpreted as the level 2 variability (between schools) accounts for 16.6% of the total variability observed in the GCSE scores of the pupils.

For a random intercepts model, the VPC is equal to the expected correlation between 2 randomly selected observations that belong to the same group, also known as the **intra-class correlation**. This means that a for VPC of 0.166 for the random intercepts model above, the expected correlation between the GCSE scores of 2 randomly selected pupils within the same school is 0.166.

### Random intercepts mixed-effects modelling

Random intercepts mixed modelling takes into account that observations come from different groups through using *both* fixed and random effects, where the general notation for the form of the (2-level) model is as follows.

$$ y_{ij} = \beta_0 + \beta_1x_{ij} + u_j +e_{ij}, $$
where $i$ is the index for level 1, $j$ is the index for level 2 and

  - $\beta_0$: overall intercept (fixed effect)
  - $\beta_1$: overall slope (fixed effect)
  - $u_j$: group level effect (random effect)
    - Normally distributed: $u_j \sim N(0, \sigma^2_u)$  where $\sigma^2_u$ is the between-groups variance
    - Independence: $\text{cov}(u_j, u_{k})=0$
  - $e_{ij}$: individual level error (random error)
    - Normally distributed: $e_{ij} \sim N(0, \sigma^2_e)$   where $\sigma^2_e$ is the within-groups variance
    - Independence: $\text{cov}(e_{ij}, e_{kj})=0$

It is also assumed that there is independence between the random effects and random errors ($\text{cov}(e_{ij}, u_{j})=\text{cov}(e_{ij}, u_{k})=0$) and independence between the group level effects ($u$) and the covariates ($X$).

As with the random intercepts only modelling, the random effect for the group level accounts for the difference between groups by "shifting" the intercept up or down by a random amount $u_j$ characteristic of each group. 

Using the `exam` dataset, an empty 2-level random intercepts mixed model can be fitted to explore the GCSE scores of pupils by different schools. The empty model that has only an intercept term (no covariates) for the fixed effects and a random intercept for the random effects takes the following form.

$$ \text{GCSE}_{ij} = \beta_0 + u_j + e_{ij},$$

which can be re-written as 

$$ \text{GCSE}_i = \beta_{0j} + e_{i},$$
where $\beta_{0j}=\beta_0+u_j$ is the intercept for school $j$.

```{r exam empty MRI}
#fit the empty random intercepts model to the exam dataset
exam_empty2lvlMRI <- lme(GCSEscore ~ 1, random = ~1|SchoolID, method = 'ML', 
                    data = exam)
summary(exam_empty2lvlMRI)
```



To compute the estimated total variance for the two-level random intercepts mixed-effects model the variance components for both the random effects and the error terms must be summed, where both components are provided by the summary output for the model. 

$$\hat{\sigma}_u^2 +\hat{\sigma}_e^2 = 0.411^2 + 0.921^2 = 1.017,$$
which is the same total variance as found from the empty random intercepts model above, indicating that adding the (fixed effect) intercept does not alter the total variance seen. This means that the VPC is also the same as in the empty RI model. 

As with the other types of modelling covered in this module, covariates can also be included in a random intercepts model. To see which (continuous) covariates might have a relationship with GCSE score, the `pairs()` function can be used with the dataset as an argument to produce a matrix of scatter plots. 

```{r pairs exam}
#produce pairs plot for exam data
pairs(exam)
```

The resulting scatter plots indicate that there may be a strong relationship between the GCSE score and LRT score of the pupils, so this covariate will be added to the random intercepts model, resulting in the following model.

$$ \text{GCSE}_{ij} = \beta_0 + \beta_1\text{LRT}_{ij} + u_j + e_{ij},$$
where the mean of GCSE is allowed to vary across schools after adjusting by LRT in addition to the assumption that the effect for LRT is the same for all schools. Given that this is a random intercept model and there is no random slope component, this model creates parallel cluster-specific (school-specific) lines which have different intercepts. To fit this model in `R`, the `lme` function can be used again but with the addition of a fixed effect term for LRT. 

```{r exam RI LRT}
#fit the random intercept model to the exam data with a covariate for LRTscore
exam_2lvlMRI1 <- lme(GCSEscore ~ LRTscore, random = ~ 1|SchoolID, method = 'ML',
                data = exam)
summary(exam_2lvlMRI1)
```

The summary output for this model contains the same information as for the empty random intercepts model, with the addition of coefficient and correlation information for the LRT covariate. The coefficient estimate indicates that for a pupil with an average LRT score, the estimated overall mean of standardised GCSE score is 0.002, but for each unit increase in the LRT score, the expected standardised GCSE score of a pupil will increase by 0.563.

Looking at the variance for this model, the within-group (within-school) variance has reduced meaning that the heterogeneity (difference) in the reading abilities of the pupils explains some of the heterogeneity in the GCSE scores. The between-group variance is also reduced, leading to the total variance reducing to $\hat{\sigma}^2_u+\hat{\sigma}^2_e = 0.304^2+0.752^2 = 0.658$.

The updated VPC is also reduced slightly, computed as 

$$ \text{VPC} =\frac{\sigma^2_u}{\sigma^2_u+\sigma^2_e} = \frac{0.304^2}{0.658} = 0.140,$$
meaning that 14% of the total variability observed in the GCSE scores can be attributed to the variability between schools, once controlled by the London Reading Test scores. 

The model fit information for this model indicates that it is a better fit than the empty random intercept model, with lower AIC and BIC values. In addition, the variance for both the random effects and random error are smaller due to the addition of a covariate which aids in explaining some of the variability seen in the data. 

To demonstrate how the school level impacts the intercept of the model, a random selection of schools can be taken and the corresponding lines of best fit can be plotted. To do this, predicted values need to be computed, where from the model above, two types of predicted values can be computed:
  
  - Population average predicted values: (unconditional inference) beneficial for making predictions for groups not included in the available data since only the coefficient estimates for the fixed part of the model are used.
  - Group-specific predicted values: (conditional inference) beneficial for making predictions for groups included in the available data since the coefficient estimates for both the fixed and random parts of the model are used. 
  
To obtain these predicted values, the `fitted()` function can be used in a similar way to the `predict()` function introduced earlier in this module, however, instead of including a new dataset in the function, a level argument is included, where `level = 0` computes the population average predicted values and `level = 1` computes the group-specific predicted values.

```{r exam plot random schools}
attach(exam)
#identify the unique schools and record their types
unique_school <- unique(exam[,c(1,6)])
#sample 6 of the schools
set.seed(1)
sampled_school <- sample(unique_school$SchoolID, 6)

#school-specific predicted values from model with covariate for LRTscore
fitted1 <- fitted(exam_2lvlMRI1, level=1)

#subset original data for the 6 sampled schools
subset_school <- data.frame(exam[SchoolID %in% sampled_school, ],
                            fit1 = fitted1[SchoolID %in% sampled_school])

#plot the GCSE and LRT scores for the sampled schools with lines of best fit
plot(subset_school$LRTscore, subset_school$GCSEscore, pch = 16, cex = 0.2,
     col=seq(1:length(sampled_school)), xlab = "LRT score", ylab = "GCSE")
for (i in 1:length(sampled_school)){
  lines(subset_school$LRTscore[subset_school$SchoolID == sampled_school[i]],
        subset_school$fit1[subset_school$SchoolID == sampled_school[i]], 
        col = i, lty = 1)
}
n <- table(exam$SchoolID) #sizes by school
legend("bottomright",legend=paste(sampled_school, "(",n[sampled_school], ")") ,
       col = seq(1:length(sampled_school)), lty = 1, cex = 0.7)
```

The plot provides the school-specific predicted values for 6 randomly selected schools, where it can be seen that the higher the LRT score, the higher the GCSE score for all of the schools, where the slope of each school-specific line is the same. However, the intercept for each school varies due to the random intercept included in the model, where on average, the school with ID number 1 has the highest GCSE scores, and the school with ID number 23 has the lowest GCSE scores. 

### Random slope mixed-effects modelling

Random slope (and intercepts) mixed-effects modelling is an extension of random intercepts modelling that also allows for the relationship between the dependent and independent variables to differ for each group, meaning that not only can the intercept differ by group, but so can the slope. This difference is accounted for through an additional random effect for the slope which acts as an interaction between the group and the independent variable $X$, leading to lines which are not parallel as in the random intercepts modelling. The general notation for the (2-level) model is as follows.

$$ y_{ij} = \beta_0 + \beta_1x_{ij} + u_{0j} + u_{1j}x_{ij} + e_{ij}, $$
where $i$ is the index for level $1$, $j$ is the index for level $2$ and 

  - $\beta_0$: overall intercept (fixed effect)
  - $\beta_1$: overall slope (fixed effect)
  - $u_{0j}$: group level intercept (random effect)
    - Normally distributed: $u_{0j} \sim N(0, \sigma^2_{u0})$  where $\sigma^2_u$ is the between-groups variance
    - Independence (across groups): $\text{cov}(u_{0j}, u_{0jk})=0$
  - $u_{1j}$ group level slope (random effect)
    - Normally distributed: $u_{1j} \sim N(0, \sigma^2_{u1})$
    - Independence (across groups): $\text{cov}(u_{1j}, u_{1jk})=0$
  - $e_{ij}$: individual level error (random error)
    - Normally distributed: $e_{ij} \sim N(0, \sigma^2_e)$   where $\sigma^2_e$ is the within-groups variance
    - Independence: $\text{cov}(e_{ij}, e_{kj})=\text{cov}(e_{ij}, u_{0j})=\text{cov}(e_{ij}, u_{1j})=0$

It is also assumed that the random intercept and random slope are dependent within a group, $\text{cov}(u_{0j}, u_{1j})=\sigma_{u01}$. Additionally, the inclusion of the random slope means that there is heterogeneous variance, meaning that the variance is not constant as it is in the random intercepts modelling (variance in random slopes modelling is a quadratic function of x).


Using the `exam` dataset, the 2-level random slopes model can be fitted to explore the GCSE scores of pupils by different schools, controlling for the LRT score of the pupils and allowing for the relationship between GCSE score and LRT score to differ by school, with the model taking the following form.

$$ GCSE_{ij} = \beta_0 + \beta_1 \times LRTscore_{ij} + u_{0j} + u_{1j}\times LRTscore_{ij} + e_{ij},$$

which can be rewritten as 

$$GCSE_{ij} = \beta_{0ij} + \beta_{1j}\times LRTscore_{ij},$$ 
where $\beta_{0ij}=\beta_0 + u_{0j}+e_{ij}$ and $\beta_{1j}=\beta_1+u_{1j}$.

To incorporate this random slope into the model in `R`, the random slope term can be included in the `random` argument in the `lme()` function after the random intercept term. This is seen in the `R` code as follows.

```{r random slope modelling}
#fit the random slope mixed model to the exam dataset
exam_2lvlMRS <- lme(GCSEscore ~ LRTscore, random = ~ 1 + LRTscore | SchoolID,
                method = 'ML', data = exam)
summary(exam_2lvlMRS)
```
Population average (or unconditional) inference can be done simply through interpreting the results from the summary output. For example, over the whole population of the schools, if a pupil has an average LRT score (LRT score = 0), the pupil is expected to get a GCSE score of $-0.012$. Additionally, over the whole population of the schools, for each unit increase in the LRT score, the expected GCSE score of a pupil increases by 0.557. 

To test if the additional random slope parameter is significant, a likelihood ratio test can be conducted, computing the log-likelihood values with the function `logLik()` and using the function `pchisq()` to find the corresponding $p$-value.

```{r LRT RS}
#compute lambda
llmod1 <- logLik(exam_2lvlMRI1)
llmod2 <- logLik(exam_2lvlMRS)
lambda <- -2*(llmod1 - llmod2)

#compute the corresponding p-value (df = 2 because of the additional terms)
pchisq(lambda[1], df = 2, lower.tail = FALSE)
```

The resulting $p$-value is approximately zero, meaning that it is much smaller than the 5% significance level, indicating that the additional term for the random slope is highly significant. The interpretation of this result is that there is evidence that the effect that the LRT scores have on GCSE scores varies across the schools.

Alternatively, an ANOVA table can be used to test the significance as follows.

```{r ANOVA RS}
#anova of two models
anova(exam_2lvlMRI1, exam_2lvlMRS)
```
Where the results from the ANOVA table also indicate that the addition of the random slope is significant. 

Given the additional terms involved with random slopes modelling, the VPC formula changes, with the resulting formula as follows.

$$ \text{VPC}(x) = \frac{\text{Level 2 variance}}{\text{Total variance}} = \frac{\text{Var}(u_{0j}+u_{1j}x_{ij})}{\text{Var}(u_{0j}+u_{1j}x_{ij}+e_{ij})}=\frac{\sigma^2_{u0}+2x_{ij}\sigma_{u01}+x_{ij}^2\sigma^2_{u1}}{\sigma^2_{u0}+2x_{ij}\sigma_{u01}+x_{ij}^2\sigma^2_{u1}+\sigma^2_e}, $$
where each of the variance and covariance terms can be found from reading the summary output of the model in the random effects part. The VPC is also dependent on the value of $x$ and a quadratic function of $x$, unlike with the random intercepts models where the VPC is independent of $x$ so is constant across the different values of $x$. 

```{r lme RS summary, echo=FALSE, fig.align='center', out.width="80%", fig.cap = "Interpreting the summary output"}
knitr::include_graphics("figures/4_images/RSoutput.png")
```

$$\text{VPC(LRTscore)} = \frac{\sigma^2_{u0}+2x\sigma_{u01}+x^2\sigma^2_{u1}}{\sigma^2_{u0}+2x\sigma_{u01}+x^2\sigma^2_{u1}+\sigma^2_e} = \frac{0.301^2 + 2\times \text{LRTscore}_{ij} \times 0.497 + \text{LRTscore}_{ij}^2 \times 0.121^2}{0.301^2 + 2\times \text{LRTscore}_{ij} \times 0.497 + \text{LRTscore}_{ij}^2 \times 0.121^2 + 0.744^2},$$
which is dependent on the LRT score. 

As with the random intercepts mixed-effects model, the results from the random slopes mixed-effects model can be plotted to visually demonstrate the effect that the additional terms have on the models. This is done in the same way as before, through randomly sampling a selection of schools, computing the conditional predicted values with the `fitted()` function and plotting the results with a line of best fit for each of the sampled schools.


```{r exam plot random schools RS}
#identify the unique schools and record their types
unique_school <- unique(exam[,c(1,6)])
#sample 6 of the schools
set.seed(1)
sampled_school <- sample(unique_school$SchoolID, 6)

#school-specific predicted values from model with covariate for LRTscore
fitted2 <- fitted(exam_2lvlMRS, level = 1)

#subset original data for the 6 sampled schools
subset_school2 <- data.frame(exam[SchoolID %in% sampled_school,],
                            fit2 = fitted2[SchoolID %in% sampled_school])

#plot the GCSE and LRT scores for the sampled schools with lines of best fit
plot(subset_school2$LRTscore, subset_school2$GCSEscore, pch = 16, cex = 0.2,
     col=seq(1:length(sampled_school)), xlab = "LRT score", ylab = "GCSE")
for (i in 1:length(sampled_school)){
  lines(subset_school2$LRTscore[subset_school2$SchoolID == sampled_school[i]],
        subset_school2$fit2[subset_school2$SchoolID == sampled_school[i]], 
        col = i, lty = 1)
}
n <- table(exam$SchoolID) #sizes by school
legend("bottomright", legend=paste(sampled_school, "(",n[sampled_school], ")") ,
       col = seq(1:length(sampled_school)), lty = 1, cex = 0.7)
```

The plot provides the school-specific predicted values for 6 randomly selected schools, where the same trend between LRT score and GSCE score is seen as before (the higher the LRT score the higher the GCSE score), however, the effect that LRT score has on GCSE score changes depending on the school. For example, the slope for the school with ID number 1 has a steeper slope than that of the school with ID number 23, indicating that for school 1, a higher LRT score has more of an impact on the pupil's GCSE score than for school 23.


### Adding an extra level

When there are only 2 levels in a model, it is assumed that the level 2 groups are independent of each other. However, this is not always the case, and this is where it is important to add a level 3 group. When there are three levels, $i$ corresponds to level 1, $j$ corresponds to level 2 and $k$ corresponds to level 3. There are two cases for random structure in 3-level models which are important to distinguish between given that they are specified differently in the `lme()` function in `R`.


**Case 1:** The first case for the 3-level mixed-effects model form is that where the models at all 3 of the levels are equal. This can mean that there are either only random intercepts at each level or there are random intercepts and random slopes at each level. 
The general form of the 3-level random intercepts mixed-effects model where there are only random intercepts at each level is given as follows.

$$y_{ijk} = \beta_0 + \beta_1x_{ijk} + v_{0k} + u_{0jk} + e_{0ijk}, $$
which can be rewritten as 

$$ y_{ijk} = \beta_{0ijk} + \beta_{1}x_{ijk},$$
where $\beta_{0ijk} = \beta_0 + v_{0k} + u_{0jk} + e_{ijk}$. Then, the general form of the 3-level random slopes mixed-effects model where there are random intercepts and random slopes at each level is given as follows.

$$y_{ijk} = \beta_0 + \beta_1x_{ijk} + v_{0k}+ v_{1k}x_{ijk} + u_{0jk} + u_{1jk}x_{ijk} + e_{0ijk}, $$
which can be rewritten as 

$$ y_{ijk} = \beta_{0ijk} + \beta_{1jk}x_{ijk},$$

where $\beta_{0ijk} = \beta_0 + v_{0k} + u_{0jk} + e_{ijk}$ and $\beta_{1jk} = \beta_1 + v_{1k} + u_{1jk}$. 


**Case 2:** The second case for the 3-level mixed-effects model form is when the models at some levels are different, for example, if only the second level has a random slope, but all levels have a random intercept. The general form of this model is given as follows.

$$y_{ijk} = \beta_0 + \beta_1x_{ijk} + v_{0k} + u_{0jk} + u_{1jk}x_{ijk} + e_{0ijk}, $$
which can be rewritten as 

$$ y_{ijk} = \beta_{0ijk} + \beta_{1jk}x_{ijk},$$
where $\beta_{0ijk} = \beta_0 + v_{0k} + u_{0jk} + e_{ijk}$ and $\beta_{1jk} = \beta_1 + u_{1jk}$. 


When using the `lme()` function in `R`, the random structure in the first case is specified as `random = ~ () | level3/level2`, compared to the second case where the random structure is specified as `random = list(~ | ~ | ~ |)`. These differences are explored in this section with application to the `exam` dataset example, where pupils are nested within schools, and schools can be grouped by district, and schools within the same district share common characteristics. The indices of each level are then given as pupil $i$ is nested in school $j$ which is nested in district $k$. 

```{r add district to exam}
#add district level to the exam data
district <- read.table(paste0(data_path, 'exam_district.csv'), header = FALSE)
colnames(district) <- "district"
exam[, 7] <- district
```

**Case 1:** The 3-level mixed-effects model with random intercepts only is given as follows.
$$GCSE_{ijk} =( \beta_0 +  v_{0k} + u_{0jk} ) + \beta_1 LRTscore_{ijk} + e_{0ijk},$$

which is fitted in `R` using the `lme()` function in the following code.
```{r RI 3 level}
#fit the 3-level random intercepts only model
exam_3lvlMRI <- lme(GCSEscore ~ LRTscore, 
                   random = ~ 1|district/SchoolID, 
                   method = 'ML', data = exam)
summary(exam_3lvlMRI)
```

The 3-level mixed-effects model with random intercepts and random slopes at each level is given as follows.
$$GCSE_{ijk} = (\beta_0  + v_{0k} + u_{0jk})+ (\beta_1 + v_{1k}  + u_{1jk})LRTscore_{ijk} + e_{0ijk},$$

which is fitted in `R` using the `lme()` function in the following code.
```{r RS 3 level}
#fit the 3-level random intercepts only model
exam_3lvlMRS <- lme(GCSEscore ~ LRTscore, 
                   random = ~ (1 + LRTscore) | district/SchoolID,
                   data = exam, method = 'ML',
                   control = lmeControl(opt = "optim")) 

summary(exam_3lvlMRS)
```

**Case 2:** The 3-level mixed-effects model with random intercepts for all levels and a random slope for level 2 is given as follows.
$$GCSE_{ijk} = (\beta_0 + v_{0k} + u_{0jk}) + (\beta_1 + u_{1jk}) LRTscore_{ijk} + e_{0ijk},$$

which is fitted in `R` using the `lme()` function in the following code.
```{r RI RS 3 level}
#fit the 3-level random intercepts only model
exam_3lvlMRIS <- lme(GCSEscore ~ LRTscore, 
                   random = list( ~ 1|district, ~ 1 + LRTscore|SchoolID), 
                   method = 'ML', data = exam,                   
                   control = lmeControl(opt = "optim")) 

summary(exam_3lvlMRIS)
```
As with the other modelling approaches, the a likelihood ratio test can be used to check the significance, in this case of the 3rd level by comparing the 2-level random slopes model created above with the 3-level model with random intercepts at each level and a random slope at the school level. 


```{r LRT 3rd level}
#compute lambda
llmod1 <- logLik(exam_2lvlMRS)
llmod2 <- logLik(exam_3lvlMRIS)
lambda <- -2*(llmod1-llmod2)

#compute the corresponding p-value (df = 1 because of the additional term)
pchisq(lambda[1], df = 1, lower.tail = FALSE)
```
Which indicates that the addition of the district as a 3rd level is not significant given that the resulting $p$-value is not less than the 5% significance level.


<!-- ## Cameroon application -->

<!-- ```{r cameroon example} -->
<!-- Data_CMR <- read.csv(paste0(data_path,"CMR/Pop_Data_Complete.csv")) -->


<!-- Data_CMR$Density = Data_CMR$Total_Pop/Data_CMR$Total_Building_Count -->
<!-- Data_CMR$LDensity = log(Data_CMR$Density) -->
<!-- Data_CMR$y = Data_CMR$LDensity -->

<!-- fit <- lm(Data_CMR$y ~ Data_CMR$x3, random = ~1, data = Data_CMR) -->
<!-- summary(fit) -->

<!-- ``` -->
<!-- The total variance is given as $\text{Var}(v_{0k} + u_{0jk} + e_{ijk}) = \sigma^2_{v0} + \sigma^2_{u0}+ \sigma^2_{e0}$ under the assumption of independence between $v$, $u$ and $e$, with correlation between two schools in the same district given as  -->
<!-- $$ \frac{\sigma^2_{v0}}{\sigma^2_{v0} + \sigma^2_{u0}+ \sigma^2_{e0}},$$ -->
<!-- and correlation between two pupils in the same school given as -->
<!-- $$\frac{\sigma^2_{v0} + \sigma^2_{u0}}{\sigma^2_{v0} + \sigma^2_{u0}+ \sigma^2_{e0}}.$$ -->

<!-- ## End of module exercises -->

## Useful resources

  - Statistical modelling: [Introduction to Statistical Modeling](https://tjfisher19.github.io/introStatModeling/)
  - Linear regression: [Simple Linear Regression: Everything You Need to Know](https://www.datacamp.com/tutorial/simple-linear-regression)
  - Polynomial regression: [Polynomial Regression: An Introduction](https://builtin.com/machine-learning/polynomial-regression)
  - Non-linear regression: [GeeksforGeeks](https://www.geeksforgeeks.org/non-linear-regression-examples-ml/)
  - Generalised linear regression: [GeeksforGeeks](https://www.geeksforgeeks.org/generalized-linear-models/)
  - Model predictions: [Using Linear Regression for Predictive Modeling in R](https://www.dataquest.io/blog/statistical-learning-for-predictive-modeling-r/)
  - Model selection: [Model Selection](https://bookdown.org/animestina/intro_stats_rms/model-selection.html)
  - Stepwise regression: [A Complete Guide to Stepwise Regression in R](https://www.r-bloggers.com/2023/12/a-complete-guide-to-stepwise-regression-in-r/)
  - Stepwise regression: [GeeksforGeeks](https://www.geeksforgeeks.org/stepwise-regression-in-r/)
  - Cross-validation:[GeeksforGeeks](https://www.geeksforgeeks.org/cross-validation-in-r-programming/)
  - Hierarchical regression: [Hierarchical Linear Models](https://methodenlehre.github.io/intro-to-rstats/hierarchical-linear-models.html)